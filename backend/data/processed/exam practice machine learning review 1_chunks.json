[
  {
    "text": "Practice Exams: AWS Machine Learning Engineer Associate Cert\nPractice Test 1: Practice Test #1 - Full Exam - AWS Certified\nMachine Learning Engineer - Associate (MLA-C01)\nSearch course content\nStart a new search\nTo find captions, lectures or resources\n•\nStart\nPractice Test 1: Practice Test #1 - Full Exam - AWS Certified\nMachine Learning Engineer - Associate (MLA-C01)\n\n•\nStart\nPractice Test 2: Practice Test #2 - Full Exam - AWS Certified\nMachine Learning Engineer - Associate (MLA-C01)\n•\nStart\nPractice Test 3: Practice Test #3 - Full Exam - AWS Certified\nMachine Learning Engineer - Associate (MLA-C01)\nPrepare for AWS Certified Machine Learning\nEngineer Associate. 195 high-quality test\nquestions and detailed explanations\nRating: 4.6 out of 5\n4.6\n532 ratings\n9,076\nStudents\n195 questions\nTotal\nLast updated July 2025\nEnglish\ninformation alert\nSchedule learning time",
    "chunk_id": 0,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Learning a little each day adds up. Research shows that students who\nmake learning a habit are more likely to reach their goals. Set time aside to\nlearn and get reminders using your learning scheduler.\nBy the numbers\nSkill level: All Levels\nStudents: 9076\nLanguages: English\nCaptions: No\nDescription\nPreparing for AWS Certified Machine Learning Engineer -\nAssociate (MLA-C01)? This is THE practice exams course to give\nyou the winning edge.\nThese practice exams have been co-authored by Stephane\nMaarek and Abhishek Singh who bring their collective experience\nof passing 18 AWS Certifications to the table.\nThe tone and tenor of the questions mimic the real exam. Along\nwith the detailed description and “exam alert” provided within the\nexplanations, we have also extensively referenced AWS\ndocumentation to get you up to speed on all domain areas being\ntested for the MLA-C01 exam.\nWe want you to think of this course as the final pit-stop so that",
    "chunk_id": 1,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "documentation to get you up to speed on all domain areas being\ntested for the MLA-C01 exam.\nWe want you to think of this course as the final pit-stop so that\nyou can cross the winning line with absolute confidence and get\nAWS Certified! Trust our process, you are in good hands.\nAll questions have been written from scratch! More questions are\nbeing added based on the student feedback!\nYou will get a warm-up practice exam and ONE high-quality\nFULL-LENGTH practice exam to be ready for your certification.\nQuality speaks for itself:\nSAMPLE QUESTION:\nYou are working as a data scientist at a financial services\ncompany tasked with developing a credit risk prediction model.\nAfter experimenting with several models, including logistic\nregression, decision trees, and support vector machines, you find\nthat none of the models individually achieves the desired level of",
    "chunk_id": 2,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "accuracy and robustness. Your goal is to improve overall model\nperformance by combining these models in a way that leverages\ntheir strengths while minimizing their weaknesses.\nGiven the scenario, which of the following approaches is the\nMOST LIKELY to improve the model’s performance?\n1. Use a simple voting ensemble, where the final prediction is\nbased on the majority vote from the logistic regression, decision\ntree, and support vector machine models\n2. Implement boosting by training sequentially different types of\nmodels - logistic regression, decision trees, and support vector\nmachines - where each new model corrects the errors of the\nprevious ones\n3. Apply stacking, where the predictions from logistic regression,\ndecision trees, and support vector machines are used as inputs\nto a meta-model, such as a random forest, to make the final\nprediction\n4. Use bagging, where different types of models - logistic\nregression, decision trees, and support vector machines - are",
    "chunk_id": 3,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "to a meta-model, such as a random forest, to make the final\nprediction\n4. Use bagging, where different types of models - logistic\nregression, decision trees, and support vector machines - are\ntrained on different subsets of the data, and their predictions are\naveraged to produce the final result\nWhat's your guess? Scroll below for the answer.",
    "chunk_id": 4,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Correct: 3\nExplanation:\nCorrect option:\nApply stacking, where the predictions from logistic regression,\ndecision trees, and support vector machines are used as inputs\nto a meta-model, such as a random forest, to make the final\nprediction\nIn bagging, data scientists improve the accuracy of weak\nlearners by training several of them at once on multiple datasets.\nIn contrast, boosting trains weak learners one after another.\nStacking involves training a meta-model on the predictions of\nseveral base models. This approach can significantly improve\nperformance because the meta-model learns to leverage the\nstrengths of each base model while compensating for their\nweaknesses.\nFor the given use case, leveraging a meta-model like a random\nforest can help capture the relationships between the predictions\nof logistic regression, decision trees, and support vector\nmachines.\n<Solution reference image>\n<via - reference link>\nIncorrect options:",
    "chunk_id": 5,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "of logistic regression, decision trees, and support vector\nmachines.\n<Solution reference image>\n<via - reference link>\nIncorrect options:\nUse a simple voting ensemble, where the final prediction is based\non the majority vote from the logistic regression, decision tree,\nand support vector machine models - A voting ensemble is a\nstraightforward way to combine models, and it can improve\nperformance. However, it typically does not capture the complex\ninteractions between models as effectively as stacking.\nImplement boosting by training sequentially different types of\nmodels - logistic regression, decision trees, and support vector\nmachines - where each new model corrects the errors of the\nprevious ones - Boosting is a powerful technique for improving\nmodel performance by training models sequentially, where each\nmodel focuses on correcting the errors of the previous one.",
    "chunk_id": 6,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "However, it typically involves the same base model, such as\ndecision trees (e.g., XGBoost), rather than combining different\ntypes of models.\nUse bagging, where different types of models - logistic\nregression, decision trees, and support vector machines - are\ntrained on different subsets of the data, and their predictions are\naveraged to produce the final result - Bagging, like boosting, is\neffective for reducing variance and improving the stability of\nmodels, particularly for high-variance models like decision trees.\nHowever, it usually involves training multiple instances of the\nsame model type (e.g., decision trees in random forests) rather\nthan combining different types of models.\n<With multiple reference links from AWS documentation>\nInstructor\nMy name is Stéphane Maarek, I am passionate about Cloud\nComputing, and I will be your instructor in this course. I teach\nabout AWS certifications, focusing on helping my students\nimprove their professional proficiencies in AWS.",
    "chunk_id": 7,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Computing, and I will be your instructor in this course. I teach\nabout AWS certifications, focusing on helping my students\nimprove their professional proficiencies in AWS.\nI have already taught 2,500,000+ students and gotten 800,000+\nreviews throughout my career in designing and delivering these\ncertifications and courses!\nI'm delighted to welcome Abhishek Singh as my co-instructor for\nthese practice exams!\nWelcome to the best practice exams to help you prepare for your\nAWS Certified Machine Learning Engineer - Associate exam.\n• You can retake the exams as many times as you want\n• This is a huge original question bank\n• You get support from instructors if you have questions\n• Each question has a detailed explanation\n• Mobile-compatible with the Udemy app\n• 30-days money-back guarantee if you're not satisfied",
    "chunk_id": 8,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "We hope that by now you're convinced! And there are a lot more\nquestions inside the course.\nHappy learning and best of luck for your AWS Certified Machine\nLearning Engineer - Associate exam!\nWhat you’ll learn\n• Guaranteed chance to pass the exam if you score 90%+ on\neach practice exam\n• Ace your AWS Certified Machine Learning Engineer -\nAssociate (MLA-C01) exam\n• Practice with high quality practice exams alongside detailed\nexplanation to learn concepts\n• The MLA-C01 practice exams have been written from\nscratch\nAre there any course requirements or prerequisites?\n• Recommended: Preparing for the exam with \"AWS Certified\nMachine Learning Engineer Associate: Hands On!\" course\nby Stephane Maarek, Frank Kane\nWho this course is for:\n• Anyone preparing for the AWS Certified Machine Learning\nEngineer - Associate (MLA-C01) exam\nInstructor\nStephane Maarek | AWS Certified Cloud\nPractitioner,Solutions Architect,Developer\nBest Selling Instructor, 11x AWS Certified, Kafka Guru",
    "chunk_id": 9,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Stephane is a solutions architect, consultant and software\ndeveloper that has a particular interest in all things related to\nCloud & Big Data. He's also a many-times best seller instructor\non Udemy for his courses in AWS and Apache Kafka.\n[See FAQ below to see in which order you can take my courses]\nStéphane is recognized as an AWS Hero and is an AWS Certified\nSolutions Architect Professional & AWS Certified DevOps\nProfessional. He loves to teach people how to use the\nAWS properly, to get them ready for their AWS certifications, and\nmost importantly for the real world.\nHe also loves Apache Kafka. He used on the Program Committee\norganizing the Kafka Summit in New York, London and San\nFrancisco. He also was an active member of the Apache Kafka\ncommunity, and has authored blogs on Medium and the guest\nblog for Confluent. He also has co-founded Conduktor, a\nprominent company in the Kafka ecosystem.\nDuring his spare time he enjoys cooking, practicing yoga,",
    "chunk_id": 10,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "blog for Confluent. He also has co-founded Conduktor, a\nprominent company in the Kafka ecosystem.\nDuring his spare time he enjoys cooking, practicing yoga,\nsurfing, watching TV shows, and traveling to awesome\ndestinations!\nFAQ: In which order should you learn?...\nAWS Cloud: Start with AWS Certified Cloud Practitioner or\nAWS Certified Solutions Architect Associate, then move on to\nAWS Certified Developer Associate and then AWS Certified\nSysOps Administrator. Afterwards you can either do AWS\nCertified Solutions Architect Professional or AWS Certified\nDevOps Professional, or a specialty certification of your\nchoosing. You can also learn about AI with the AWS Certified\nAI Practitioner course!",
    "chunk_id": 11,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Apache Kafka: Start with Apache Kafka for Beginners, then you\ncan learn Connect, Streams and Schema Registry if you're a\ndeveloper, and Setup and Monitoring courses if you're an admin.\nBoth tracks are needed to pass the Confluent Kafka certification.\nInstructor\nAbhishek Singh\nCloud Evangelist\nAbhishek is an AWS veteran and has built successful SaaS and\nconsumer solutions using AWS services since 2012. Over the\ncourse of his professional career, Abhishek has interviewed and\nmentored hundreds of candidates for entry-level and lateral\npositions for Cloud based IT solutions development. Abhishek is\npassionate about sharing his knowledge on AWS Cloud, Machine\nLearning and Big Data. He wants to help his fellow IT\nProfessionals level-up their skills to ace the AWS Certifications\nand above all, get ready for the real world AWS ecosystem.\nHe is an AWS Certified Solutions Architect Professional, AWS\nCertified DevOps Engineer Professional, AWS Certified Machine",
    "chunk_id": 12,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "and above all, get ready for the real world AWS ecosystem.\nHe is an AWS Certified Solutions Architect Professional, AWS\nCertified DevOps Engineer Professional, AWS Certified Machine\nLearning Specialist, AWS Certified Big Data Specialist and AWS\nCertified Database Specialist.\nOverall, Abhishek has over 15 years of experience working on a\ndiverse range of Enterprise Technologies based on AI/ML, Big\nData and Analytics. He runs a successful AI/ML and Big Data",
    "chunk_id": 13,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Consultancy advocating solutions on AWS Cloud and has\nadvised multiple clients in the US to architect and implement\ntheir AI/ML and Big Data solutions using the AWS suite of\nservices.\nClick the \"Create a new note\" box, the \"+\" button, or press \"B\" to\nmake your first note.\n\nLearning reminders\nSet up push notifications or calendar events to stay on track for\nyour learning goals.\nTeach the world online\nCreate an online video course, reach students across the globe,\nand earn money\nTop companies choose Udemy Business to build in-demand\ncareer skills.\n• Udemy Business\n• Teach on Udemy\n• Get the app\n• About us\n• Contact us\n• Careers\n• Blog\n• Help and Support\n• Affiliate\n• Investors\n• Terms\n• Privacy policy\n•\n• Sitemap\n• Accessibility statement\n© 2025 Udemy, Inc.",
    "chunk_id": 14,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Practice Test 1: Practice Test #1 - Full Exam - AWS Certified\nMachine Learning Engineer - Associate (MLA-C01)\nPractice Test #1 - Full Exam - AWS Certified\nMachine Learning Engineer - Associate (MLA-\nC01) - Results\nAttempt 2\n•\nQuestion 1\nCorrect\nYou are a machine learning engineer at a financial services\ncompany tasked with building a real-time fraud detection system.\nThe model needs to be highly accurate to minimize false\npositives and false negatives. However, the company has a\nlimited budget for cloud resources, and the model needs to be\nretrained frequently to adapt to new fraud patterns. You must\ncarefully balance model performance, training time, and cost to\nmeet these requirements.",
    "chunk_id": 15,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Which of the following strategies is the MOST LIKELY to achieve\nan optimal balance between model performance, training time,\nand cost?\nYour answer is correct\nImplement a tree-based model like XGBoost with early stopping\nand hyperparameter tuning, balancing accuracy with reduced\ntraining time and computational cost\nUse a deep neural network with multiple layers and complex\narchitecture to maximize performance, even if it requires\nsignificant computational resources and longer training times\nDeploy a simpler model like logistic regression to reduce training\ntime and cost, while accepting a slight reduction in model\naccuracy\nChoose a support vector machine (SVM) with a nonlinear kernel\nto enhance accuracy, regardless of the increased training time\nand cost associated with large datasets\nOverall explanation\nCorrect option:\nImplement a tree-based model like XGBoost with early stopping\nand hyperparameter tuning, balancing accuracy with reduced\ntraining time and computational cost",
    "chunk_id": 16,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Overall explanation\nCorrect option:\nImplement a tree-based model like XGBoost with early stopping\nand hyperparameter tuning, balancing accuracy with reduced\ntraining time and computational cost\nXGBoost is known for its ability to deliver high performance with\nrelatively efficient training times, especially with techniques like\nearly stopping and hyperparameter tuning. This approach\nbalances the need for accuracy with reduced computational cost\nand training time, making it an ideal choice for this scenario.\nIncorrect options:\nUse a deep neural network with multiple layers and complex\narchitecture to maximize performance, even if it requires\nsignificant computational resources and longer training times - A\ndeep neural network may provide high accuracy but typically\nrequires significant computational resources and longer training",
    "chunk_id": 17,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "times, leading to higher costs. This approach may not be feasible\nwithin a limited budget, especially with the need for frequent\nretraining.\nDeploy a simpler model like logistic regression to reduce training\ntime and cost, while accepting a slight reduction in model\naccuracy - Logistic regression is simple and cost-effective but\nmay not achieve the level of accuracy required for a critical\napplication like fraud detection. This tradeoff might be too\nsignificant if accuracy is compromised.\nChoose a support vector machine (SVM) with a nonlinear kernel\nto enhance accuracy, regardless of the increased training time\nand cost associated with large datasets - SVMs with nonlinear\nkernels can be very accurate but are computationally intensive,\nparticularly with large datasets. The increased training time and\ncost might outweigh the benefits, especially when there are more\ncost-effective alternatives like XGBoost.\nReference:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/algos.html\nDomain",
    "chunk_id": 18,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "cost might outweigh the benefits, especially when there are more\ncost-effective alternatives like XGBoost.\nReference:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/algos.html\nDomain\nML Model Development\nQuestion 2\nCorrect\nA financial analytics company runs a data aggregation job every\nSaturday night to process transactional data from the past week.\nThe job is scheduled to run for approximately 2 hours and can\ntolerate interruptions without impacting the results. The company\nplans to run this job consistently every weekend for the next 6\nmonths.\nWhich EC2 instance purchasing option will meet these\nrequirements MOST cost-effectively?\nUse EC2 Dedicated Hosts to run the job for full control over the\ninfrastructure and long-term cost efficiency",
    "chunk_id": 19,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Use EC2 On-Demand Instances to run the job each weekend for\n2 hours, paying only for the compute time consumed\nYour answer is correct\nUse EC2 Spot Instances to run the aggregation job, as they\nprovide substantial cost savings and can handle interruptions\nUse EC2 Reserved Instances with a 6-month commitment to\nensure lower costs and guaranteed availability\nOverall explanation\nCorrect option:\nUse EC2 Spot Instances to run the aggregation job, as they\nprovide substantial cost savings and can handle interruptions\nEC2 Spot Instances are the best fit for this scenario because they\nallow the company to:\nLeverage unused EC2 capacity at significantly reduced prices,\noffering up to 90% cost savings compared to On-Demand\npricing.\nRun the job in off-peak hours (Saturday night), reducing the\nlikelihood of interruptions.\nBenefit from the fact that the job tolerates interruptions, aligning\nperfectly with Spot Instances' use case.\nThis makes Spot Instances the most cost-effective solution for a",
    "chunk_id": 20,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Benefit from the fact that the job tolerates interruptions, aligning\nperfectly with Spot Instances' use case.\nThis makes Spot Instances the most cost-effective solution for a\nshort-duration, intermittent, and interruption-tolerant workload.\nIncorrect options:\nUse EC2 On-Demand Instances to run the job each weekend for\n2 hours, paying only for the compute time consumed - On-\nDemand Instances are more expensive compared to Spot\nInstances. While they provide guaranteed availability, the\nworkload can handle interruptions, making Spot Instances a\nbetter choice.\nUse EC2 Reserved Instances with a 6-month commitment to\nensure lower costs and guaranteed availability - Reserved\nInstances are suitable for long-term, predictable workloads that\nrequire guaranteed availability. The available term for Reserved\nInstances is 1 or 3 years. For a weekly job that is slated to run",
    "chunk_id": 21,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "only for 6 months, this approach would result in wasted costs for\nunused hours.\nUse EC2 Dedicated Hosts to run the job for full control over the\ninfrastructure and long-term cost efficiency - Dedicated Hosts\nare expensive and are typically used for workloads requiring\ndedicated physical servers due to compliance or licensing\nrequirements. They are not cost-effective for short-duration batch\njobs.\nReference:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/\ninstance-purchasing-options.html\nDomain\nDeployment and Orchestration of ML Workflows\nQuestion 3\nCorrect\nA healthcare company is training a neural network to classify\nmedical images as healthy or abnormal. The ML engineer\nobserves that the model's performance on the validation dataset\nimproves significantly during the early epochs but begins to\ndegrade after a certain number of epochs.\nWhich solutions will mitigate this problem? (Select two)\nYour selection is correct",
    "chunk_id": 22,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "improves significantly during the early epochs but begins to\ndegrade after a certain number of epochs.\nWhich solutions will mitigate this problem? (Select two)\nYour selection is correct\nAdd dropout layers to the neural network architecture to prevent\noverfitting\nIncrease the number of epochs to ensure the model learns the\ntraining data thoroughly before stopping\nIncrease the number of layers as well as the number of neurons\nto prevent overfitting\nReduce the size of the training dataset to simplify the learning\nprocess and avoid overfitting\nYour selection is correct",
    "chunk_id": 23,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Use early stopping to halt training once the validation loss stops\nimproving\nOverall explanation\nCorrect option:\nUse early stopping to halt training once the validation loss stops\nimproving\nEarly stopping monitors the performance of the model on the\nvalidation dataset and halts training when validation performance\nstops improving. This prevents overfitting by stopping training at\nthe optimal number of epochs.\nAdd dropout layers to the neural network architecture to prevent\noverfitting\nDropout layers randomly set a fraction of neurons to zero during\ntraining, reducing the model's reliance on specific neurons and\nencouraging it to learn more generalized patterns. This is a\nproven method to mitigate overfitting.",
    "chunk_id": 24,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://aws.amazon.com/what-is/overfitting/\nIncorrect options:\nIncrease the number of epochs to ensure the model learns the\ntraining data thoroughly before stopping - Increasing the number\nof epochs will exacerbate overfitting as the model will continue to\noptimize for the training data, further degrading performance on\nthe validation dataset.\nReduce the size of the training dataset to simplify the learning\nprocess and avoid overfitting - Reducing the size of the training\ndataset would likely decrease the model's ability to generalize, as\nit would have less data to learn from. This would worsen\noverfitting rather than mitigate it.\nIncrease the number of layers as well as the number of neurons\nto prevent overfitting - Increasing neurons and layers can lead to",
    "chunk_id": 25,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "overfitting if the model capacity exceeds the complexity of the\ntask. So, this option is ruled out.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/automatic-\nmodel-tuning-early-stopping.html\nhttps://aws.amazon.com/blogs/machine-learning/amazon-\nsagemaker-automatic-model-tuning-now-supports-early-\nstopping-of-training-jobs/\nhttps://aws.amazon.com/what-is/overfitting/\nDomain\nML Model Development\nQuestion 4\nCorrect\nYou are a data scientist at a financial institution tasked with\nbuilding a model to detect fraudulent transactions. The dataset is\nhighly imbalanced, with only a small percentage of transactions\nbeing fraudulent. After experimenting with several models, you\ndecide to implement a boosting technique to improve the\nmodel’s accuracy, particularly on the minority class. You are\nconsidering different types of boosting, including Adaptive\nBoosting (AdaBoost), Gradient Boosting, and Extreme Gradient\nBoosting (XGBoost).",
    "chunk_id": 26,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "considering different types of boosting, including Adaptive\nBoosting (AdaBoost), Gradient Boosting, and Extreme Gradient\nBoosting (XGBoost).\nGiven the problem context and the need to effectively handle\nclass imbalance, which boosting technique is MOST SUITABLE\nfor this scenario?\nUse Adaptive Boosting (AdaBoost) to focus on correcting the\nerrors of weak classifiers, giving more weight to incorrectly\nclassified instances during each iteration\nUse Gradient Boosting and manually adjust the learning rate and\nclass weights to improve performance on the minority class,\navoiding the complexities of XGBoost",
    "chunk_id": 27,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Implement Gradient Boosting to sequentially train weak learners,\nusing the gradient of the loss function to improve performance on\nthe minority class\nYour answer is correct\nApply Extreme Gradient Boosting (XGBoost) for its ability to\nhandle imbalanced datasets effectively through regularization,\nweighted classes, and optimized computational efficiency\nOverall explanation\nCorrect option:\nApply Extreme Gradient Boosting (XGBoost) for its ability to\nhandle imbalanced datasets effectively through regularization,\nweighted classes, and optimized computational efficiency\nThe XGBoost (eXtreme Gradient Boosting) is a popular and\nefficient open-source implementation of the gradient boosted\ntrees algorithm. Gradient boosting is a supervised learning\nalgorithm that tries to accurately predict a target variable by\ncombining multiple estimates from a set of simpler models. The\nXGBoost algorithm performs well in machine learning\ncompetitions for the following reasons:",
    "chunk_id": 28,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "combining multiple estimates from a set of simpler models. The\nXGBoost algorithm performs well in machine learning\ncompetitions for the following reasons:\nIts robust handling of a variety of data types, relationships,\ndistributions.\nThe variety of hyperparameters that you can fine-tune.\nXGBoost is an extension of Gradient Boosting that includes\nadditional features such as regularization, handling of missing\nvalues, and support for weighted classes, making it particularly\nwell-suited for imbalanced datasets like fraud detection. It also\noffers significant computational efficiency, which is beneficial\nwhen working with large datasets.",
    "chunk_id": 29,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://aws.amazon.com/what-is/boosting/\nIncorrect options:\nUse Adaptive Boosting (AdaBoost) to focus on correcting the\nerrors of weak classifiers, giving more weight to incorrectly\nclassified instances during each iteration - AdaBoost works by\nfocusing on correcting the errors of weak classifiers, assigning\nmore weight to misclassified instances in each iteration.\nHowever, it may struggle with noisy data and extreme class\nimbalance, as it can overemphasize hard-to-classify instances.\nImplement Gradient Boosting to sequentially train weak learners,\nusing the gradient of the loss function to improve performance on\nthe minority class - Gradient Boosting is a powerful technique\nthat uses the gradient of the loss function to improve the model\niteratively. While it can be adapted to handle class imbalance, it",
    "chunk_id": 30,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "does not inherently provide the same level of flexibility and\ncomputational optimization as XGBoost for this specific problem.\nUse Gradient Boosting and manually adjust the learning rate and\nclass weights to improve performance on the minority class,\navoiding the complexities of XGBoost - While manually adjusting\nthe learning rate and class weights in Gradient Boosting can\nhelp, XGBoost already provides built-in mechanisms to handle\nthese challenges more effectively, including advanced\nregularization techniques and hyperparameter optimization.\nReferences:\nhttps://aws.amazon.com/what-is/boosting/\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/\nxgboost.html\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/\nxgboost_hyperparameters.html\nhttps://aws.amazon.com/blogs/gametech/fraud-detection-for-\ngames-using-machine-learning/\nhttps://d1.awsstatic.com/events/reinvent/2019/\nREPEAT_1_Build_a_fraud_detection_system_with_Amazon_Sage\nMaker_AIM359-R1.pdf\nDomain\nML Model Development\nQuestion 5",
    "chunk_id": 31,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "games-using-machine-learning/\nhttps://d1.awsstatic.com/events/reinvent/2019/\nREPEAT_1_Build_a_fraud_detection_system_with_Amazon_Sage\nMaker_AIM359-R1.pdf\nDomain\nML Model Development\nQuestion 5\nCorrect\nA marketing analytics company is building a customer\nsegmentation model to identify groups of users based on their\npurchasing behavior and engagement. The dataset includes user\ndemographic data, transaction history, and website interaction\nlogs. The demographic data and transaction history are stored in\nAmazon S3, while website interaction logs are stored in an on-\npremises PostgreSQL database. The dataset includes a mix of\ncategorical features (e.g., \"region,\" \"customer tier\") and\nnumerical features (e.g., \"purchase amount,\" \"session duration\").\nTo improve the model's performance, the ML engineer must\ntransform and preprocess the data. The solution must minimize",
    "chunk_id": 32,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "operational overhead while ensuring the dataset is ready for\nmodel training.\nWhich solution will meet these requirements with the LEAST\noperational overhead?\nYour answer is correct\nUse Amazon SageMaker Data Wrangler to preprocess the data,\nincluding transforming categorical data into numerical data using\none-hot encoding and standardizing numerical features\nUse Amazon SageMaker Data Wrangler to preprocess the data,\nincluding transforming numerical data into categorical data using\none-hot encoding and standardizing numerical features\nUse Amazon Athena queries to manually transform categorical\ndata into numerical representations and store the output back\ninto S3\nUse AWS Glue ETL jobs to convert categorical data into\nnumerical format and apply custom transformations to the\ndataset\nOverall explanation\nCorrect option:\nUse Amazon SageMaker Data Wrangler to preprocess the data,\nincluding transforming categorical data into numerical data using",
    "chunk_id": 33,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "dataset\nOverall explanation\nCorrect option:\nUse Amazon SageMaker Data Wrangler to preprocess the data,\nincluding transforming categorical data into numerical data using\none-hot encoding and standardizing numerical features\nAmazon SageMaker Data Wrangler simplifies the process of data\npreprocessing and transformation for machine learning. It\nprovides a visual, no-code interface to:\nTransform categorical data: Convert it into numerical\nrepresentations using techniques like one-hot encoding or ordinal\nencoding. Process numerical data: Apply transformations like\nstandardization or normalization to ensure numerical features\ncontribute effectively to the model. Handle class imbalances:\nData Wrangler allows applying oversampling or undersampling\ntechniques. Data Wrangler integrates seamlessly with Amazon",
    "chunk_id": 34,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "S3, SageMaker pipelines, and other AWS services, reducing the\nneed for manual coding or operational overhead.\nKey Benefits:\nMinimal operational effort with a visual, interactive interface.\nBuilt-in transformations for categorical and numerical data.\nEasily exports transformed datasets to SageMaker for training.\nSageMaker Data Wrangler provides a data quality and insights\nreport that automatically verifies data quality (such as missing\nvalues, duplicate rows, and data types) and helps detect\nanomalies (such as outliers, class imbalance, and data leakage)\nin your data. SageMaker Data Wrangler offers over 300 prebuilt\nPySpark transformations and a natural language interface to\nprepare tabular, timeseries, text and image data without coding.\nCommon use cases such vectorize text, featurize datetime,\nencoding, balancing data, or image augmentation are covered.\nvia - https://aws.amazon.com/sagemaker-ai/data-wrangler/",
    "chunk_id": 35,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://aws.amazon.com/sagemaker-ai/data-wrangler/\nIncorrect options:\nUse AWS Glue ETL jobs to convert categorical data into\nnumerical format and apply custom transformations to the\ndataset - AWS Glue ETL jobs require writing custom scripts for\ndata transformations, increasing operational overhead. Glue is\nbetter suited for large-scale ETL rather than quick, visual feature\ntransformations.\nUse Amazon Athena queries to manually transform categorical\ndata into numerical representations and store the output back\ninto S3 - Athena is designed for querying data using SQL but\ndoes not support efficient preprocessing or feature\ntransformations like standardization and encoding.\nUse Amazon SageMaker Data Wrangler to preprocess the data,\nincluding transforming numerical data into categorical data using\none-hot encoding and standardizing numerical features - You\ncannot transform numerical data into categorical data using one-\nhot encoding. So this option is incorrect.\nReferences:",
    "chunk_id": 36,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "one-hot encoding and standardizing numerical features - You\ncannot transform numerical data into categorical data using one-\nhot encoding. So this option is incorrect.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/data-\nwrangler.html\nhttps://aws.amazon.com/sagemaker-ai/data-wrangler/\nDomain",
    "chunk_id": 37,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Data Preparation for Machine Learning (ML)\nQuestion 6\nCorrect\nYou are working as a data scientist at a financial services\ncompany tasked with developing a credit risk prediction model.\nAfter experimenting with several models, including logistic\nregression, decision trees, and support vector machines, you find\nthat none of the models individually achieves the desired level of\naccuracy and robustness. Your goal is to improve overall model\nperformance by combining these models in a way that leverages\ntheir strengths while minimizing their weaknesses.\nGiven the scenario, which of the following approaches is the\nMOST LIKELY to improve the model’s performance?\nUse bagging, where different types of models - logistic\nregression, decision trees, and support vector machines - are\ntrained on different subsets of the data, and their predictions are\naveraged to produce the final result\nImplement boosting by training sequentially different types of",
    "chunk_id": 38,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "trained on different subsets of the data, and their predictions are\naveraged to produce the final result\nImplement boosting by training sequentially different types of\nmodels - logistic regression, decision trees, and support vector\nmachines - where each new model corrects the errors of the\nprevious ones\nYour answer is correct\nApply stacking, where the predictions from logistic regression,\ndecision trees, and support vector machines are used as inputs\nto a meta-model, such as a random forest, to make the final\nprediction\nUse a simple voting ensemble, where the final prediction is based\non the majority vote from the logistic regression, decision tree,\nand support vector machine models\nOverall explanation\nCorrect option:",
    "chunk_id": 39,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Apply stacking, where the predictions from logistic regression,\ndecision trees, and support vector machines are used as inputs\nto a meta-model, such as a random forest, to make the final\nprediction\nvia - https://aws.amazon.com/blogs/machine-learning/efficiently-\ntrain-tune-and-deploy-custom-ensembles-using-amazon-\nsagemaker/\nIn bagging, data scientists improve the accuracy of weak\nlearners by training several of them at once on multiple datasets.\nIn contrast, boosting trains weak learners one after another.\nStacking involves training a meta-model on the predictions of\nseveral base models. This approach can significantly improve",
    "chunk_id": 40,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "performance because the meta-model learns to leverage the\nstrengths of each base model while compensating for their\nweaknesses.\nFor the given use case, leveraging a meta-model like a random\nforest can help capture the relationships between the predictions\nof logistic regression, decision trees, and support vector\nmachines.\nIncorrect options:\nUse a simple voting ensemble, where the final prediction is based\non the majority vote from the logistic regression, decision tree,\nand support vector machine models - A voting ensemble is a\nstraightforward way to combine models, and it can improve\nperformance. However, it typically does not capture the complex\ninteractions between models as effectively as stacking.\nImplement boosting by training sequentially different types of\nmodels - logistic regression, decision trees, and support vector\nmachines - where each new model corrects the errors of the\nprevious ones - Boosting is a powerful technique for improving",
    "chunk_id": 41,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "models - logistic regression, decision trees, and support vector\nmachines - where each new model corrects the errors of the\nprevious ones - Boosting is a powerful technique for improving\nmodel performance by training models sequentially, where each\nmodel focuses on correcting the errors of the previous one.\nHowever, it typically involves the same base model, such as\ndecision trees (e.g., XGBoost), rather than combining different\ntypes of models.\nUse bagging, where different types of models - logistic\nregression, decision trees, and support vector machines - are\ntrained on different subsets of the data, and their predictions are\naveraged to produce the final result - Bagging, like boosting, is\neffective for reducing variance and improving the stability of\nmodels, particularly for high-variance models like decision trees.\nHowever, it usually involves training multiple instances of the\nsame model type (e.g., decision trees in random forests) rather",
    "chunk_id": 42,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "models, particularly for high-variance models like decision trees.\nHowever, it usually involves training multiple instances of the\nsame model type (e.g., decision trees in random forests) rather\nthan combining different types of models.\nReferences:\nhttps://aws.amazon.com/blogs/machine-learning/efficiently-train-\ntune-and-deploy-custom-ensembles-using-amazon-sagemaker/\nhttps://aws.amazon.com/what-is/boosting/",
    "chunk_id": 43,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Domain\nML Model Development\nQuestion 7\nCorrect\nA retail company is developing a customer churn prediction\nmodel on AWS to identify customers likely to cancel their\nsubscriptions. The training dataset includes customer purchase\nhistory, support interaction logs, and subscription data. The\npurchase history and support logs are stored in Amazon S3,\nwhile the subscription data resides in an on-premises\nPostgreSQL database.\nThe dataset has two major challenges:\nA class imbalance where very few customers are labeled as\n\"churned\", impacting the model’s learning.\nThere are strong feature interdependencies among categorical\nfeatures (e.g., \"membership tier\") and numerical features (e.g.,\n\"purchase frequency\").\nThe ML engineer needs to select an Amazon SageMaker built-in\nalgorithm to train the model and address the challenges with the\nleast operational effort. Which algorithm should the ML engineer\nuse to meet these requirements?\nYour answer is correct\nAmazon SageMaker Linear Learner",
    "chunk_id": 44,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "least operational effort. Which algorithm should the ML engineer\nuse to meet these requirements?\nYour answer is correct\nAmazon SageMaker Linear Learner\nAmazon SageMaker Random Cut Forest (RCF)\nAmazon SageMaker K-Means\nAmazon SageMaker Neural Topic Model (NTM)\nOverall explanation\nCorrect option:\nAmazon SageMaker Linear Learner\nAmazon SageMaker Linear Learner is ideal for supervised\nlearning tasks like binary classification (e.g., churn prediction). It",
    "chunk_id": 45,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "is specifically designed to handle class imbalance by adjusting\nclass weights. Linear Learner ensures that the minority class\n(churned customers) is adequately represented during training. It\nminimizes operational effort as Linear Learner is straightforward\nto use, optimized for AWS, and requires less hyperparameter\ntuning compared to other complex algorithms.\nIncorrect options:\nAmazon SageMaker K-Means - K-Means is an unsupervised\nclustering algorithm that identifies groups but does not solve\nsupervised problems like predicting churn.\nAmazon SageMaker Random Cut Forest (RCF) - RCF is an\nunsupervised anomaly detection algorithm, not suitable for\nsupervised churn classification tasks.\nAmazon SageMaker Neural Topic Model (NTM) - Amazon\nSageMaker AI NTM is an unsupervised learning algorithm that is\nused to organize a corpus of documents into topics that contain\nword groupings based on their statistical distribution. It is not\nsuitable for supervised churn classification tasks.",
    "chunk_id": 46,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "used to organize a corpus of documents into topics that contain\nword groupings based on their statistical distribution. It is not\nsuitable for supervised churn classification tasks.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/linear-\nlearner.html\nhttps://community.aws/content/\n2eux4F6yvezbPPZFWnQbdHF9HwC/linear-learner-in-\nsagemaker-hyperparameter-tuning?lang=en\nDomain\nML Model Development\nQuestion 8\nIncorrect\nYou are an ML engineer at a startup that is developing a\nrecommendation engine for an e-commerce platform. The\nworkload involves training models on large datasets and\ndeploying them to serve real-time recommendations to\ncustomers. The training jobs are sporadic but require significant\ncomputational power, while the inference workloads must handle\nvarying traffic throughout the day. The company is cost-",
    "chunk_id": 47,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "conscious and aims to balance cost efficiency with the need for\nscalability and performance.\nGiven these requirements, which approach to resource allocation\nis the MOST SUITABLE for training and inference, and why?\nUse provisioned resources with reserved instances for both\ntraining and inference to lock in lower costs and guarantee\nresource availability, ensuring predictability in budgeting\nYour answer is incorrect\nUse provisioned resources with spot instances for both training\nand inference to take advantage of the lowest possible costs,\naccepting the potential for interruptions during workload\nexecution\nUse on-demand instances for both training and inference to\nensure that the company only pays for the compute resources it\nuses when it needs them, avoiding any upfront commitments\nCorrect answer\nUse on-demand instances for training, allowing the flexibility to\nscale resources as needed, and use provisioned resources with\nauto-scaling for inference to handle varying traffic while",
    "chunk_id": 48,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Use on-demand instances for training, allowing the flexibility to\nscale resources as needed, and use provisioned resources with\nauto-scaling for inference to handle varying traffic while\ncontrolling costs\nOverall explanation\nCorrect option:\nUse on-demand instances for training, allowing the flexibility to\nscale resources as needed, and use provisioned resources with\nauto-scaling for inference to handle varying traffic while\ncontrolling costs\nUsing on-demand instances for training offers flexibility, allowing\nyou to allocate resources only when needed, which is ideal for\nsporadic training jobs. For inference, provisioned resources with\nauto-scaling ensure that the system can handle varying traffic\nwhile controlling costs, as it can scale down during periods of\nlow demand.",
    "chunk_id": 49,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://aws.amazon.com/ec2/pricing/\nIncorrect options:\nUse on-demand instances for both training and inference to\nensure that the company only pays for the compute resources it\nuses when it needs them, avoiding any upfront commitments -\nOn-demand instances are flexible and ensure that you only pay\nfor what you use, but they can be more expensive over time\ncompared to provisioned resources, especially if workloads are\nconsistent and predictable. This approach may be suboptimal for\ncost-sensitive long-term use.\nUse provisioned resources with reserved instances for both\ntraining and inference to lock in lower costs and guarantee\nresource availability, ensuring predictability in budgeting -\nProvisioned resources with reserved instances provide cost\nsavings and guaranteed availability but lack the flexibility needed\nfor sporadic training jobs. For inference workloads with\nfluctuating demand, this approach might not handle traffic spikes",
    "chunk_id": 50,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "savings and guaranteed availability but lack the flexibility needed\nfor sporadic training jobs. For inference workloads with\nfluctuating demand, this approach might not handle traffic spikes\nefficiently without additional auto-scaling mechanisms.\nUse provisioned resources with spot instances for both training\nand inference to take advantage of the lowest possible costs,\naccepting the potential for interruptions during workload\nexecution - Spot instances provide significant cost savings but\ncome with the risk of interruptions, which can be problematic for\nboth training and real-time inference workloads. This option is\ngenerally better suited for non-critical batch jobs where\ninterruptions can be tolerated.\nReferences:\nhttps://aws.amazon.com/ec2/pricing/\nhttps://aws.amazon.com/ec2/pricing/reserved-instances/\nhttps://aws.amazon.com/ec2/spot/\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-\nauto-scaling-prerequisites.html\nDomain\nDeployment and Orchestration of ML Workflows",
    "chunk_id": 51,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "https://aws.amazon.com/ec2/spot/\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-\nauto-scaling-prerequisites.html\nDomain\nDeployment and Orchestration of ML Workflows\nQuestion 9",
    "chunk_id": 52,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Correct\nYou are tasked with building a predictive model for customer\nlifetime value (CLV) using Amazon SageMaker. Given the\ncomplexity of the model, it’s crucial to optimize hyperparameters\nto achieve the best possible performance. You decide to use\nSageMaker’s automatic model tuning (hyperparameter\noptimization) with Random Search strategy to fine-tune the\nmodel. You have a large dataset, and the tuning job involves\nseveral hyperparameters, including the learning rate, batch size,\nand dropout rate.\nDuring the tuning process, you observe that some of the trials are\nnot converging effectively, and the results are not as expected.\nYou suspect that the hyperparameter ranges or the strategy you\nare using may need adjustment.\nWhich of the following approaches is MOST LIKELY to improve\nthe effectiveness of the hyperparameter tuning process?\nYour answer is correct\nSwitch from the Random Search strategy to the Bayesian\nOptimization strategy and narrow the range of critical\nhyperparameters",
    "chunk_id": 53,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Your answer is correct\nSwitch from the Random Search strategy to the Bayesian\nOptimization strategy and narrow the range of critical\nhyperparameters\nIncrease the number of hyperparameters being tuned and widen\nthe range for all hyperparameters\nUse the Grid Search strategy with a wide range for all\nhyperparameters and increase the number of total trials\nDecrease the number of total trials but increase the number of\nparallel jobs to speed up the tuning process\nOverall explanation\nCorrect option:\nSwitch from the Random Search strategy to the Bayesian\nOptimization strategy and narrow the range of critical\nhyperparameters",
    "chunk_id": 54,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "When you’re training machine learning models, each dataset and\nmodel needs a different set of hyperparameters, which are a kind\nof variable. The only way to determine these is through multiple\nexperiments, where you pick a set of hyperparameters and run\nthem through your model. This is called hyperparameter tuning.\nIn essence, you're training your model sequentially with different\nsets of hyperparameters. This process can be manual, or you can\npick one of several automated hyperparameter tuning methods.\nBayesian Optimization is a technique based on Bayes’\ntheorem, which describes the probability of an event occurring\nrelated to current knowledge. When this is applied to\nhyperparameter optimization, the algorithm builds a probabilistic\nmodel from a set of hyperparameters that optimizes a specific\nmetric. It uses regression analysis to iteratively choose the best\nset of hyperparameters.\nRandom Search selects groups of hyperparameters randomly on",
    "chunk_id": 55,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "metric. It uses regression analysis to iteratively choose the best\nset of hyperparameters.\nRandom Search selects groups of hyperparameters randomly on\neach iteration. It works well when a relatively small number of the\nhyperparameters primarily determine the model outcome.\nBayesian Optimization is more efficient than Random Search\nfor hyperparameter tuning, especially when dealing with complex\nmodels and large hyperparameter spaces. It learns from previous\ntrials to predict the best set of hyperparameters, thus focusing\nthe search more effectively. Narrowing the range of critical\nhyperparameters can further improve the chances of finding the\noptimal values, leading to better model convergence and\nperformance.\nHow hyperparameter tuning with Amazon SageMaker works:",
    "chunk_id": 56,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://docs.aws.amazon.com/sagemaker/latest/dg/\nautomatic-model-tuning-how-it-works.html\nIncorrect options:\nIncrease the number of hyperparameters being tuned and widen\nthe range for all hyperparameters - Increasing the number of\nhyperparameters and widening the range without any strategic\napproach can lead to a more extensive search space, which\ncould cause the tuning process to become inefficient and less\nlikely to converge on optimal values.\nDecrease the number of total trials but increase the number of\nparallel jobs to speed up the tuning process - Reducing the total\nnumber of trials might speed up the tuning process, but it also\nreduces the chances of finding the best hyperparameters,\nespecially if the model is complex. Increasing parallel jobs can\nimprove throughput but doesn't necessarily enhance the quality\nof the search.",
    "chunk_id": 57,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Use the Grid Search strategy with a wide range for all\nhyperparameters and increase the number of total trials - Grid\nSearch works well, but it’s relatively tedious and computationally\nintensive, especially with large numbers of hyperparameters. It is\nless efficient than Bayesian Optimization for complex models.\nA wide range of hyperparameters without focus would result in\nmore trials, but it is not guaranteed to find the best values,\nespecially with a larger search space.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/automatic-\nmodel-tuning-how-it-works.html\nhttps://aws.amazon.com/what-is/hyperparameter-tuning/\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/automatic-\nmodel-tuning.html\nDomain\nML Model Development\nQuestion 10\nCorrect\nYou are a machine learning engineer at a fintech company tasked\nwith developing and deploying an end-to-end machine learning\nworkflow for fraud detection. The workflow involves multiple",
    "chunk_id": 58,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Correct\nYou are a machine learning engineer at a fintech company tasked\nwith developing and deploying an end-to-end machine learning\nworkflow for fraud detection. The workflow involves multiple\nsteps, including data extraction, preprocessing, feature\nengineering, model training, hyperparameter tuning, and\ndeployment. The company requires the solution to be scalable,\nsupport complex dependencies between tasks, and provide\nrobust monitoring and versioning capabilities. Additionally, the\nworkflow needs to integrate seamlessly with existing AWS\nservices.\nWhich deployment orchestrator is the MOST SUITABLE for\nmanaging and automating your ML workflow?\nYour answer is correct\nUse Amazon SageMaker Pipelines to orchestrate the entire ML\nworkflow, leveraging its built-in integration with SageMaker\nfeatures like training, tuning, and deployment",
    "chunk_id": 59,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Use AWS Lambda functions to manually trigger each step of the\nML workflow, enabling flexible execution without needing a\npredefined orchestration tool\nUse AWS Step Functions to build a serverless workflow that\nintegrates with SageMaker for model training and deployment,\nensuring scalability and fault tolerance\nUse Apache Airflow to define and manage the workflow with\ncustom DAGs (Directed Acyclic Graphs), integrating with AWS\nservices through operators and hooks\nOverall explanation\nCorrect option:\nUse Amazon SageMaker Pipelines to orchestrate the entire ML\nworkflow, leveraging its built-in integration with SageMaker\nfeatures like training, tuning, and deployment\nAmazon SageMaker Pipelines is a purpose-built workflow\norchestration service to automate machine learning (ML)\ndevelopment.\nSageMaker Pipelines is specifically designed for orchestrating\nML workflows. It provides native integration with SageMaker\nfeatures like model training, tuning, and deployment. It also",
    "chunk_id": 60,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "development.\nSageMaker Pipelines is specifically designed for orchestrating\nML workflows. It provides native integration with SageMaker\nfeatures like model training, tuning, and deployment. It also\nsupports versioning, lineage tracking, and automatic execution of\nworkflows, making it the ideal choice for managing end-to-end\nML workflows in AWS.\nvia - https://docs.aws.amazon.com/sagemaker/latest/dg/\npipelines.html\nIncorrect options:\nUse Apache Airflow to define and manage the workflow with\ncustom DAGs (Directed Acyclic Graphs), integrating with AWS\nservices through operators and hooks - Apache Airflow is a\npowerful orchestration tool that allows you to define complex\nworkflows using custom DAGs. However, it requires significant",
    "chunk_id": 61,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "setup and maintenance, and while it can integrate with AWS\nservices, it does not provide the seamless, built-in integration\nwith SageMaker that SageMaker Pipelines offers.\nAmazon Managed Workflows for Apache Airflow (Amazon\nMWAA):\nvia - https://aws.amazon.com/managed-workflows-for-apache-\nairflow/\nUse AWS Step Functions to build a serverless workflow that\nintegrates with SageMaker for model training and deployment,\nensuring scalability and fault tolerance - AWS Step Functions is a\nserverless orchestration service that can integrate with\nSageMaker and other AWS services. However, it is more general-\npurpose and lacks some of the ML-specific features, such as\nmodel lineage tracking and hyperparameter tuning, that are built\ninto SageMaker Pipelines.\nUse AWS Lambda functions to manually trigger each step of the\nML workflow, enabling flexible execution without needing a\npredefined orchestration tool - AWS Lambda is useful for\ntriggering specific tasks, but manually managing each step of a",
    "chunk_id": 62,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "ML workflow, enabling flexible execution without needing a\npredefined orchestration tool - AWS Lambda is useful for\ntriggering specific tasks, but manually managing each step of a\ncomplex ML workflow without a comprehensive orchestration\ntool is not scalable or maintainable. It does not provide the task\ndependency management, monitoring, and versioning required\nfor an end-to-end ML workflow.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/\npipelines.html",
    "chunk_id": 63,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "https://aws.amazon.com/managed-workflows-for-apache-\nairflow/\nDomain\nDeployment and Orchestration of ML Workflows\nQuestion 11\nIncorrect\nA manufacturing company is building an anomaly detection\nsystem to identify defective products in its production lines. The\ndatasets include sensor logs from IoT devices stored in Amazon\nS3 and a list of production metadata from an on-premises SQL\ndatabase.\nThe company must:\nAggregate and preprocess the data from multiple sources.\nImplement a solution to detect anomalies automatically in the\nsensor data.\nVisualize the results for analysis by the operations team.\nWhich solution will meet these requirements most efficiently?\nYour answer is incorrect\nUse Amazon Kinesis Data Streams to process and analyze the\nsensor data in real time for anomaly detection. Use Amazon\nQuickSight to visualize the results\nCorrect answer\nUse Amazon SageMaker Data Wrangler to aggregate, clean, and\nprepare data for anomaly detection while generating visual\ninsights",
    "chunk_id": 64,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "QuickSight to visualize the results\nCorrect answer\nUse Amazon SageMaker Data Wrangler to aggregate, clean, and\nprepare data for anomaly detection while generating visual\ninsights\nUse Amazon Athena to query the sensor data and identify\nanomalies through SQL queries. Use Amazon QuickSight to\nvisualize the results\nUse Amazon EMR with Spark MLlib to run anomaly detection\nalgorithms and visualize the results using custom dashboards\nOverall explanation",
    "chunk_id": 65,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Correct option:\nUse Amazon SageMaker Data Wrangler to aggregate, clean, and\nprepare data for anomaly detection while generating visual\ninsights\nAmazon SageMaker Data Wrangler is a managed service\ndesigned to simplify data aggregation, cleaning, and feature\nengineering for machine learning workflows. It connects\nseamlessly to data sources like Amazon S3 and SQL databases,\nallowing engineers to preprocess large datasets and detect\nanomalies efficiently. It also provides built-in visualizations for\nanalyzing trends, correlations, and outliers.\nKey Benefits:\nIntegrates with Amazon S3 and on-premises databases for data\naggregation.\nProvides built-in anomaly detection tools and visual insights for\nstreamlined analysis.\nSimplifies feature engineering and model input preparation.\nSageMaker Data Wrangler:\nvia - https://aws.amazon.com/sagemaker-ai/data-wrangler/\nIncorrect options:\nUse Amazon Kinesis Data Streams to process and analyze the",
    "chunk_id": 66,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "SageMaker Data Wrangler:\nvia - https://aws.amazon.com/sagemaker-ai/data-wrangler/\nIncorrect options:\nUse Amazon Kinesis Data Streams to process and analyze the\nsensor data in real time for anomaly detection. Use Amazon\nQuickSight to visualize the results - Amazon Kinesis Data\nStreams is designed for real-time data ingestion and streaming.\nWhile it can process large volumes of sensor data in real time, it",
    "chunk_id": 67,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "does not include built-in anomaly detection capabilities or\nsupport data aggregation and preprocessing for machine\nlearning workflows. Additional services like Amazon SageMaker\nor custom code would be required to analyze anomalies, thereby\nincreasing complexity for the solution.\nUse Amazon Athena to query the sensor data and identify\nanomalies through SQL queries. Use Amazon QuickSight to\nvisualize the results - Athena is a query service that allows\nrunning SQL queries directly on Amazon S3 data. While useful for\nad hoc analysis, it does not support automatic anomaly detection\nor data preprocessing, making it less suitable for this use case.\nUse Amazon EMR with Spark MLlib to run anomaly detection\nalgorithms and visualize the results using custom dashboards -\nWhile EMR with Spark MLlib is powerful for anomaly detection, it\nrequires significant manual configuration, coding, and operational\noverhead. Custom dashboards for visualization further add",
    "chunk_id": 68,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "While EMR with Spark MLlib is powerful for anomaly detection, it\nrequires significant manual configuration, coding, and operational\noverhead. Custom dashboards for visualization further add\ncomplexity compared to SageMaker Data Wrangler.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/data-\nwrangler.html\nhttps://aws.amazon.com/sagemaker-ai/data-wrangler/\nDomain\nData Preparation for Machine Learning (ML)\nQuestion 12\nIncorrect\nYou are a data scientist at a healthcare startup tasked with\ndeveloping a machine learning model to predict the likelihood of\npatients developing a specific chronic disease within the next five\nyears. The dataset available includes patient demographics,\nmedical history, lab results, and lifestyle factors, but it is relatively\nsmall, with only 1,000 records. Additionally, the dataset has\nmissing values in some critical features, and the class distribution\nis highly imbalanced, with only 5% of patients labeled as having\ndeveloped the disease.",
    "chunk_id": 69,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Given the data limitations and the complexity of the problem,\nwhich of the following approaches is the MOST LIKELY to\ndetermine the feasibility of an ML solution and guide your next\nsteps?\nYour answer is incorrect\nIncrease the dataset size by generating synthetic data and then\ntrain a simple logistic regression model to avoid overfitting\nProceed with training a deep neural network (DNN) model using\nthe available data, as DNNs can handle small datasets by\nlearning complex patterns\nCorrect answer\nConduct exploratory data analysis (EDA) to understand the data\ndistribution, address missing values, and assess the class\nimbalance before determining if an ML solution is feasible\nImmediately apply an oversampling technique to balance the\ndataset, then train an XGBoost model to maximize performance\non the minority class\nOverall explanation\nCorrect option:\nConduct exploratory data analysis (EDA) to understand the data\ndistribution, address missing values, and assess the class",
    "chunk_id": 70,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "on the minority class\nOverall explanation\nCorrect option:\nConduct exploratory data analysis (EDA) to understand the data\ndistribution, address missing values, and assess the class\nimbalance before determining if an ML solution is feasible\nConducting exploratory data analysis (EDA) is the most\nappropriate first step. EDA allows you to understand the data\ndistribution, identify and address missing values, and assess the\nextent of the class imbalance. This process helps determine\nwhether the available data is sufficient to build a reliable model\nand what preprocessing steps might be necessary.",
    "chunk_id": 71,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://aws.amazon.com/blogs/machine-learning/\nexploratory-data-analysis-feature-engineering-and-\noperationalizing-your-data-flow-into-your-ml-pipeline-with-\namazon-sagemaker-data-wrangler/\nIncorrect options:\nProceed with training a deep neural network (DNN) model using\nthe available data, as DNNs can handle small datasets by\nlearning complex patterns - Training a deep neural network on a\nsmall dataset is not advisable, as DNNs typically require large\namounts of data to perform well and avoid overfitting.\nAdditionally, jumping directly to model training without assessing\nthe data first may lead to poor results.\nIncrease the dataset size by generating synthetic data and then\ntrain a simple logistic regression model to avoid overfitting -\nWhile generating synthetic data can help increase the dataset\nsize, it may introduce biases if not done carefully. Additionally,\nwithout first understanding the data through EDA, you risk",
    "chunk_id": 72,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "While generating synthetic data can help increase the dataset\nsize, it may introduce biases if not done carefully. Additionally,\nwithout first understanding the data through EDA, you risk\napplying the wrong strategy or misinterpreting the results.\nImmediately apply an oversampling technique to balance the\ndataset, then train an XGBoost model to maximize performance\non the minority class - Although oversampling can address class\nimbalance, it’s important to first understand the underlying data\nissues through EDA. Oversampling should not be the immediate\nnext step without understanding the data quality, feature\nimportance, and potential need for feature engineering.\nReferences:\nhttps://aws.amazon.com/blogs/machine-learning/exploratory-\ndata-analysis-feature-engineering-and-operationalizing-your-\ndata-flow-into-your-ml-pipeline-with-amazon-sagemaker-data-\nwrangler/\nhttps://aws.amazon.com/blogs/machine-learning/use-amazon-\nsagemaker-canvas-for-exploratory-data-analysis/\nDomain",
    "chunk_id": 73,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "data-flow-into-your-ml-pipeline-with-amazon-sagemaker-data-\nwrangler/\nhttps://aws.amazon.com/blogs/machine-learning/use-amazon-\nsagemaker-canvas-for-exploratory-data-analysis/\nDomain\nML Model Development\nQuestion 13",
    "chunk_id": 74,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Correct\nYou are a data scientist working for a financial institution that\nuses a machine learning model to predict loan defaults. The\nmodel was trained on historical data from the past five years, but\nafter being deployed for several months, its accuracy has\ngradually decreased. Upon investigation, you suspect that the\nunderlying data distribution has changed due to economic shifts\nand changes in customer behavior. This phenomenon is known\nas model drift, and you need to address it to ensure the model\ncontinues to perform well.\nWhich of the following approaches would you combine for\ndetecting and managing drift in your ML model? (Select two)\nDeploy a secondary model trained on different data and compare\nits predictions with the original model to detect any significant\ndifferences, indicating potential drift\nIncrease the complexity of the model by adding more features\nand deeper layers, ensuring it can adapt to changing data\ndistributions over time",
    "chunk_id": 75,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "differences, indicating potential drift\nIncrease the complexity of the model by adding more features\nand deeper layers, ensuring it can adapt to changing data\ndistributions over time\nDecrease the complexity of the model by removing features and\nlayers, thereby turning it into a simpler model that can various\ntypes of data distributions\nYour selection is correct\nRetrain the model on the most recent data to ensure it captures\ncurrent trends, and use model versioning to track performance\nimprovements over time\nYour selection is correct\nImplement continuous monitoring of input data features and\nmodel predictions using statistical tests to detect shifts in data\ndistribution or performance, triggering an alert when drift is\ndetected\nOverall explanation",
    "chunk_id": 76,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Correct options:\nImplement continuous monitoring of input data features and\nmodel predictions using statistical tests to detect shifts in data\ndistribution or performance, triggering an alert when drift is\ndetected\nRetrain the model on the most recent data to ensure it captures\ncurrent trends, and use model versioning to track performance\nimprovements over time\nFor a model to predict accurately, the data that it is making\npredictions on must have a similar distribution as the data on\nwhich the model was trained. Because data distributions can be\nexpected to drift over time, deploying a model is not a one-time\nexercise but rather a continuous process. It is a good practice to\ncontinuously monitor the incoming data and retrain your model\non newer data if you find that the data distribution has deviated\nsignificantly from the original training data distribution. If\nmonitoring data to detect a change in the data distribution has a",
    "chunk_id": 77,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "on newer data if you find that the data distribution has deviated\nsignificantly from the original training data distribution. If\nmonitoring data to detect a change in the data distribution has a\nhigh overhead, then a simpler strategy is to retrain the model\nperiodically, for example, daily, weekly, or monthly.",
    "chunk_id": 78,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://aws.amazon.com/blogs/machine-learning/automate-\nmodel-retraining-with-amazon-sagemaker-pipelines-when-drift-\nis-detected/\nIncorrect options:",
    "chunk_id": 79,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Deploy a secondary model trained on different data and compare\nits predictions with the original model to detect any significant\ndifferences, indicating potential drift - Comparing predictions\nfrom a secondary model might indicate drift, but it’s not a robust\nor scalable solution. It requires maintaining multiple models,\nwhich can be resource-intensive and complex to manage.\nIncrease the complexity of the model by adding more features\nand deeper layers, ensuring it can adapt to changing data\ndistributions over time - Increasing model complexity may help\ncapture more nuances in the data, but it does not address drift\ndirectly. In fact, a more complex model could exacerbate issues\nrelated to overfitting or make the model more sensitive to minor\nchanges in data distribution.\nDecrease the complexity of the model by removing features and\nlayers, thereby turning it into a simpler model that can various\ntypes of data distributions - Decreasing the model complexity",
    "chunk_id": 80,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Decrease the complexity of the model by removing features and\nlayers, thereby turning it into a simpler model that can various\ntypes of data distributions - Decreasing the model complexity\nand turning the model into a simpler one would introduce more\nbias into the model, thereby rendering it ineffective to address\nchanges in the data distribution.\nReferences:\nhttps://docs.aws.amazon.com/machine-learning/latest/dg/\nretraining-models-on-new-data.html\nhttps://aws.amazon.com/blogs/machine-learning/automate-\nmodel-retraining-with-amazon-sagemaker-pipelines-when-drift-\nis-detected/\nDomain\nML Solution Monitoring, Maintenance, and Security\nQuestion 14\nCorrect\nYou are a lead machine learning engineer at a growing tech\nstartup that is developing a recommendation system for a mobile\napp. The recommendation engine must be able to scale quickly\nas the user base grows, remain cost-effective to align with the\nstartup’s budget constraints, and be easy to maintain by a small",
    "chunk_id": 81,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "app. The recommendation engine must be able to scale quickly\nas the user base grows, remain cost-effective to align with the\nstartup’s budget constraints, and be easy to maintain by a small\nteam of engineers. The company has decided to use AWS for the",
    "chunk_id": 82,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "ML infrastructure. Your goal is to design an infrastructure that\nmeets these needs, ensuring that it can handle rapid scaling,\nremains within budget, and is simple to update and monitor.\nWhich combination of practices and AWS services is MOST\nLIKELY to result in a maintainable, scalable, and cost-effective\nML infrastructure?\nYour answer is correct\nUse Amazon SageMaker for both training and deployment,\nleverage auto-scaling endpoints for real-time inference, and\napply SageMaker Pipelines for orchestrating end-to-end ML\nworkflows, ensuring scalability and automation\nUse Amazon SageMaker for training, deploy models on Amazon\nECS for flexible scaling, and implement infrastructure monitoring\nwith a combination of CloudWatch and AWS Systems Manager\nto ensure maintainability\nTrain models using Amazon EMR for cost efficiency, deploy the\nmodels using AWS Lambda for serverless inference, and\nmanually monitor the system using CloudWatch to reduce\noperational overhead",
    "chunk_id": 83,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Train models using Amazon EMR for cost efficiency, deploy the\nmodels using AWS Lambda for serverless inference, and\nmanually monitor the system using CloudWatch to reduce\noperational overhead\nImplement Amazon SageMaker for model training, deploy the\nmodels using Amazon EC2 with manual scaling to handle\ninference, and use AWS CloudFormation for managing\ninfrastructure as code to ensure repeatability\nOverall explanation\nCorrect option:\nUse Amazon SageMaker for both training and deployment,\nleverage auto-scaling endpoints for real-time inference, and\napply SageMaker Pipelines for orchestrating end-to-end ML\nworkflows, ensuring scalability and automation\nAmazon SageMaker provides a managed service for both training\nand deployment, which simplifies the infrastructure and reduces\noperational overhead. Auto-scaling endpoints in SageMaker",
    "chunk_id": 84,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "ensure the system can handle increasing demand without\nmanual intervention. SageMaker Pipelines automates the entire\nML workflow, enabling continuous integration and delivery (CI/\nCD) practices, making the infrastructure scalable, maintainable,\nand cost-effective.\nIncorrect options:\nImplement Amazon SageMaker for model training, deploy the\nmodels using Amazon EC2 with manual scaling to handle\ninference, and use AWS CloudFormation for managing\ninfrastructure as code to ensure repeatability - Using Amazon\nSageMaker for training and Amazon EC2 for inference with\nmanual scaling can work, but it requires more effort to manage\nscaling, and manually managing infrastructure is less\nmaintainable. Auto-scaling and automation would be more\neffective for a growing startup.\nTrain models using Amazon EMR for cost efficiency, deploy the\nmodels using AWS Lambda for serverless inference, and\nmanually monitor the system using CloudWatch to reduce",
    "chunk_id": 85,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Train models using Amazon EMR for cost efficiency, deploy the\nmodels using AWS Lambda for serverless inference, and\nmanually monitor the system using CloudWatch to reduce\noperational overhead - While Amazon EMR is cost-effective for\nbig data processing, it’s not optimized for ML model training in\nthe same way that SageMaker is. AWS Lambda is useful for\nserverless inference but may not scale effectively for high-\nvolume, real-time recommendations. Manual monitoring adds\noperational overhead.\nUse Amazon SageMaker for training, deploy models on Amazon\nECS for flexible scaling, and implement infrastructure monitoring\nwith a combination of CloudWatch and AWS Systems Manager\nto ensure maintainability - Amazon ECS offers flexible scaling,\nbut SageMaker’s auto-scaling capabilities and built-in integration\nwith ML workflows make it more suitable for this use case.\nAdditionally, SageMaker Pipelines offers better orchestration for\nML tasks compared to a manually managed solution.\nReferences:",
    "chunk_id": 86,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "with ML workflows make it more suitable for this use case.\nAdditionally, SageMaker Pipelines offers better orchestration for\nML tasks compared to a manually managed solution.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-\nauto-scaling-prerequisites.html",
    "chunk_id": 87,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "https://docs.aws.amazon.com/sagemaker/latest/dg/\npipelines.html\nDomain\nDeployment and Orchestration of ML Workflows\nQuestion 15\nCorrect\nWhich of the following strategies best aligns with the defense-in-\ndepth security approach for generative AI applications on AWS?\nRelying solely on data encryption to protect the AI training data\nImplementing a single-layer firewall to block unauthorized access\nto the AI models\nYour answer is correct\nApplying multiple layers of security measures including input\nvalidation, access controls, and continuous monitoring to\naddress vulnerabilities\nUsing a single authentication mechanism for all users and\nservices accessing the AI models\nOverall explanation\nCorrect option:\nApplying multiple layers of security measures including input\nvalidation, access controls, and continuous monitoring to\naddress vulnerabilities\nArchitecting a defense-in-depth security approach involves\nimplementing multiple layers of security to protect generative AI",
    "chunk_id": 88,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "address vulnerabilities\nArchitecting a defense-in-depth security approach involves\nimplementing multiple layers of security to protect generative AI\napplications. This includes input validation to prevent malicious\ndata inputs, strict access controls to limit who can interact with\nthe AI models, and continuous monitoring to detect and respond\nto security incidents. These measures can help address common\nvulnerabilities and meet the best practices for securing\ngenerative AI applications on AWS.\nIncorrect options:",
    "chunk_id": 89,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Implementing a single-layer firewall to block unauthorized access\nto the AI models - While a firewall is an important security\nmeasure, relying on a single layer of defense is insufficient for\ncomprehensive security. Defense-in-depth requires multiple,\noverlapping layers of protection.\nRelying solely on data encryption to protect the AI training data -\nData encryption is crucial for protecting data at rest and in\ntransit, but it does not address other vulnerabilities such as input\nvalidation or unauthorized access. A holistic security strategy is\nneeded.\nUsing a single authentication mechanism for all users and\nservices accessing the AI models - Employing a single\nauthentication mechanism is a weak security practice. Multiple\nauthentication and authorization mechanisms should be used to\nensure robust access control.\nReference:\nhttps://aws.amazon.com/blogs/machine-learning/architect-\ndefense-in-depth-security-for-generative-ai-applications-using-\nthe-owasp-top-10-for-llms/\nDomain",
    "chunk_id": 90,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "ensure robust access control.\nReference:\nhttps://aws.amazon.com/blogs/machine-learning/architect-\ndefense-in-depth-security-for-generative-ai-applications-using-\nthe-owasp-top-10-for-llms/\nDomain\nML Solution Monitoring, Maintenance, and Security\nQuestion 16\nIncorrect\nA fintech company is developing an AI-driven fraud detection\nsystem using Amazon SageMaker. The system must provide\nend-to-end ML capabilities, including data preprocessing, model\ntraining, versioned model storage, deployment, and monitoring.\nCustomer transaction data is stored securely in Amazon S3.\nThe company requires the following:\nSecure access to training data for different ML workflows to\nensure data isolation.\nA centralized model registry to manage model versions and\ndeployments with minimal operational overhead.\nWhat is the most appropriate approach to meet these\nrequirements with the LEAST operational overhead?",
    "chunk_id": 91,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Use Amazon Elastic Container Registry (Amazon ECR) to store\nmodel artifacts and versions\nUse unique tags for each model version and use SageMaker\nModel Registry to manage model deployments\nCorrect answer\nUse SageMaker Model Registry to manage model versions and\nassociate models with model groups\nYour answer is incorrect\nUse Amazon S3 versioning to track model artifacts and manage\ndeployments manually\nOverall explanation\nCorrect option:\nUse SageMaker Model Registry to manage model versions and\nassociate models with model groups\nAmazon SageMaker Model Registry is purpose-built for storing,\nversioning, and managing ML models as part of the SageMaker\necosystem. It allows users to organize models into model groups,\nwhich act as containers for different versions of a model. This\nfeature integrates seamlessly with SageMaker workflows like\ntraining, deployment, and monitoring, reducing operational\noverhead while maintaining robust version control.\nBy using SageMaker Model Registry:",
    "chunk_id": 92,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "training, deployment, and monitoring, reducing operational\noverhead while maintaining robust version control.\nBy using SageMaker Model Registry:\nDifferent versions of a model can be tracked and managed\nefficiently.\nSecure and isolated use of training data can be implemented by\nleveraging IAM roles and SageMaker training jobs.\nDeployment pipelines can be automated using SageMaker\nendpoints, further reducing manual intervention.\nModel Registry Models, Model Versions, and Model Groups:",
    "chunk_id": 93,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://docs.aws.amazon.com/sagemaker/latest/dg/model-\nregistry.html\nIncorrect options:\nUse Amazon Elastic Container Registry (Amazon ECR) to store\nmodel artifacts and versions - Amazon ECR is designed for\nstoring and managing container images, not directly for ML\nmodel versioning. While SageMaker models might use Docker\ncontainers stored in ECR for deployment, ECR does not provide\ncapabilities like tracking model metadata, associating models\nwith model groups, or automating versioning. This would add\noperational overhead.\nUse unique tags for each model version and use SageMaker\nModel Registry to manage model deployments - While unique\ntags can help organize models, SageMaker Model Registry\nalready provides built-in capabilities for versioning and organizing\nmodels through model groups. Adding tags would add\nunnecessary complexity and does not add significant benefits\nover using model groups.",
    "chunk_id": 94,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Use Amazon S3 versioning to track model artifacts and manage\ndeployments manually - Amazon S3 versioning can track\ndifferent versions of objects (artifacts), but it does not offer a\ncentralized model registry or automated ML lifecycle\nmanagement. Managing deployments manually increases\noverhead, making this approach inefficient.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/model-\nregistry.html\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/model-\nregistry-deploy.html\nDomain\nML Solution Monitoring, Maintenance, and Security\nQuestion 17\nCorrect\nA retail company is building a web-based AI application using\nAmazon SageMaker to predict customer purchase behavior. The\nsystem must support full ML lifecycle features such as\nexperimentation, training, centralized model registry, deployment,\nand monitoring. The training data is securely stored in Amazon\nS3, and the models need to be deployed to real-time endpoints\nto serve predictions. The company is now planning to run an on-",
    "chunk_id": 95,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "and monitoring. The training data is securely stored in Amazon\nS3, and the models need to be deployed to real-time endpoints\nto serve predictions. The company is now planning to run an on-\ndemand workflow to monitor for bias drift in the deployed models\nto ensure fairness and accuracy in predictions.\nWhat do you recommend?\nUse AWS Glue Data Quality to validate and monitor data quality\nfor detecting bias drift in real-time endpoints\nUse Amazon SageMaker Lineage Tracking to trace and analyze\nbias drift in model predictions\nYour answer is correct\nUse Amazon SageMaker Clarify to analyze captured inference\ndata from real-time endpoints for bias detection on demand",
    "chunk_id": 96,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Use the sagemaker-model-monitor-analyzer built-in SageMaker\nimage to automatically analyze and resolve bias drift in deployed\nmodels\nOverall explanation\nCorrect option:\nUse Amazon SageMaker Clarify to analyze captured inference\ndata from real-time endpoints for bias detection on demand\nAmazon SageMaker Clarify can analyze bias in the input and\noutput data captured from SageMaker real-time endpoints. By\nenabling data capture in SageMaker endpoints and integrating\nClarify, you can run on-demand bias analysis workflows to\nidentify and measure bias drift.\nThis solution leverages Clarify’s post-deployment bias detection\ncapabilities, allowing the company to analyze inference data over\ntime and ensure the model maintains fairness in production.\nKey Steps:\nEnable data capture for real-time endpoints.\nUse SageMaker Clarify to analyze the captured data for bias.\nGenerate detailed reports to detect and quantify bias drift.",
    "chunk_id": 97,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-\nmodel-monitor-bias-drift.html\nIncorrect options:\nUse Amazon SageMaker Lineage Tracking to trace and analyze\nbias drift in model predictions - SageMaker Lineage Tracking is\ndesigned to track the lineage of ML artifacts, such as datasets,\nmodels, and experiments. While useful for auditability and",
    "chunk_id": 98,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "reproducibility, it does not perform bias drift analysis or real-time\nmodel monitoring.\nUse AWS Glue Data Quality to validate and monitor data quality\nfor detecting bias drift in real-time endpoints - AWS Glue Data\nQuality is designed to evaluate and enforce data quality rules on\ndatasets processed through AWS Glue ETL jobs. While it ensures\ninput data meets quality thresholds during extraction,\ntransformation, and loading, it does not provide bias drift\nmonitoring for deployed machine learning models at real-time\nendpoints.\nUse the sagemaker-model-monitor-analyzer built-in SageMaker\nimage to automatically analyze and resolve bias drift in deployed\nmodels - The sagemaker-model-monitor-analyzer built-in image\nis used as part of Amazon SageMaker Model Monitor to run\ncustom monitoring jobs, such as analyzing data captured from\nendpoints for drift, bias, or feature issues. However, this built-in\nimage is only used for analysis and reporting purposes. It does",
    "chunk_id": 99,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "custom monitoring jobs, such as analyzing data captured from\nendpoints for drift, bias, or feature issues. However, this built-in\nimage is only used for analysis and reporting purposes. It does\nnot automatically resolve or fix bias drift.\nReference:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/clarify-\nmodel-monitor-bias-drift.html\nDomain\nML Solution Monitoring, Maintenance, and Security\nQuestion 18\nCorrect\nYou are a data scientist at an insurance company developing a\nmachine learning model to predict the likelihood of claims being\nfraudulent. The company has a strong commitment to fairness\nand wants to ensure that the model does not disproportionately\naffect any specific demographic group. You decide to use\nAmazon SageMaker Clarify to assess potential bias in your\nmodel. In particular, you are interested in understanding how the\nmodel’s predictions differ across demographic groups when\nconditioned on relevant factors like income level, which could",
    "chunk_id": 100,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "model. In particular, you are interested in understanding how the\nmodel’s predictions differ across demographic groups when\nconditioned on relevant factors like income level, which could\ninfluence the likelihood of fraudulent claims.",
    "chunk_id": 101,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Given this scenario, which of the following BEST describes how\nConditional Demographic Disparity (CDD) can be used to assess\nand mitigate bias in your model?\nCDD focuses on the relationship between feature importance and\ndemographic groups, highlighting whether certain features\ndisproportionately influence predictions for specific groups\nCDD measures the difference in average predicted outcomes\nbetween demographic groups, helping to identify overall bias\nwithout considering other factors\nCDD assesses the proportion of correctly predicted outcomes for\neach demographic group, helping to ensure that the model is\nequally accurate across groups\nYour answer is correct\nCDD evaluates the disparity in positive prediction rates across\ndemographic groups, conditioned on a specific feature like\nincome, to detect bias that may not be apparent when only\nconsidering overall outcomes\nOverall explanation\nCorrect option:\nCDD evaluates the disparity in positive prediction rates across",
    "chunk_id": 102,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "income, to detect bias that may not be apparent when only\nconsidering overall outcomes\nOverall explanation\nCorrect option:\nCDD evaluates the disparity in positive prediction rates across\ndemographic groups, conditioned on a specific feature like\nincome, to detect bias that may not be apparent when only\nconsidering overall outcomes\nConditional Demographic Disparity (CDD) measures the\ndifference in positive prediction rates between demographic\ngroups, while conditioning on relevant features like income. This\nallows you to identify subtle biases that might be masked when\nlooking only at overall predictions, ensuring that the model's\ndecisions are fair across different groups given their specific\ncircumstances.",
    "chunk_id": 103,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-\ndata-bias-metric-cddl.html\nIncorrect options:\nCDD measures the difference in average predicted outcomes\nbetween demographic groups, helping to identify overall bias\nwithout considering other factors - This describes a general\nmeasure of demographic disparity but does not account for\nconditioning on other relevant features. CDD specifically\nconditions on features like income to assess disparity more\naccurately.\nCDD assesses the proportion of correctly predicted outcomes for\neach demographic group, helping to ensure that the model is\nequally accurate across groups - This describes accuracy\nmetrics rather than Conditional Demographic Disparity. CDD is\nfocused on measuring differences in prediction rates, not the\ncorrectness of predictions.\nCDD focuses on the relationship between feature importance and\ndemographic groups, highlighting whether certain features\ndisproportionately influence predictions for specific groups - This",
    "chunk_id": 104,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "CDD focuses on the relationship between feature importance and\ndemographic groups, highlighting whether certain features\ndisproportionately influence predictions for specific groups - This\noption describes feature importance analysis rather than CDD.",
    "chunk_id": 105,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "While understanding the influence of features on predictions is\nimportant, CDD specifically examines disparities conditioned on\ncertain features.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/clarify-data-\nbias-metric-cddl.html\nhttps://aws.amazon.com/blogs/machine-learning/learn-how-\namazon-sagemaker-clarify-helps-detect-bias/\nDomain\nML Model Development\nQuestion 19\nCorrect\nYou are a data scientist at a retail company responsible for\ndeploying a machine learning model that predicts customer\npurchase behavior. The model needs to serve real-time\npredictions with low latency to support the company’s\nrecommendation engine on its e-commerce platform. The\ndeployment solution must also be scalable to handle varying\ntraffic loads during peak shopping periods, such as Black Friday\nand holiday sales. Additionally, you need to monitor the model's\nperformance and automatically roll out updates when a new\nversion of the model is available.",
    "chunk_id": 106,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "and holiday sales. Additionally, you need to monitor the model's\nperformance and automatically roll out updates when a new\nversion of the model is available.\nGiven these requirements, which AWS deployment service and\nconfiguration is the MOST SUITABLE for deploying the machine\nlearning model?\nDeploy the model on Amazon EC2 instances with a load balancer\nto distribute traffic, manually scaling the instances based on\nexpected traffic during peak periods\nUse AWS Lambda to deploy the model as a serverless function,\nautomatically scaling based on the number of requests, and store\nthe model artifacts in Amazon S3\nYour answer is correct",
    "chunk_id": 107,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Deploy the model using Amazon SageMaker real-time hosting\nservices with an auto-scaling endpoint, enabling you to\nautomatically adjust the number of instances based on traffic\ndemand\nDeploy the model on Amazon SageMaker with batch transform\njobs, running the jobs periodically to generate predictions and\nstoring the results in Amazon S3 for the recommendation engine\nOverall explanation\nCorrect option:\nDeploy the model using Amazon SageMaker real-time hosting\nservices with an auto-scaling endpoint, enabling you to\nautomatically adjust the number of instances based on traffic\ndemand\nAmazon SageMaker hosting services provide a managed\nenvironment for deploying models in real-time, with support for\nauto-scaling based on traffic. This ensures low latency and\nscalability during peak periods, making it ideal for an e-\ncommerce platform with fluctuating traffic. SageMaker also offers\nmonitoring and versioning capabilities, allowing you to manage\nmodel updates efficiently.",
    "chunk_id": 108,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "commerce platform with fluctuating traffic. SageMaker also offers\nmonitoring and versioning capabilities, allowing you to manage\nmodel updates efficiently.\nReal-time inference is ideal for inference workloads where you\nhave real-time, interactive, low latency requirements. You can\ndeploy your model to SageMaker hosting services and get an\nendpoint that can be used for inference. These endpoints are\nfully managed and support autoscaling.",
    "chunk_id": 109,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://docs.aws.amazon.com/sagemaker/latest/dg/\npipelines.html\nIncorrect options:\nDeploy the model on Amazon EC2 instances with a load balancer\nto distribute traffic, manually scaling the instances based on\nexpected traffic during peak periods - Deploying on Amazon EC2\nwith a load balancer can work, but it requires manual scaling,\nwhich may not be responsive enough to sudden traffic spikes. It\nalso lacks the integrated monitoring and management features\nprovided by SageMaker.\nUse AWS Lambda to deploy the model as a serverless function,\nautomatically scaling based on the number of requests, and store\nthe model artifacts in Amazon S3 - AWS Lambda is suitable for\nserverless deployments with lightweight models and use cases\nwhere request volume is unpredictable. However, it may not be\nideal for high-throughput, low-latency applications like real-time\nrecommendation engines, especially if the model is large or\nrequires significant compute resources.",
    "chunk_id": 110,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "ideal for high-throughput, low-latency applications like real-time\nrecommendation engines, especially if the model is large or\nrequires significant compute resources.\nDeploy the model on Amazon SageMaker with batch transform\njobs, running the jobs periodically to generate predictions and\nstoring the results in Amazon S3 for the recommendation engine\n- Batch transform jobs in SageMaker are designed for batch",
    "chunk_id": 111,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "predictions rather than real-time inference. While this approach\nworks for generating predictions in bulk, it does not meet the\nrequirement for real-time, low-latency predictions needed by the\nrecommendation engine.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/how-it-\nworks-deployment.html\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/realtime-\nendpoints.html\nDomain\nDeployment and Orchestration of ML Workflows\nQuestion 20\nIncorrect\nYou are a data scientist at a financial technology company\ndeveloping a fraud detection system. The system needs to\nidentify fraudulent transactions in real-time based on patterns in\ntransaction data, including amounts, locations, times, and\naccount histories. The dataset is large and highly imbalanced,\nwith only a small percentage of transactions labeled as\nfraudulent. Your team has access to Amazon SageMaker and is\nconsidering various built-in algorithms to build the model.\nGiven the need for both high accuracy and the ability to handle",
    "chunk_id": 112,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "fraudulent. Your team has access to Amazon SageMaker and is\nconsidering various built-in algorithms to build the model.\nGiven the need for both high accuracy and the ability to handle\nimbalanced data, which SageMaker built-in algorithm is the\nMOST SUITABLE for this use case?\nCorrect answer\nApply the XGBoost algorithm with a custom objective function to\noptimize for precision and recall\nImplement the K-Nearest Neighbors (k-NN) algorithm to classify\ntransactions based on similarity to known fraudulent cases\nSelect the Random Cut Forest (RCF) algorithm for its ability to\ndetect anomalies in transaction data\nYour answer is incorrect",
    "chunk_id": 113,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Use the Linear Learner algorithm with weighted classification to\naddress the class imbalance\nOverall explanation\nCorrect option:\nApply the XGBoost algorithm with a custom objective function to\noptimize for precision and recall\nThe XGBoost (eXtreme Gradient Boosting) is a popular and\nefficient open-source implementation of the gradient boosted\ntrees algorithm. Gradient boosting is a supervised learning\nalgorithm that tries to accurately predict a target variable by\ncombining multiple estimates from a set of simpler models. The\nXGBoost algorithm performs well in machine learning\ncompetitions for the following reasons:\nIts robust handling of a variety of data types, relationships,\ndistributions.\nThe variety of hyperparameters that you can fine-tune.\nXGBoost is a powerful gradient boosting algorithm that excels in\nstructured data problems, such as fraud detection. It allows for\ncustom objective functions, making it highly suitable for",
    "chunk_id": 114,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "XGBoost is a powerful gradient boosting algorithm that excels in\nstructured data problems, such as fraud detection. It allows for\ncustom objective functions, making it highly suitable for\noptimizing precision and recall, which are critical in imbalanced\ndatasets. Additionally, XGBoost has built-in techniques for\nhandling class imbalance, such as scale_pos_weight.\nIncorrect options:\nUse the Linear Learner algorithm with weighted classification to\naddress the class imbalance - The Linear Learner algorithm can\nhandle classification tasks, and weighting classes can help with\nimbalance. However, it may not be as effective in capturing\ncomplex patterns in the data as more sophisticated algorithms\nlike XGBoost.\nSelect the Random Cut Forest (RCF) algorithm for its ability to\ndetect anomalies in transaction data - Random Cut Forest (RCF)\nis designed for anomaly detection, which can be relevant for\nfraud detection. However, RCF is unsupervised and may not",
    "chunk_id": 115,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "detect anomalies in transaction data - Random Cut Forest (RCF)\nis designed for anomaly detection, which can be relevant for\nfraud detection. However, RCF is unsupervised and may not\nleverage the labeled data effectively, leading to suboptimal\nresults in a supervised classification task like this.",
    "chunk_id": 116,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Implement the K-Nearest Neighbors (k-NN) algorithm to classify\ntransactions based on similarity to known fraudulent cases - K-\nNearest Neighbors (k-NN) can classify based on similarity, but it\ndoes not scale well with large datasets and may struggle with the\nhigh-dimensional, imbalanced nature of the data in this context.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/\nxgboost.html\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/\nxgboost_hyperparameters.html\nhttps://aws.amazon.com/blogs/gametech/fraud-detection-for-\ngames-using-machine-learning/\nhttps://d1.awsstatic.com/events/reinvent/2019/\nREPEAT_1_Build_a_fraud_detection_system_with_Amazon_Sage\nMaker_AIM359-R1.pdf\nDomain\nML Model Development\nQuestion 21\nCorrect\nWhich benefits might persuade a developer to choose a\ntransparent and explainable machine learning model? (Select\ntwo)\nThey require less computational power and storage\nYour selection is correct\nThey foster trust and confidence in model predictions",
    "chunk_id": 117,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "transparent and explainable machine learning model? (Select\ntwo)\nThey require less computational power and storage\nYour selection is correct\nThey foster trust and confidence in model predictions\nThey enhance security by concealing model logic\nThey simplify the integration process with other systems\nYour selection is correct\nThey facilitate easier debugging and optimization\nOverall explanation",
    "chunk_id": 118,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Correct options:\nThey facilitate easier debugging and optimization\nTransparent models allow developers to understand how inputs\nare transformed into outputs, making it easier to identify and\ncorrect errors or inefficiencies in the model. This capability is\ncrucial for optimizing the model’s performance and ensuring it\nbehaves as expected.\nThey foster trust and confidence in model predictions\nWhen stakeholders can understand the decision-making process\nof a model, it builds trust in its predictions. Transparency is key in\nhigh-stakes scenarios, such as healthcare or finance, where\nunderstanding the rationale behind predictions is critical for\nacceptance and trust.\nIncorrect options:\nThey require less computational power and storage - The\ncomputational and storage requirements of a model depend on\nits complexity and the amount of data it processes, not\nnecessarily on its transparency. Both transparent and opaque\nmodels can vary widely in their resource needs.",
    "chunk_id": 119,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "its complexity and the amount of data it processes, not\nnecessarily on its transparency. Both transparent and opaque\nmodels can vary widely in their resource needs.\nThey enhance security by concealing model logic - Opaque\nmodels, not transparent ones, are typically associated with\nenhanced security through obscurity. Transparent models, by\ndefinition, reveal their internal workings, which can be less secure\nif the logic itself needs to be protected.\nThey simplify the integration process with other systems - The\nease of integrating a model with other systems is more related to\nthe architecture and compatibility of the model with existing\nsystems rather than its transparency. Transparent models do not\ninherently simplify integration processes.\nReferences:\nhttps://docs.aws.amazon.com/whitepapers/latest/model-\nexplainability-aws-ai-ml/interpretability-versus-explainability.html\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/clarify-\nmodel-explainability.html\nDomain\nML Model Development",
    "chunk_id": 120,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Question 22\nCorrect\nYou are a data scientist at a marketing agency tasked with\ncreating a sentiment analysis model to analyze customer reviews\nfor a new product. The company wants to quickly deploy a\nsolution with minimal training time and development effort. You\ndecide to leverage a pre-trained natural language processing\n(NLP) model and fine-tune it using a custom dataset of labeled\ncustomer reviews. Your team has access to both Amazon\nBedrock and SageMaker JumpStart.\nWhich approach is the MOST APPROPRIATE for fine-tuning the\npre-trained model with your custom dataset?\nUse Amazon Bedrock to select a base foundation model from a\nthird-party provider, then fine-tune the base model directly in the\nBedrock interface using your custom dataset\nUse Amazon Bedrock to train a model from scratch using your\ncustom dataset, as Bedrock is optimized for training large\nmodels efficiently\nUse SageMaker JumpStart to create a custom container for your",
    "chunk_id": 121,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Use Amazon Bedrock to train a model from scratch using your\ncustom dataset, as Bedrock is optimized for training large\nmodels efficiently\nUse SageMaker JumpStart to create a custom container for your\npre-trained model and manually implement fine-tuning with\nTensorFlow\nYour answer is correct\nUse SageMaker JumpStart to deploy a pre-trained NLP model\nand use the built-in fine-tuning functionality with your custom\ndataset to create a customized sentiment analysis model\nOverall explanation\nCorrect option:\nUse SageMaker JumpStart to deploy a pre-trained NLP model\nand use the built-in fine-tuning functionality with your custom\ndataset to create a customized sentiment analysis model",
    "chunk_id": 122,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Amazon Bedrock is the easiest way to build and scale generative\nAI applications with foundation models. Amazon Bedrock is a\nfully managed service that offers a choice of high-performing\nfoundation models (FMs) from leading AI companies like AI21\nLabs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and\nAmazon through a single API, along with a broad set of\ncapabilities you need to build generative AI applications with\nsecurity, privacy, and responsible AI.\nAmazon SageMaker JumpStart is a machine learning (ML) hub\nthat can help you accelerate your ML journey. With SageMaker\nJumpStart, you can evaluate, compare, and select FMs quickly\nbased on pre-defined quality and responsibility metrics to\nperform tasks like article summarization and image generation.\nSageMaker JumpStart provides managed infrastructure and tools\nto accelerate scalable, reliable, and secure model building,\ntraining, and deployment of ML models.\nFine-tuning trains a pretrained model on a new dataset without",
    "chunk_id": 123,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "to accelerate scalable, reliable, and secure model building,\ntraining, and deployment of ML models.\nFine-tuning trains a pretrained model on a new dataset without\ntraining from scratch. This process, also known as transfer\nlearning, can produce accurate models with smaller datasets and\nless training time.\nSageMaker JumpStart is specifically designed for scenarios like\nthis, where you can quickly deploy a pre-trained model and fine-\ntune it using your custom dataset. This approach allows you to\nleverage existing NLP models, reducing both development time\nand computational resources needed for training from scratch.",
    "chunk_id": 124,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://docs.aws.amazon.com/sagemaker/latest/dg/\njumpstart-fine-tune.html\nIncorrect options:\nUse Amazon Bedrock to select a base foundation model from a\nthird-party provider, then fine-tune the base model directly in the\nBedrock interface using your custom dataset - Amazon Bedrock\nprovides access to foundation models from third-party providers,\nallowing for easy deployment and integration into applications.\nHowever, Bedrock does not support fine-tuning the base model\nwithin its interface. You need to create your own private copy of\nthe base Foundation Model and then fine-tune this copy with\nyour custom dataset.\nUse Amazon Bedrock to train a model from scratch using your\ncustom dataset, as Bedrock is optimized for training large\nmodels efficiently - Amazon Bedrock is not intended for training\nmodels from scratch, especially not for scenarios where fine-\ntuning a pre-trained model would be more efficient. Bedrock is\noptimized for deploying and scaling foundation models, not for",
    "chunk_id": 125,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "models from scratch, especially not for scenarios where fine-\ntuning a pre-trained model would be more efficient. Bedrock is\noptimized for deploying and scaling foundation models, not for\nraw model training.",
    "chunk_id": 126,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Use SageMaker JumpStart to create a custom container for your\npre-trained model and manually implement fine-tuning with\nTensorFlow - While it’s possible to create a custom container and\nmanually fine-tune a model, SageMaker JumpStart already offers\nan integrated solution for fine-tuning pre-trained models without\nthe need for custom containers or manual implementation. This\nmakes it a more efficient and straightforward option for the task\nat hand.\nReference:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-\nfine-tune.html\nDomain\nML Model Development\nQuestion 23\nCorrect\nA healthcare company is building a predictive model to identify\nhigh-risk patients for hospital readmission. The dataset includes\npatient records such as demographic information, past\ndiagnoses, and admission history. The data is stored in Amazon\nS3 and a relational database hosted on an on-premises\nPostgreSQL server. The dataset has a class imbalance issue",
    "chunk_id": 127,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "diagnoses, and admission history. The data is stored in Amazon\nS3 and a relational database hosted on an on-premises\nPostgreSQL server. The dataset has a class imbalance issue\nwhere very few patients are flagged as high-risk, which affects\nthe performance of the model. Additionally, the dataset contains\nboth categorical features (e.g., \"diagnosis type\") and numerical\nfeatures (e.g., \"days in hospital\"). The ML engineer must\npreprocess the data to resolve the class imbalance and ensure\nthe dataset is ready for training, using a solution that requires\nminimal operational effort.\nWhich solution will meet these requirements?\nUse AWS Glue ETL jobs to write custom scripts for handling\nclass imbalance by oversampling the minority class",
    "chunk_id": 128,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Leverage AWS Glue DataBrew’s native features to clean and\ntransform the dataset, including attempting to balance class\ndistribution by duplicating records from the minority class\nUse SageMaker Feature Store to automatically balance the class\ndistribution in the dataset\nYour answer is correct\nUse Amazon SageMaker Data Wrangler's 'balance data'\noperation to oversample the minority class to resolve the class\nimbalance\nOverall explanation\nCorrect option:\nUse Amazon SageMaker Data Wrangler's 'balance data'\noperation to oversample the minority class to resolve the class\nimbalance\nAmazon SageMaker Data Wrangler provides an intuitive, visual\ninterface for performing data preparation tasks. Key benefits\ninclude:\nNo-code approach with minimal operational overhead.\nSupports popular techniques for balancing data (oversampling/\nundersampling).\nIntegrates seamlessly with Amazon S3 and SageMaker\nworkflows.\nThe \"balance data\" operation in SageMaker Data Wrangler",
    "chunk_id": 129,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Supports popular techniques for balancing data (oversampling/\nundersampling).\nIntegrates seamlessly with Amazon S3 and SageMaker\nworkflows.\nThe \"balance data\" operation in SageMaker Data Wrangler\nallows users to easily address class imbalance issues using\ntechniques like -\nOversampling: Duplicating data from the minority class.\nUndersampling: Reducing the majority class data to match the\nminority class.\nThis built-in operation eliminates the need for custom scripts or\ncomplex workflows, ensuring the task is completed with minimal\noperational effort. The balanced dataset can then be directly\nexported to SageMaker for model training.\nBalance your data for machine learning with Amazon SageMaker\nData Wrangler:",
    "chunk_id": 130,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://aws.amazon.com/blogs/machine-learning/balance-\nyour-data-for-machine-learning-with-amazon-sagemaker-data-\nwrangler/\nIncorrect options:\nLeverage AWS Glue DataBrew’s native features to clean and\ntransform the dataset, including attempting to balance class",
    "chunk_id": 131,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "distribution by duplicating records from the minority class - AWS\nGlue DataBrew provides built-in capabilities for data cleaning and\ntransformations, such as filtering rows, handling missing data,\nand applying basic aggregations. However, DataBrew does not\nhave native features for oversampling or undersampling to\nbalance class distributions. Addressing class imbalance would\nrequire custom steps or manual workarounds, which increase\noperational overhead compared to SageMaker Data Wrangler's\nbuilt-in \"balance data\" operation.\nUse SageMaker Feature Store to automatically balance the class\ndistribution in the dataset - SageMaker Feature Store is used for\nstoring and serving preprocessed features, but it does not\nprovide functionality to balance data.\nUse AWS Glue ETL jobs to write custom scripts for handling\nclass imbalance by oversampling the minority class - AWS Glue\nETL jobs require custom Python or PySpark scripts for balancing\ndata, which increases operational overhead compared to",
    "chunk_id": 132,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "class imbalance by oversampling the minority class - AWS Glue\nETL jobs require custom Python or PySpark scripts for balancing\ndata, which increases operational overhead compared to\nSageMaker Data Wrangler's built-in operations.\nReferences:\nhttps://aws.amazon.com/sagemaker-ai/data-wrangler/\nhttps://aws.amazon.com/blogs/machine-learning/balance-your-\ndata-for-machine-learning-with-amazon-sagemaker-data-\nwrangler/\nhttps://aws.amazon.com/blogs/big-data/7-most-common-data-\npreparation-transformations-in-aws-glue-databrew/\nDomain\nData Preparation for Machine Learning (ML)\nQuestion 24\nIncorrect\nA healthcare company is deploying an ML model to predict\npatient readmission rates using Amazon SageMaker. The\ncompany’s ML engineer is setting up a CI/CD pipeline using AWS\nCodePipeline to automate the model retraining and deployment\nprocess. The pipeline must automatically trigger when new\ntraining data is uploaded to an Amazon S3 bucket. The goal is to",
    "chunk_id": 133,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "retrain the model and deploy it for real-time inference. Given this\ncontext, consider the following steps:\n1. The pipeline deploys the model version in SageMaker Model\nMonitor for real-time inferences.\n2. A new data upload triggers the pipeline via an Amazon S3\nevent notification.\n3. The pipeline deploys the retrained model to a SageMaker\nendpoint for real-time predictions.\n4. Amazon SageMaker retrains the model using updated data\nstored in the S3 bucket.\n5. An S3 Lifecycle rule attempts to start the pipeline when\nfresh data arrives.\nWhich three steps should be selected and ordered correctly to\nconfigure the pipeline?\nYour answer is incorrect\n2,4,1\n5,4,1\nCorrect answer\n2,4,3\n5,4,3\nOverall explanation\nCorrect option:\n2,4,3\n1. A new data upload triggers the pipeline via an Amazon S3\nevent notification.\nAmazon S3 event notifications can be configured to\nautomatically trigger the CI/CD pipeline in AWS CodePipeline\nwhen new training data is uploaded. This ensures that the",
    "chunk_id": 134,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "event notification.\nAmazon S3 event notifications can be configured to\nautomatically trigger the CI/CD pipeline in AWS CodePipeline\nwhen new training data is uploaded. This ensures that the\npipeline starts automatically without manual intervention.\n1. Amazon SageMaker retrains the model using updated data\nstored in the S3 bucket.",
    "chunk_id": 135,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "After the pipeline is triggered, Amazon SageMaker retrains the\nmodel using the updated training data in the S3 bucket. This step\nis essential for ensuring the model reflects the most recent data.\n1. The pipeline deploys the retrained model to a SageMaker\nendpoint for real-time predictions.\nOnce the model is retrained, the CI/CD pipeline automatically\ndeploys the updated model to a SageMaker endpoint for real-\ntime inference.\nIncorrect options:\n2,4,1\n5,4,1\nSageMaker Model Monitor is designed to detect drift and\nmonitor model quality after deployment. You cannot deploy a\nmodel to SageMaker Model Monitor. Rather, deploying a model\nto a SageMaker endpoint ensures it serves real-time predictions,\naligning with the question's deployment requirement. So, both\nthese options are incorrect.\n5,4,3 - S3 Lifecycle rules are used to transition or expire objects\nbased on predefined policies. They cannot invoke pipelines or\ntrigger actions when data is uploaded. So, this option is\nincorrect.",
    "chunk_id": 136,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "5,4,3 - S3 Lifecycle rules are used to transition or expire objects\nbased on predefined policies. They cannot invoke pipelines or\ntrigger actions when data is uploaded. So, this option is\nincorrect.\nReference:\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/how-\nto-create-pipeline.html\nDomain\nDeployment and Orchestration of ML Workflows\nQuestion 25\nIncorrect\nYou are a machine learning engineer at a biotech company\ndeveloping a custom deep learning model for analyzing genomic\ndata. The model relies on a specific version of TensorFlow with\ncustom Python libraries and dependencies that are not available\nin the standard SageMaker environments. To ensure compatibility\nand flexibility, you decide to use the \"Bring Your Own Container\"",
    "chunk_id": 137,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "(BYOC) approach with Amazon SageMaker for both training and\ninference.\nGiven this scenario, which steps are MOST IMPORTANT for\nsuccessfully deploying your custom container with SageMaker,\nensuring that it meets the company’s requirements?\nPackage the model as a SageMaker-compatible file, upload it to\nAmazon S3, and use a pre-built SageMaker container for training,\nensuring that the training job uses the custom environment\nCorrect answer\nCreate a Docker container with the required environment, push\nthe container image to Amazon ECR (Elastic Container Registry),\nand use SageMaker’s Script Mode to execute the training script\nwithin the container\nDeploy the model locally using Docker, then use the AWS\nManagement Console to manually copy the environment and\nmodel files to a SageMaker instance for training\nYour answer is incorrect\nBuild a Docker container with the required TensorFlow version\nand dependencies, push the container image to Docker Hub, and",
    "chunk_id": 138,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "model files to a SageMaker instance for training\nYour answer is incorrect\nBuild a Docker container with the required TensorFlow version\nand dependencies, push the container image to Docker Hub, and\nreference the image in SageMaker when creating the training job\nOverall explanation\nCorrect option:\nCreate a Docker container with the required environment, push\nthe container image to Amazon ECR (Elastic Container Registry),\nand use SageMaker’s Script Mode to execute the training script\nwithin the container\nScript mode enables you to write custom training and inference\ncode while still utilizing common ML framework containers\nmaintained by AWS.\nSageMaker supports most of the popular ML frameworks\nthrough pre-built containers, and has taken the extra step to\noptimize them to work especially well on AWS compute and",
    "chunk_id": 139,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "network infrastructure in order to achieve near-linear scaling\nefficiency. These pre-built containers also provide some\nadditional Python packages, such as Pandas and NumPy, so you\ncan write your own code for training an algorithm. These\nframeworks also allow you to install any Python package hosted\non PyPi by including a requirements.txt file with your training\ncode or to include your own code directories.\nThis is the correct approach for using the BYOC strategy with\nSageMaker. You build a Docker container that includes the\nrequired TensorFlow version and custom dependencies, then\npush the image to Amazon ECR. SageMaker can reference this\nimage to create training jobs and deploy endpoints. By using\nScript Mode, you can execute your custom training script within\nthe container, ensuring compatibility with your specific\nenvironment.",
    "chunk_id": 140,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://aws.amazon.com/blogs/machine-learning/bring-\nyour-own-model-with-amazon-sagemaker-script-mode/\nIncorrect options:\nBuild a Docker container with the required TensorFlow version\nand dependencies, push the container image to Docker Hub, and",
    "chunk_id": 141,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "reference the image in SageMaker when creating the training job\n- While Docker Hub can be used to host container images,\nAmazon SageMaker is optimized to work with images stored in\nAmazon ECR, providing better security, performance, and\nintegration with AWS services. Additionally, using Docker Hub for\nproduction ML workloads may pose security and compliance\nrisks.\nPackage the model as a SageMaker-compatible file, upload it to\nAmazon S3, and use a pre-built SageMaker container for training,\nensuring that the training job uses the custom environment - This\noption describes a standard SageMaker workflow using pre-built\ncontainers, which does not provide the customization required by\nthe BYOC approach. SageMaker pre-built containers may not\nsupport the specific custom libraries and dependencies your\nmodel requires.\nDeploy the model locally using Docker, then use the AWS\nManagement Console to manually copy the environment and\nmodel files to a SageMaker instance for training - Manually",
    "chunk_id": 142,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "model requires.\nDeploy the model locally using Docker, then use the AWS\nManagement Console to manually copy the environment and\nmodel files to a SageMaker instance for training - Manually\ndeploying the model and environment locally and then copying\nfiles to SageMaker instances is not scalable or maintainable.\nSageMaker BYOC allows for a more robust, automated, and\nintegrated solution.\nReferences:\nhttps://aws.amazon.com/blogs/machine-learning/bring-your-\nown-model-with-amazon-sagemaker-script-mode/\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/docker-\ncontainers.html\nDomain\nDeployment and Orchestration of ML Workflows\nQuestion 26\nIncorrect\nA financial services company is developing an AI-based credit\nrisk assessment system using Amazon SageMaker. The system\nneeds to support end-to-end ML workflows, including\nexperimentation, model training, version management,",
    "chunk_id": 143,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "deployment, and monitoring. To comply with internal governance\npolicies, the company requires a manual approval-based\nworkflow to ensure that only approved models can be deployed\nto production endpoints. All training data should be securely\nstored in Amazon S3, and the models should be managed\nthrough a centralized system.\nWhich solution will best meet these requirements?\nCorrect answer\nUse SageMaker Pipelines with conditional steps to implement\nmanual approval workflows for model deployment\nUse Amazon SageMaker Lineage Tracking to validate and\napprove models before deployment\nYour answer is incorrect\nUse Amazon SageMaker Model Monitor to validate and approve\nmodels before deployment\nUse AWS CodePipeline to manage deployments and set manual\napproval actions for endpoint updates\nOverall explanation\nCorrect option:\nUse SageMaker Pipelines with conditional steps to implement\nmanual approval workflows for model deployment\nAmazon SageMaker Pipelines is the recommended solution for",
    "chunk_id": 144,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Correct option:\nUse SageMaker Pipelines with conditional steps to implement\nmanual approval workflows for model deployment\nAmazon SageMaker Pipelines is the recommended solution for\nimplementing manual approval-based workflows for model\ndeployment. SageMaker Pipelines allows you to design\nautomated ML workflows, including steps for training, registering\nmodels in SageMaker Model Registry, and deploying models to\nendpoints. You can use conditional steps in SageMaker Pipelines\nto introduce a manual approval step before proceeding to\nproduction deployments. This ensures only models explicitly\napproved by a human reviewer are deployed, which aligns\nperfectly with the requirement for governance and control.\nKey Benefits:",
    "chunk_id": 145,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Supports manual approval workflows within automated pipelines.\nIntegrates seamlessly with SageMaker Model Registry to manage\napproved models.\nReduces operational overhead by automating model deployment\nwith built-in approval checks.\nIncorrect options:\nUse Amazon SageMaker Model Monitor to validate and approve\nmodels before deployment - SageMaker Model Monitor is\ndesigned to detect drift and monitor model quality after\ndeployment. It does not provide manual approval workflows or\nenforce governance before deployment.\nUse AWS CodePipeline to manage deployments and set manual\napproval actions for endpoint updates - While AWS CodePipeline\ncan include manual approval actions, it is primarily a CI/CD tool.\nIt does not integrate natively with SageMaker Model Registry or\nsupport the orchestration of ML workflows like SageMaker\nPipelines does. This makes it less suitable for the ML-specific\nuse case described.\nUse Amazon SageMaker Lineage Tracking to validate and",
    "chunk_id": 146,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "support the orchestration of ML workflows like SageMaker\nPipelines does. This makes it less suitable for the ML-specific\nuse case described.\nUse Amazon SageMaker Lineage Tracking to validate and\napprove models before deployment - SageMaker Lineage\nTracking is used to track the lineage of artifacts (e.g., datasets,\nmodels, and experiments) within an ML workflow. It provides\nvisibility into the relationships between components, such as\nwhich dataset and training job produced a specific model\nversion. However, it does not support manual approval workflows\nor enforce governance for deploying models to production. While\nlineage tracking is valuable for auditing and reproducibility, it\ndoes not include built-in mechanisms for validating or approving\nmodels for deployment.\nReference:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/model-\nregistry-approve.html\nDomain\nML Solution Monitoring, Maintenance, and Security\nQuestion 27",
    "chunk_id": 147,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Correct\nWhich AWS service is used to store, share and manage inputs to\nMachine Learning models used during training and inference?\nAmazon SageMaker Clarify\nYour answer is correct\nAmazon SageMaker Feature Store\nAmazon SageMaker Data Wrangler\nAmazon SageMaker Ground Truth\nOverall explanation\nCorrect option:\nAmazon SageMaker Feature Store\nAmazon SageMaker Feature Store is a fully managed, purpose-\nbuilt repository to store, share, and manage features for machine\nlearning (ML) models. Features are inputs to ML models used\nduring training and inference. For example, in an application that\nrecommends a music playlist, features could include song\nratings, listening duration, and listener demographics.\nYou can ingest data into SageMaker Feature Store from a variety\nof sources, such as application and service logs, clickstreams,\nsensors, and tabular data from Amazon Simple Storage Service\n(Amazon S3), Amazon Redshift, AWS Lake Formation,\nSnowflake, and Databricks Delta Lake.",
    "chunk_id": 148,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "sensors, and tabular data from Amazon Simple Storage Service\n(Amazon S3), Amazon Redshift, AWS Lake Formation,\nSnowflake, and Databricks Delta Lake.\nHow Feature Store works:",
    "chunk_id": 149,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://aws.amazon.com/sagemaker/feature-store/\nIncorrect options:\nAmazon SageMaker Clarify - SageMaker Clarify helps identify\npotential bias during data preparation without writing code. You\nspecify input features, such as gender or age, and SageMaker\nClarify runs an analysis job to detect potential bias in those\nfeatures.\nAmazon SageMaker Data Wrangler - Amazon SageMaker Data\nWrangler reduces the time it takes to aggregate and prepare\ntabular and image data for ML from weeks to minutes. With\nSageMaker Data Wrangler, you can simplify the process of data\npreparation and feature engineering, and complete each step of\nthe data preparation workflow (including data selection,\ncleansing, exploration, visualization, and processing at scale)\nfrom a single visual interface.\nAmazon SageMaker Ground Truth - Amazon SageMaker Ground\nTruth offers the most comprehensive set of human-in-the-loop\ncapabilities, allowing you to harness the power of human",
    "chunk_id": 150,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Amazon SageMaker Ground Truth - Amazon SageMaker Ground\nTruth offers the most comprehensive set of human-in-the-loop\ncapabilities, allowing you to harness the power of human\nfeedback across the ML lifecycle to improve the accuracy and\nrelevancy of models. You can complete a variety of human-in-\nthe-loop tasks with SageMaker Ground Truth, from data\ngeneration and annotation to model review, customization, and\nevaluation, either through a self-service or an AWS-managed\noffering.\nReferences:",
    "chunk_id": 151,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "https://aws.amazon.com/sagemaker/feature-store/\nhttps://aws.amazon.com/sagemaker/groundtruth\nhttps://aws.amazon.com/sagemaker/clarify/\nhttps://aws.amazon.com/sagemaker/data-wrangler/\nDomain\nData Preparation for Machine Learning (ML)\nQuestion 28\nIncorrect\nYou are working as a machine learning engineer for a startup that\nprovides image recognition services. The service is currently in its\nbeta phase, and the company expects varying levels of traffic,\nwith some days having very few requests and other days\nexperiencing sudden spikes. The company wants to minimize\ncosts during low-traffic periods while still being able to handle\nlarge, infrequent spikes of requests efficiently. Given these\nrequirements, you are considering using Amazon SageMaker for\nyour deployment.\nWhich of the following statements is the BEST recommendation\nfor the given scenario?\nUse Amazon SageMaker Real-time Inference that minimizes\ncosts during low-traffic periods while managing large infrequent",
    "chunk_id": 152,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "for the given scenario?\nUse Amazon SageMaker Real-time Inference that minimizes\ncosts during low-traffic periods while managing large infrequent\nspikes of requests efficiently\nUse Batch transform to run inference with Amazon SageMaker\nthat minimizes costs during low-traffic periods while managing\nlarge infrequent spikes of requests efficiently\nYour answer is incorrect\nUse Amazon SageMaker Asynchronous Inference that minimizes\ncosts during low-traffic periods while managing large infrequent\nspikes of requests efficiently\nCorrect answer",
    "chunk_id": 153,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Use Amazon SageMaker Serverless Inference that minimizes\ncosts during low-traffic periods while managing large infrequent\nspikes of requests efficiently\nOverall explanation\nCorrect option:\nUse Amazon SageMaker Serverless Inference that minimizes\ncosts during low-traffic periods while managing large infrequent\nspikes of requests efficiently\nvia - https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-\nworks-deployment.html\nServerless Inference is designed to automatically scale the\ncompute resources based on incoming requests, making it highly\nefficient for handling varying levels of traffic. It is cost-effective\nbecause you only pay for the compute time used when requests\nare being processed. This makes it an excellent choice for\nscenarios where traffic is unpredictable, with periods of low or no\ntraffic. It is ideal for workloads that have idle periods between\ntraffic spikes and can tolerate cold starts.",
    "chunk_id": 154,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://docs.aws.amazon.com/sagemaker/latest/dg/\nserverless-endpoints.html\nIncorrect options:\nUse Amazon SageMaker Asynchronous Inference that minimizes\ncosts during low-traffic periods while managing large infrequent\nspikes of requests efficiently - Asynchronous Inference is ideal for\nhandling large and long-running inference requests that do not\nrequire an immediate response. However, it may not be as cost-\neffective for handling fluctuating traffic where immediate scaling\nand low-latency are priorities.\nUse Amazon SageMaker Real-time Inference that minimizes\ncosts during low-traffic periods while managing large infrequent\nspikes of requests efficiently - Real-time inference is ideal for\ninference workloads where you have real-time, interactive, low\nlatency requirements.\nUse Batch transform to run inference with Amazon SageMaker\nthat minimizes costs during low-traffic periods while managing\nlarge infrequent spikes of requests efficiently - To get predictions",
    "chunk_id": 155,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Use Batch transform to run inference with Amazon SageMaker\nthat minimizes costs during low-traffic periods while managing\nlarge infrequent spikes of requests efficiently - To get predictions\nfor an entire dataset, you can use Batch transform with Amazon\nSageMaker.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/serverless-\nendpoints.html",
    "chunk_id": 156,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-\nworks-deployment.html\nDomain\nDeployment and Orchestration of ML Workflows\nQuestion 29\nCorrect\nAn e-commerce company’s ML engineer is tasked with building a\nmachine learning model to predict customer purchase behavior.\nTo ensure consistency and reusability of features, the engineer\ndecides to use Amazon SageMaker Feature Store to create,\nmanage, and store features for training and inference. Given this\ncontext, consider the following steps:\n1. Prepare training dataset by accessing the feature data from\nthe store\n2. Load the feature data into the store.\n3. Set up a feature group to organize and store features.\nWhat should be the correct order of steps to create and use the\nfeatures in Amazon SageMaker Feature Store?\n1,2,3\n2,3,1\n3,1,2\nYour answer is correct\n3,2,1\nOverall explanation\nCorrect option:\n3,2,1\n1. Set up a feature group to organize and store features.\nA feature group is a logical grouping of features, which is the",
    "chunk_id": 157,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "3,1,2\nYour answer is correct\n3,2,1\nOverall explanation\nCorrect option:\n3,2,1\n1. Set up a feature group to organize and store features.\nA feature group is a logical grouping of features, which is the\nfoundation of the SageMaker Feature Store. It defines the\nschema of the data, such as feature names, types, and",
    "chunk_id": 158,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "metadata. Creating a feature group is the first step to structure\nand organize features for\n1. Load the feature data into the store.\nAfter the feature group is created, the engineer must ingest\nrecords into the feature group. This involves writing data (features\nand their values) into the Feature Store. Data can be ingested in\neither the online store for low-latency inference or the offline\nstore for model training.\n1. Prepare training dataset by accessing the feature data from\nthe store\nOnce the records are ingested, the engineer can query the offline\nstore to retrieve historical feature data for building datasets.\nThese datasets can then be used to train the ML model.\nIncorrect options:\n1,2,3\n2,3,1\n3,1,2\nThese three options contradict the explanation provided above,\nso these options are incorrect.\nReferences:\nhttps://aws.amazon.com/sagemaker-ai/feature-store/\nhttps://docs.aws.amazon.com/sagemaker/latest/APIReference/\nAPI_FeatureGroup.html\nDomain",
    "chunk_id": 159,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "so these options are incorrect.\nReferences:\nhttps://aws.amazon.com/sagemaker-ai/feature-store/\nhttps://docs.aws.amazon.com/sagemaker/latest/APIReference/\nAPI_FeatureGroup.html\nDomain\nData Preparation for Machine Learning (ML)\nQuestion 30\nCorrect\nYou are a data scientist working on a binary classification model\nto predict whether customers will default on their loans. The\ndataset is highly imbalanced, with only 10% of the customers\nhaving defaulted in the past. After training the model, you need to\nevaluate its performance to ensure it effectively distinguishes\nbetween defaulters and non-defaulters. Given the class\nimbalance, accuracy alone is not sufficient to assess the model’s\nperformance. Instead, you decide to use the Receiver Operating",
    "chunk_id": 160,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Characteristic (ROC) curve and the Area Under the ROC Curve\n(AUC) to evaluate the model.\nWhich of the following interpretations of the ROC and AUC\nmetrics is MOST ACCURATE for assessing the model’s\nperformance?\nA ROC curve that is close to the diagonal line (AUC ~ 0.5)\nindicates that the model performs well across all thresholds\nA ROC curve that is closer to the top-left corner of the plot (AUC\n~ 1) shows that the model is overfitting, and its predictions are\ntoo optimistic\nAn AUC close to 0 indicates that the model is highly accurate,\ncorrectly classifying almost all instances of defaulters and non-\ndefaulters\nYour answer is correct\nAn AUC close to 1.0 indicates that the model has excellent\ndiscriminatory power, effectively distinguishing between\ndefaulters and non-defaulters\nOverall explanation\nCorrect option:\nAn AUC close to 1.0 indicates that the model has excellent\ndiscriminatory power, effectively distinguishing between\ndefaulters and non-defaulters",
    "chunk_id": 161,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Overall explanation\nCorrect option:\nAn AUC close to 1.0 indicates that the model has excellent\ndiscriminatory power, effectively distinguishing between\ndefaulters and non-defaulters\nArea Under the (Receiver Operating Characteristic) Curve (AUC)\nrepresents an industry-standard accuracy metric for binary\nclassification models. AUC measures the ability of the model to\npredict a higher score for positive examples as compared to\nnegative examples. Because it is independent of the score cut-\noff, you can get a sense of the prediction accuracy of your model\nfrom the AUC metric without picking a threshold.\nThe AUC metric returns a decimal value from 0 to 1. AUC values\nnear 1 indicate an ML model that is highly accurate. Values near\n0.5 indicate an ML model that is no better than guessing at",
    "chunk_id": 162,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "random. Values near 0 are unusual to see, and typically indicate a\nproblem with the data. Essentially, an AUC near 0 says that the\nML model has learned the correct patterns, but is using them to\nmake predictions that are flipped from reality ('0's are predicted\nas '1's and vice versa). The ROC curve is the plot of the true\npositive rate (TPR) against the false positive rate (FPR) at each\nthreshold setting.\nvia - https://aws.amazon.com/blogs/machine-learning/is-your-\nmodel-good-a-deep-dive-into-amazon-sagemaker-canvas-\nadvanced-metrics/\nAn AUC close to 1.0 signifies that the model has excellent\ndiscriminatory power, meaning it can effectively distinguish\nbetween the positive class (defaulters) and the negative class\n(non-defaulters) across all thresholds. This is desirable in a\nclassification task, especially in scenarios with class imbalance.",
    "chunk_id": 163,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://docs.aws.amazon.com/machine-learning/latest/dg/\nbinary-model-insights.html\nIncorrect options:\nA ROC curve that is close to the diagonal line (AUC ~ 0.5)\nindicates that the model performs well across all thresholds - A\nROC curve close to the diagonal line (AUC ~ 0.5) indicates that\nthe model has no discriminatory power and is performing no\nbetter than random guessing. This suggests poor model\nperformance, not that the model performs well across all\nthresholds.\nA ROC curve that is closer to the top-left corner of the plot (AUC\n~ 1) shows that the model is overfitting, and its predictions are\ntoo optimistic - A ROC curve closer to the top-left corner of the\nplot (AUC closer to 1.0) indicates strong model performance, not\noverfitting. Overfitting is typically identified by other indicators,\nsuch as a large gap between training and validation performance,\nnot by the shape of the ROC curve alone.\nAn AUC close to 0 indicates that the model is highly accurate,",
    "chunk_id": 164,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "such as a large gap between training and validation performance,\nnot by the shape of the ROC curve alone.\nAn AUC close to 0 indicates that the model is highly accurate,\ncorrectly classifying almost all instances of defaulters and non-\ndefaulters - An AUC close to 0 is problematic, as it indicates that",
    "chunk_id": 165,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "the model is consistently making incorrect predictions (i.e., it\nclassifies negatives as positives and vice versa). A high AUC\n(close to 1) is what signifies strong model performance.\nReferences:\nhttps://docs.aws.amazon.com/machine-learning/latest/dg/\nbinary-model-insights.html\nhttps://aws.amazon.com/blogs/machine-learning/creating-high-\nquality-machine-learning-models-for-financial-services-using-\namazon-sagemaker-autopilot/\nhttps://aws.amazon.com/blogs/machine-learning/is-your-model-\ngood-a-deep-dive-into-amazon-sagemaker-canvas-advanced-\nmetrics/\nDomain\nML Model Development\nQuestion 31\nCorrect\nYou are an ML engineer at a data analytics company tasked with\ntraining a deep learning model on a large, computationally\nintensive dataset. The training job can tolerate interruptions and\nis expected to run for several hours or even days, depending on\nthe available compute resources. The company has a limited\nbudget for cloud infrastructure, so you need to minimize costs as",
    "chunk_id": 166,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "is expected to run for several hours or even days, depending on\nthe available compute resources. The company has a limited\nbudget for cloud infrastructure, so you need to minimize costs as\nmuch as possible.\nWhich strategy is the MOST EFFECTIVE for your ML training job\nwhile minimizing cost and ensuring the job completes\nsuccessfully?\nYour answer is correct\nUse Amazon SageMaker Managed Spot Training to dynamically\nallocate Spot Instances for the training job, automatically retrying\nany interrupted instances via checkpoints\nDeploy the training job on a fixed number of On-Demand EC2\ninstances to ensure stability, and manually add Spot Instances as\nneeded to speed up the job during off-peak hours",
    "chunk_id": 167,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Use Amazon EC2 Auto Scaling to automatically add Spot\nInstances to the training job based on demand, and configure the\njob to continue processing even if some Spot Instances are\ninterrupted\nStart the training job using only Spot Instances to minimize cost,\nand switch to On-Demand instances manually if any Spot\nInstances are interrupted during training\nOverall explanation\nCorrect option:\nUse Amazon SageMaker Managed Spot Training to dynamically\nallocate Spot Instances for the training job, automatically retrying\nany interrupted instances via checkpoints\nManaged Spot Training uses Amazon EC2 Spot instance to run\ntraining jobs instead of on-demand instances. You can specify\nwhich training jobs use spot instances and a stopping condition\nthat specifies how long SageMaker waits for a job to run using\nAmazon EC2 Spot instances. Spot instances can be interrupted,\ncausing jobs to take longer to start or finish. You can configure\nyour managed spot training job to use checkpoints. SageMaker",
    "chunk_id": 168,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Amazon EC2 Spot instances. Spot instances can be interrupted,\ncausing jobs to take longer to start or finish. You can configure\nyour managed spot training job to use checkpoints. SageMaker\ncopies checkpoint data from a local path to Amazon S3. When\nthe job is restarted, SageMaker copies the data from Amazon S3\nback into the local path. The training job can then resume from\nthe last checkpoint instead of restarting.",
    "chunk_id": 169,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://aws.amazon.com/blogs/aws/managed-spot-training-\nsave-up-to-90-on-your-amazon-sagemaker-training-jobs/\nIncorrect options:\nUse Amazon EC2 Auto Scaling to automatically add Spot\nInstances to the training job based on demand, and configure the\njob to continue processing even if some Spot Instances are\ninterrupted - Amazon EC2 Auto Scaling can add Spot Instances\nbased on demand, but it does not provide the same level of\nautomation and resilience as SageMaker Managed Spot Training,\nespecially for ML-specific workloads where Spot interruptions\nneed to be handled gracefully.\nDeploy the training job on a fixed number of On-Demand EC2\ninstances to ensure stability, and manually add Spot Instances as\nneeded to speed up the job during off-peak hours - Using a fixed\nnumber of On-Demand EC2 instances provides stability, but\nmanually adding Spot Instances introduces complexity and may",
    "chunk_id": 170,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "not fully optimize costs. Automating this process with SageMaker\nis more efficient.\nStart the training job using only Spot Instances to minimize cost,\nand switch to On-Demand instances manually if any Spot\nInstances are interrupted during training - Starting with only Spot\nInstances minimizes costs, but manually switching to On-\nDemand instances increases the risk of delays and interruptions\nif Spot capacity becomes unavailable. SageMaker Managed Spot\nTraining offers a more reliable and automated solution.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/model-\nmanaged-spot-training.html\nhttps://aws.amazon.com/blogs/aws/managed-spot-training-\nsave-up-to-90-on-your-amazon-sagemaker-training-jobs/\nDomain\nDeployment and Orchestration of ML Workflows\nQuestion 32\nIncorrect\nA healthcare company is building an AI application to predict\npatient readmission rates using Amazon SageMaker. The\napplication must support end-to-end machine learning",
    "chunk_id": 171,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Question 32\nIncorrect\nA healthcare company is building an AI application to predict\npatient readmission rates using Amazon SageMaker. The\napplication must support end-to-end machine learning\nworkflows, including data preprocessing, model training, version\nmanagement, and deployment. The training data, stored securely\nin Amazon S3, must be used in isolated and secure environments\nto comply with regulatory requirements. As part of model\nexperimentation, the data science team is running multiple\ntraining jobs back-to-back to test different hyperparameter\nconfigurations.\nTo improve the team’s productivity, the company needs to reduce\nthe startup time for each consecutive training job. What is the\nmost efficient solution to achieve this goal?\nYour answer is incorrect\nUse SageMaker Training Compiler to minimize data transfer\nlatency",
    "chunk_id": 172,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Use Amazon EC2 On-Demand Instances with pre-configured\nAMIs for SageMaker\nCorrect answer\nEnable SageMaker Warm Pools to reuse training instances\nbetween jobs\nUse Amazon SageMaker Managed Spot Training for faster\nresource allocation\nOverall explanation\nCorrect option:\nEnable SageMaker Warm Pools to reuse training instances\nbetween jobs\nAmazon SageMaker Warm Pools allow reuse of ML compute\ninfrastructure between consecutive training jobs. This\nsignificantly reduces startup times because instances remain\nwarm and do not require new provisioning or configuration.\nWarm Pools work seamlessly with SageMaker training jobs,\nhelping minimize infrastructure startup overhead while ensuring\nthe infrastructure is reused securely and efficiently. This is ideal\nfor use cases where consecutive training jobs are frequent, as in\nexperimentation workflows.\nKey Benefits:\nReduces time spent on infrastructure provisioning.\nOptimizes compute resource utilization for iterative training.",
    "chunk_id": 173,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "experimentation workflows.\nKey Benefits:\nReduces time spent on infrastructure provisioning.\nOptimizes compute resource utilization for iterative training.\nSupports secure training job execution as it integrates with\nSageMaker's role-based permissions.",
    "chunk_id": 174,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://docs.aws.amazon.com/sagemaker/latest/dg/model-\nregistry.html\nIncorrect options:\nUse Amazon SageMaker Managed Spot Training for faster\nresource allocation - Managed Spot Training in Amazon\nSageMaker is a cost-saving feature that uses spare EC2 capacity\nto run training jobs at a reduced cost. However, spot instances\nare not guaranteed to always be available and may lead to longer\ninfrastructure startup times due to capacity provisioning delays. It\ndoes not minimize startup times and is not ideal for consecutive\ntraining jobs requiring quick infrastructure readiness.\nUse Amazon EC2 On-Demand Instances with pre-configured\nAMIs for SageMaker - While pre-configured AMIs can reduce\nconfiguration time, EC2 On-Demand Instances still require\nprovisioning at the start of each job. SageMaker Warm Pools are\nspecifically designed to minimize startup times for training jobs.\nUse SageMaker Training Compiler to minimize data transfer",
    "chunk_id": 175,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "provisioning at the start of each job. SageMaker Warm Pools are\nspecifically designed to minimize startup times for training jobs.\nUse SageMaker Training Compiler to minimize data transfer\nlatency - The SageMaker Training Compiler is used to optimize\ndeep learning training workloads by accelerating model training",
    "chunk_id": 176,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "and reducing costs. While it improves training performance, it\ndoes not address infrastructure startup times or reduce the time\nrequired to initialize the training environment.\nReferences:\nhttps://aws.amazon.com/blogs/machine-learning/best-practices-\nfor-amazon-sagemaker-training-managed-warm-pools/\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/train-warm-\npools.html\nDomain\nDeployment and Orchestration of ML Workflows\nQuestion 33\nCorrect\nYou are a Data Scientist working for an e-commerce company\nthat is developing a machine learning model to predict whether a\ncustomer will make a purchase based on their browsing behavior.\nYou need to evaluate the model's performance using different\nevaluation metrics to understand how well the model is\npredicting the positive class (i.e., customers who will make a\npurchase). The dataset is imbalanced, with a small percentage of\ncustomers making a purchase. Given this context, you must\ndecide on the most appropriate evaluation techniques to assess",
    "chunk_id": 177,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "purchase). The dataset is imbalanced, with a small percentage of\ncustomers making a purchase. Given this context, you must\ndecide on the most appropriate evaluation techniques to assess\nyour model's effectiveness and identify potential areas for\nimprovement.\nWhich of the following evaluation techniques and metrics should\nyou prioritize when assessing the performance of your model,\nconsidering the dataset's imbalance and the need for a\ncomprehensive understanding of both false positives and false\nnegatives? (Select two)\nYour selection is correct\nUse precision and recall to focus on the model's ability to\ncorrectly identify positive cases while minimizing false positives\nand false negatives\nYour selection is correct",
    "chunk_id": 178,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Evaluate the model using the confusion matrix, which provides\ninsights into true positives, false positives, true negatives, and\nfalse negatives, allowing you to calculate additional metrics such\nas precision, recall, and F1 score\nUse accuracy as the primary metric, as it measures the\npercentage of correct predictions out of all predictions made by\nthe model\nUtilize the AUC-ROC curve to evaluate the model’s ability to\ndistinguish between classes across various thresholds,\nparticularly in the presence of class imbalance\nPrioritize Root mean squared error (RMSE) as the key metric, as\nit measures the average magnitude of the errors between\npredicted and actual values\nOverall explanation\nCorrect options:\nEvaluate the model using the confusion matrix, which provides\ninsights into true positives, false positives, true negatives, and\nfalse negatives, allowing you to calculate additional metrics such\nas precision, recall, and F1 score\nThe confusion matrix illustrates in a table the number or",
    "chunk_id": 179,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "false negatives, allowing you to calculate additional metrics such\nas precision, recall, and F1 score\nThe confusion matrix illustrates in a table the number or\npercentage of correct and incorrect predictions for each class by\ncomparing an observation's predicted class and its true class.\nThe confusion matrix is crucial for understanding the detailed\nperformance of your model, especially in an imbalanced dataset.\nIt allows you to calculate additional metrics such as precision,\nrecall, and F1 score, which are essential for understanding how\nwell your model handles false positives and false negatives.\nUse precision and recall to focus on the model's ability to\ncorrectly identify positive cases while minimizing false positives\nand false negatives\nPrecision and recall are particularly important in an imbalanced\ndataset. Precision measures the proportion of true positive\npredictions among all positive predictions, while recall measures",
    "chunk_id": 180,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "the proportion of actual positives that are correctly identified.\nFocusing on these metrics helps in assessing how well the model\navoids false positives and false negatives, which is critical in your\nscenario.\nKey metrics to measure machine learning model performance:",
    "chunk_id": 181,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://docs.aws.amazon.com/sagemaker/latest/dg/\nautopilot-metrics-validation.html\nIncorrect options:\nUse accuracy as the primary metric, as it measures the\npercentage of correct predictions out of all predictions made by\nthe model - While accuracy is a common metric, it is not suitable\nfor imbalanced datasets because it can be misleading. A model\npredicting the majority class most of the time can achieve high\naccuracy without effectively capturing the minority class (e.g.,\ncustomers who make a purchase).\nPrioritize Root mean squared error (RMSE) as the key metric, as\nit measures the average magnitude of the errors between\npredicted and actual values - RMSE is a regression metric, not\nsuitable for classification problems. In this scenario, you are\ndealing with a classification task, so metrics like precision, recall,\nand F1 score are more appropriate.\nUtilize the AUC-ROC curve to evaluate the model’s ability to\ndistinguish between classes across various thresholds,",
    "chunk_id": 182,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "and F1 score are more appropriate.\nUtilize the AUC-ROC curve to evaluate the model’s ability to\ndistinguish between classes across various thresholds,\nparticularly in the presence of class imbalance - The AUC-ROC",
    "chunk_id": 183,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "curve is a useful tool, especially in imbalanced datasets.\nHowever, understanding the confusion matrix and calculating\nprecision and recall provide more direct insights into the types of\nerrors the model is making, which is crucial for improving the\nmodel’s performance in your specific context.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-\nmetrics-validation.html\nhttps://docs.aws.amazon.com/machine-learning/latest/dg/\nbinary-classification.html\nDomain\nML Model Development\nQuestion 34\nCorrect\nYou are an ML Engineer working for a healthcare company that\nuses a machine learning model to recommend personalized\ntreatment plans to patients. The model is deployed on Amazon\nSageMaker and is critical to the company's operations, as any\nincorrect predictions could have significant consequences. A\nnew version of the model has been developed, and you need to\ndeploy it in production. However, you want to ensure that the",
    "chunk_id": 184,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "incorrect predictions could have significant consequences. A\nnew version of the model has been developed, and you need to\ndeploy it in production. However, you want to ensure that the\ndeployment process is robust, allowing you to quickly roll back to\nthe previous version if any issues arise. Additionally, you need to\nmaintain version control for future updates and manage traffic\nbetween different model versions.\nWhich of the following strategies should you implement to ensure\na smooth and reliable deployment of the new model version\nusing Amazon SageMaker, considering best practices for\nversioning and rollback strategies? (Select two)\nDeploy the new model version alongside the current one, and\nuse Amazon SageMaker’s multi-model endpoint to serve both\nmodels simultaneously, splitting traffic between them\nYour selection is correct",
    "chunk_id": 185,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Utilize Amazon SageMaker’s blue/green deployment strategy to\nshift traffic gradually from the old model to the new one, ensuring\nthat you can monitor performance and quickly revert if needed\nYour selection is correct\nUse Amazon SageMaker’s built-in versioning to manage different\nversions of the model, and deploy the new version in a canary\nrelease by redirecting a small percentage of traffic to it initially\nCreate a backup of the current model, deploy the new version,\nand if any issues arise, manually roll back by redeploying the\nprevious model version\nDeploy the new model version immediately and redirect 100% of\ntraffic to it, assuming it has been thoroughly tested and will not\nrequire a rollback\nOverall explanation\nCorrect options:\nUse Amazon SageMaker’s built-in versioning to manage different\nversions of the model, and deploy the new version in a canary\nrelease by redirecting a small percentage of traffic to it initially\nAmazon SageMaker supports model versioning, which is crucial",
    "chunk_id": 186,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "versions of the model, and deploy the new version in a canary\nrelease by redirecting a small percentage of traffic to it initially\nAmazon SageMaker supports model versioning, which is crucial\nfor tracking different iterations of your model. A canary release\nallows you to deploy the new model version to a small portion of\nusers, minimizing risk by limiting exposure in case of issues. If\nthe new version performs well, you can gradually increase traffic\nto it.\nUtilize Amazon SageMaker’s blue/green deployment strategy to\nshift traffic gradually from the old model to the new one, ensuring\nthat you can monitor performance and quickly revert if needed\nA blue/green deployment strategy is a best practice in model\ndeployment. It allows you to deploy the new model version in\nparallel with the existing one, gradually shifting traffic to the new\nversion while monitoring its performance. If issues are detected,\nyou can quickly roll back to the previous version without\ndisrupting service.",
    "chunk_id": 187,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "In a blue/green deployment, SageMaker provisions a new fleet\nwith the updates (the green fleet). Then, SageMaker shifts traffic\nfrom the old fleet (the blue fleet) to the green fleet. Once the\ngreen fleet operates smoothly for a set evaluation period (called\nthe baking period), SageMaker terminates the blue fleet. You can\nspecify Amazon CloudWatch alarms that SageMaker uses to\nmonitor the green fleet. If an issue with the updated code trips\nany of the alarms, SageMaker initiates an auto-rollback to the\nblue fleet in order to maintain availability thereby minimizing risk.\nvia - https://docs.aws.amazon.com/sagemaker/latest/dg/\ndeployment-guardrails-blue-green.html\nIncorrect options:\nDeploy the new model version immediately and redirect 100% of\ntraffic to it, assuming it has been thoroughly tested and will not\nrequire a rollback - Redirecting 100% of traffic to the new model\nversion immediately is risky, especially in a critical application like",
    "chunk_id": 188,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "require a rollback - Redirecting 100% of traffic to the new model\nversion immediately is risky, especially in a critical application like\nhealthcare. Without a rollback plan, any issues with the new\nversion could lead to significant consequences.",
    "chunk_id": 189,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Create a backup of the current model, deploy the new version,\nand if any issues arise, manually roll back by redeploying the\nprevious model version - While creating a backup and manually\nrolling back is a possible strategy, it is not as efficient or reliable\nas using a built-in rollback feature like blue/green deployment or\ncanary releases. Manual rollbacks can lead to delays and\nincreased downtime.\nDeploy the new model version alongside the current one, and\nuse Amazon SageMaker’s multi-model endpoint to serve both\nmodels simultaneously, splitting traffic between them - While\nusing a multi-model endpoint could serve both models\nsimultaneously, it is not the best approach for managing risk\nduring deployment. This strategy is more suited for scenarios\nwhere you need to serve multiple models for different purposes\nrather than managing a controlled rollout.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/deployment-\nguardrails-blue-green.html",
    "chunk_id": 190,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "rather than managing a controlled rollout.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/deployment-\nguardrails-blue-green.html\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/deployment-\nguardrails-rolling.html\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-\nendpoints.html\nDomain\nDeployment and Orchestration of ML Workflows\nQuestion 35\nCorrect\nA healthcare company has developed multiple machine learning\n(ML) models for predicting patient outcomes. These models are\nstored in Amazon Elastic Container Registry (Amazon ECR)\nrepositories across different AWS accounts where they were\ntrained. The company needs to create a centralized catalog of\nthese models to streamline version management, deployment\ntracking, and governance compliance. The catalog must provide\ncross-account access, allowing each AWS account to interact\nwith the central repository securely. The solution should also",
    "chunk_id": 191,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "enable the company to organize models logically and manage\nthe lifecycle of different model versions efficiently.\nWhich solution will meet these requirements?\nSet up an Amazon Redshift cluster to store metadata from ECR\nrepositories across accounts and use SQL queries to track and\ncatalog models centrally. Configure cross-account access to\nRedshift via federated roles\nUse Amazon ECR to replicate model images across accounts\nand configure cross-account access using IAM roles. Track\nmetadata and deployment status using custom tagging\nYour answer is correct\nUse SageMaker Model Registry to create model groups for\norganizing models and managing versions. Configure cross-\naccount access by attaching a resource-based policy to the\nSageMaker Model Registry that grants permissions to other AWS\naccounts\nCreate a central Amazon S3 bucket to store exported model\nartifacts from each account, enable cross-account access via\nbucket policies, and manually maintain a versioning structure for",
    "chunk_id": 192,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "accounts\nCreate a central Amazon S3 bucket to store exported model\nartifacts from each account, enable cross-account access via\nbucket policies, and manually maintain a versioning structure for\ncompliance\nOverall explanation\nCorrect option:\nUse SageMaker Model Registry to create model groups for\norganizing models and managing versions. Configure cross-\naccount access by attaching a resource-based policy to the\nSageMaker Model Registry that grants permissions to other AWS\naccounts\nSageMaker Model Registry enables centralized management of\nML models, including organizing models into model groups,\ntracking versions, and enforcing governance policies. By using a\nresource-based policy, the company can grant cross-account\naccess. A resource-based policy is attached directly to the",
    "chunk_id": 193,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "SageMaker Model Registry resource to define access\npermissions for specific AWS accounts or organizational units\n(OUs). This approach ensures secure cross-account access\nwithout additional setup of resource shares. It also provides\ngranular permission control for actions like registering,\naccessing, and deploying models.\nYou should note that Amazon SageMaker Model Registry also\nintegrates with AWS Resource Access Manager (AWS RAM),\nmaking it easier to securely share and discover machine learning\n(ML) models across your AWS accounts.",
    "chunk_id": 194,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://docs.aws.amazon.com/sagemaker/latest/dg/model-\nregistry-version.html#model-registry-version-xaccount\nIncorrect options:\nUse Amazon ECR to replicate model images across accounts\nand configure cross-account access using IAM roles. Track\nmetadata and deployment status using custom tagging - While\nECR supports cross-account replication, it lacks tools for\nmanaging model lifecycle features such as versioning,\ndeployment tracking, or governance. Manual tagging adds\noperational overhead and is error-prone.\nCreate a central Amazon S3 bucket to store exported model\nartifacts from each account, enable cross-account access via\nbucket policies, and manually maintain a versioning structure for\ncompliance - Amazon S3 can store models but does not provide\ntools for tracking versions, monitoring deployment status, or\nenforcing governance. Manually managing compliance using\nbucket policies and versioning is inefficient.\nSet up an Amazon Redshift cluster to store metadata from ECR",
    "chunk_id": 195,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "enforcing governance. Manually managing compliance using\nbucket policies and versioning is inefficient.\nSet up an Amazon Redshift cluster to store metadata from ECR\nrepositories across accounts and use SQL queries to track and\ncatalog models centrally. Configure cross-account access to\nRedshift via federated roles - Redshift is a data warehouse\ndesigned for analytical queries, not for managing ML models or\ntheir lifecycle. It lacks support for logical organization (model\ngroups), version tracking, and governance.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/model-\nregistry-version.html#model-registry-version-xaccount\nhttps://aws.amazon.com/about-aws/whats-new/2024/06/\namazon-sagemaker-model-registry-cross-account-ml-model-\nsharing/\nDomain\nDeployment and Orchestration of ML Workflows\nQuestion 36\nCorrect",
    "chunk_id": 196,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "You are an ML engineer at a retail company that uses a\nSageMaker model to generate product recommendations for\ncustomers in real-time. During peak shopping periods, the traffic\nto the recommendation engine increases dramatically. The\ncompany needs to ensure that the model endpoint can handle\nthese spikes in demand without compromising on response time\nor customer experience. At the same time, you want to optimize\ncosts by scaling down resources during periods of low demand.\nYou are evaluating different scaling policies to manage this\ndynamic workload effectively.\nWhich scaling policy is the MOST SUITABLE for this scenario,\nand why?\nUse a manual scaling policy where you adjust the number of\ninstances based on real-time monitoring of traffic, allowing you to\nfine-tune resource allocation as needed during high-demand\nperiods\nUse a step scaling policy that adjusts the number of instances\nbased on the size of the traffic spike, adding a set number of",
    "chunk_id": 197,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "fine-tune resource allocation as needed during high-demand\nperiods\nUse a step scaling policy that adjusts the number of instances\nbased on the size of the traffic spike, adding a set number of\ninstances for each level of increased demand\nUse scheduled scaling to preemptively add or remove instances\nbased on anticipated traffic patterns, such as known peak times\nduring Black Friday, to ensure sufficient capacity is available\nwhen needed\nYour answer is correct\nUse a target tracking scaling policy that automatically adjusts the\nnumber of instances based on a predefined target metric, such\nas CPU utilization or invocations per instance, to maintain a\nsteady level of performance during traffic spikes\nOverall explanation\nCorrect option:\nUse a target tracking scaling policy that automatically adjusts the\nnumber of instances based on a predefined target metric, such",
    "chunk_id": 198,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "as CPU utilization or invocations per instance, to maintain a\nsteady level of performance during traffic spikes\nA target tracking scaling policy is ideal for handling dynamic and\nunpredictable traffic spikes, as it continuously adjusts the\nnumber of instances to maintain a predefined metric (e.g., CPU\nutilization, invocations per instance). This ensures that\nperformance remains consistent even during high-demand\nperiods like Black Friday, while also scaling down during quieter\ntimes to save costs.\nvia - https://docs.aws.amazon.com/sagemaker/latest/dg/\nendpoint-auto-scaling-prerequisites.html\nIncorrect options:\nUse a step scaling policy that adjusts the number of instances\nbased on the size of the traffic spike, adding a set number of\ninstances for each level of increased demand - Step scaling is\nuseful for handling sudden, sharp increases in demand, but it\nmay not respond as smoothly to varying levels of traffic. It’s less",
    "chunk_id": 199,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "instances for each level of increased demand - Step scaling is\nuseful for handling sudden, sharp increases in demand, but it\nmay not respond as smoothly to varying levels of traffic. It’s less\nflexible than target tracking and might lead to over- or under-\nprovisioning if traffic patterns are unpredictable.\nUse scheduled scaling to preemptively add or remove instances\nbased on anticipated traffic patterns, such as known peak times",
    "chunk_id": 200,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "during Black Friday, to ensure sufficient capacity is available\nwhen needed - Scheduled scaling works well when traffic\npatterns are predictable, such as daily or weekly cycles.\nHowever, it’s less effective for managing unexpected traffic\nspikes, as it relies on predefined schedules rather than real-time\ndata.\nUse a manual scaling policy where you adjust the number of\ninstances based on real-time monitoring of traffic, allowing you to\nfine-tune resource allocation as needed during high-demand\nperiods - Manual scaling provides the most control but requires\nconstant monitoring and intervention, which is impractical during\nhigh-traffic events like Black Friday. It also risks delays in scaling,\nwhich could lead to performance issues during traffic surges.\nReference:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-\nauto-scaling-prerequisites.html\nDomain\nDeployment and Orchestration of ML Workflows\nQuestion 37\nCorrect\nYou are a data scientist working on a deep learning model to",
    "chunk_id": 201,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "auto-scaling-prerequisites.html\nDomain\nDeployment and Orchestration of ML Workflows\nQuestion 37\nCorrect\nYou are a data scientist working on a deep learning model to\nclassify medical images for disease detection. The model initially\nshows high accuracy on the training data but performs poorly on\nthe validation set, indicating signs of overfitting. The dataset is\nlimited in size, and the model is complex, with many parameters.\nTo improve generalization and reduce overfitting, you need to\nimplement appropriate techniques while balancing model\ncomplexity and performance.\nGiven these challenges, which combination of techniques is the\nMOST LIKELY to help prevent overfitting and improve the\nmodel’s performance on unseen data?\nYour answer is correct",
    "chunk_id": 202,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Combine data augmentation to increase the diversity of the\ntraining data with early stopping to prevent overfitting, and use\nensembling to average predictions from multiple models\nPrune the model by removing less important layers and nodes,\nand use L2 regularization to reduce the magnitude of the model’s\nweights, preventing overfitting\nUse ensembling by combining multiple versions of the same\nmodel trained with different random seeds, and apply data\naugmentation to artificially increase the size of the dataset\nApply early stopping to halt training when the validation loss\nstops improving, and use dropout as a regularization technique\nto prevent the model from becoming too reliant on specific\nneurons\nOverall explanation\nCorrect option:\nCombine data augmentation to increase the diversity of the\ntraining data with early stopping to prevent overfitting, and use\nensembling to average predictions from multiple models",
    "chunk_id": 203,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://aws.amazon.com/what-is/overfitting/\nThis option combines data augmentation to artificially expand the\ntraining dataset, which is crucial when data is limited, with early\nstopping to prevent the model from overtraining. Additionally,\nensembling helps improve generalization by averaging\npredictions from multiple models, reducing the likelihood that\noverfitting in any single model will dominate the final prediction.\nThis combination addresses both data limitations and model\noverfitting effectively.\nIncorrect options:\nApply early stopping to halt training when the validation loss\nstops improving, and use dropout as a regularization technique\nto prevent the model from becoming too reliant on specific\nneurons - Dropout is a form of regularization used in neural\nnetworks that reduces overfitting by trimming codependent",
    "chunk_id": 204,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "neurons. Early stopping and dropout are effective techniques for\npreventing overfitting, particularly in deep learning models.\nHowever, while they can help, they may not be sufficient alone,\nespecially when dealing with limited data. Combining these\ntechniques with others, such as data augmentation or\nensembling, would provide a more robust solution.\nUse ensembling by combining multiple versions of the same\nmodel trained with different random seeds, and apply data\naugmentation to artificially increase the size of the dataset -\nEnsembling and data augmentation are powerful techniques, but\nensembling by combining multiple versions of the same model\ntrained with different random seeds might not provide significant\ndiversity in predictions. A combination of diverse models or more\ncomprehensive techniques might be more effective.\nPrune the model by removing less important layers and nodes,\nand use L2 regularization to reduce the magnitude of the model’s",
    "chunk_id": 205,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "comprehensive techniques might be more effective.\nPrune the model by removing less important layers and nodes,\nand use L2 regularization to reduce the magnitude of the model’s\nweights, preventing overfitting - Regularization helps prevent\nlinear models from overfitting training data examples by\npenalizing extreme weight values. L1 regularization reduces the\nnumber of features used in the model by pushing the weight of\nfeatures that would otherwise have very small weights to zero. L1\nregularization produces sparse models and reduces the amount\nof noise in the model. L2 regularization results in smaller overall\nweight values, which stabilizes the weights when there is high\ncorrelation between the features. Pruning and L2 regularization\nare useful for reducing model complexity and preventing\noverfitting. However, pruning can sometimes lead to underfitting\nif not done carefully, and using these techniques alone might not\nfully address the overfitting issue, especially with limited data.",
    "chunk_id": 206,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "overfitting. However, pruning can sometimes lead to underfitting\nif not done carefully, and using these techniques alone might not\nfully address the overfitting issue, especially with limited data.\nReferences:\nhttps://aws.amazon.com/what-is/overfitting/\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/object2vec-\nhyperparameters.html\nhttps://docs.aws.amazon.com/machine-learning/latest/dg/\ntraining-parameters.html\nDomain",
    "chunk_id": 207,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "ML Model Development\nQuestion 38\nIncorrect\nA company’s data science team uses Amazon SageMaker\nnotebook instances to develop machine learning models. The\nteam frequently collaborates on projects that require access to\nshared datasets and specific Amazon S3 buckets. Currently,\npermissions for accessing S3 buckets are managed by creating\nindividual IAM roles for each SageMaker notebook instance. This\ndecentralized approach has led to inconsistent permissions,\nduplication of effort, and difficulty in managing access across\nteam members. The company wants to centralize permissions\nmanagement to ensure all SageMaker notebook instances used\nby the team can access the required S3 buckets consistently and\nefficiently.\nWhich solution will meet this requirement?\nCorrect answer\nAttach a single IAM role with S3 permissions to all SageMaker\nnotebook instances used by the data science team\nYour answer is incorrect\nCreate an IAM group for the data science team, associate the",
    "chunk_id": 208,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Attach a single IAM role with S3 permissions to all SageMaker\nnotebook instances used by the data science team\nYour answer is incorrect\nCreate an IAM group for the data science team, associate the\nrequired S3 access policies to the group, and attach the IAM\ngroup directly to the SageMaker notebook instances to grant\npermissions to all data scientists\nAttach the necessary permissions directly to each data scientist's\nIAM user to enable granular control over access to the notebook\ninstances\nUse inline policies directly on each notebook instance to define\nlocal permissions for the data scientists\nOverall explanation\nCorrect option:",
    "chunk_id": 209,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Attach a single IAM role with S3 permissions to all SageMaker\nnotebook instances used by the data science team\nThe correct way to centralize access to Amazon S3 buckets for\nSageMaker notebook instances is to:\nCreate a single IAM role with the required S3 access\npermissions.\nAttach this role to all SageMaker notebook instances used by the\ndata science team.\nIAM roles are specifically designed to grant AWS service\nresources (like SageMaker notebook instances) secure,\ntemporary access to other AWS services. This ensures:\nConsistent permissions across all team members.\nSimplified management of access policies without duplicating\nroles.\nScalability as new SageMaker notebook instances are created.\nIncorrect options:\nCreate an IAM group for the data science team, associate the\nrequired S3 access policies to the group, and attach the IAM\ngroup directly to the SageMaker notebook instances to grant\npermissions to all data scientists - IAM groups cannot be directly",
    "chunk_id": 210,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "required S3 access policies to the group, and attach the IAM\ngroup directly to the SageMaker notebook instances to grant\npermissions to all data scientists - IAM groups cannot be directly\nattached to Amazon SageMaker notebook instances or any AWS\nresources. IAM groups are used to organize IAM users and\nsimplify permission management by attaching policies to the\ngroup, which are then inherited by its members. To grant\npermissions to resources like SageMaker notebook instances,\nyou must use IAM roles or directly attach policies to individual\nIAM users. This option incorrectly implies that an IAM group can\nbe attached to a notebook instance, which is not supported in\nAWS.\nAttach the necessary permissions directly to each data scientist's\nIAM user to enable granular control over access to the notebook\ninstances - Assigning policies directly to IAM users creates\ndecentralized management, which becomes difficult to maintain\nas the team grows.",
    "chunk_id": 211,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "instances - Assigning policies directly to IAM users creates\ndecentralized management, which becomes difficult to maintain\nas the team grows.\nUse inline policies directly on each notebook instance to define\nlocal permissions for the data scientists - This option acts as a",
    "chunk_id": 212,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "distractor. An inline policy is a policy created for a single IAM\nidentity (a user, user group, or role). It cannot be directly attached\nto a resource, such as the SageMaker notebook instance.\nReference:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-\nroles.html\nDomain\nML Solution Monitoring, Maintenance, and Security\nQuestion 39\nIncorrect\nA financial services company uses Amazon SageMaker to\ndevelop and register machine learning (ML) models for various\nbusiness needs, such as fraud detection, risk assessment, and\ncustomer segmentation. These models are stored in model\ngroups within the SageMaker Model Registry. The data science\nteam is categorized into three specialized groups based on their\nareas of focus: fraud detection models, risk assessment models,\nand customer segmentation models. An ML engineer needs to\nimplement a solution to organize the existing models into these\nthree business categories to improve discoverability at scale,",
    "chunk_id": 213,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "and customer segmentation models. An ML engineer needs to\nimplement a solution to organize the existing models into these\nthree business categories to improve discoverability at scale,\nwhile ensuring the integrity of the model artifacts and their\nexisting groupings remains unaffected.\nWhich solution will meet these requirements?\nAttach custom tags to each model artifact in the SageMaker\nModel Registry to specify their category and filter models based\non tags\nYour answer is incorrect\nUse SageMaker Feature Store to tag models with metadata for\nfraud detection, risk assessment, and customer segmentation.\nFilter models using queries on the Feature Store.",
    "chunk_id": 214,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Move models into newly created SageMaker model groups for\nfraud detection, risk assessment, and customer segmentation,\nreassigning them from their current groups\nCorrect answer\nUse SageMaker Model Registry collections to group existing\nmodel groups into high-level categories, such as fraud detection,\nrisk assessment, and customer segmentation\nOverall explanation\nCorrect option:\nUse SageMaker Model Registry collections to group existing\nmodel groups into high-level categories, such as fraud detection,\nrisk assessment, and customer segmentation\nSageMaker Model Registry collections allow users to logically\norganize model groups into high-level categories without\ndisrupting the integrity of the underlying model groups or\nartifacts. Collections provide a scalable and efficient way to\nimprove model discoverability across a large registry as well as\nmaintain the existing structure of model groups and their\nmetadata. You can also scale seamlessly as more models and",
    "chunk_id": 215,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "improve model discoverability across a large registry as well as\nmaintain the existing structure of model groups and their\nmetadata. You can also scale seamlessly as more models and\nbusiness categories are added.\nKey Benefits of SageMaker Model Registry collections :\nNon-disruptive reorganization using collections.\nBetter model management and discoverability at scale.\nIncorrect options:\nAttach custom tags to each model artifact in the SageMaker\nModel Registry to specify their category and filter models based\non tags - Tags are useful for additional metadata but do not\nprovide a hierarchical organizational structure like model registry\ncollections.\nMove models into newly created SageMaker model groups for\nfraud detection, risk assessment, and customer segmentation,\nreassigning them from their current groups - Moving models to\nnew groups disrupts the existing integrity of the model groups\nand their associated lineage, approvals, and metadata.",
    "chunk_id": 216,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Use SageMaker Feature Store to tag models with metadata for\nfraud detection, risk assessment, and customer segmentation.\nFilter models using queries on the Feature Store. - SageMaker\nFeature Store is intended for storing and serving ML model\nfeatures, not for managing model group organization or\ndiscoverability in the Model Registry.\nReference:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/\nmodelcollections.html\nDomain\nML Solution Monitoring, Maintenance, and Security\nQuestion 40\nIncorrect\nA logistics company is building a delivery time prediction model\non AWS to estimate the number of hours it will take for packages\nto reach their destination. The dataset includes information such\nas distance traveled, traffic conditions, and package weight. The\nmodel outputs a continuous numerical value representing the\nestimated delivery time in hours. The ML engineer needs to\nevaluate the model’s performance to determine how accurately it\npredicts delivery times.",
    "chunk_id": 217,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "estimated delivery time in hours. The ML engineer needs to\nevaluate the model’s performance to determine how accurately it\npredicts delivery times.\nWhich metric should the ML engineer use to evaluate the model's\nperformance?\nPrecision\nYour answer is incorrect\nAccuracy\nCorrect answer\nMean Absolute Error (MAE)\nArea Under the ROC Curve (AUC-ROC)\nOverall explanation\nCorrect option:",
    "chunk_id": 218,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Mean Absolute Error (MAE)\nMean Absolute Error (MAE) is a suitable metric for regression\nproblems where the goal is to predict continuous numerical\nvalues. MAE calculates the average absolute difference between\nthe predicted and actual values, providing an interpretable\nmeasure of model error. Unlike MSE, MAE does not square the\nerrors, so it is less sensitive to outliers and provides a\nstraightforward interpretation in the same units as the target\nvariable.\nMetrics used to measure machine learning model performance:",
    "chunk_id": 219,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://docs.aws.amazon.com/sagemaker/latest/dg/\nautopilot-metrics-validation.html\nIncorrect options:\nPrecision - Precision measures how well an algorithm predicts\nthe true positives (TP) out of all of the positives that it identifies. It\nis defined as follows: Precision = TP/(TP+FP), with values ranging\nfrom zero to one, and is used in binary classification.\nAccuracy - Accuracy is the ratio of the number of correctly\nclassified items to the total number of (correctly and incorrectly)\nclassified items. It is used for both binary and multiclass\nclassification.\nArea Under the ROC Curve (AUC-ROC) - AUC-ROC is used to\nevaluate the performance of binary classification models. It\nmeasures how well the model distinguishes between classes,\nwhich is irrelevant for regression.\nReference:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-\nmetrics-validation.html\nDomain\nML Model Development\nQuestion 41\nIncorrect\nYou are a machine learning engineer working for a",
    "chunk_id": 220,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Reference:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-\nmetrics-validation.html\nDomain\nML Model Development\nQuestion 41\nIncorrect\nYou are a machine learning engineer working for a\ntelecommunications company that needs to develop a predictive\nmaintenance model. The goal is to predict when network\nequipment is likely to fail based on historical sensor data. The\ndata includes features such as temperature, pressure, usage, and\nerror rates recorded over time. The company wants to avoid\nunplanned downtime and optimize maintenance schedules by\npredicting failures just in time.\nGiven the nature of the data and the business objective, which\nAmazon SageMaker built-in algorithm is the MOST SUITABLE for\nthis use case?",
    "chunk_id": 221,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Time Series K-Means Algorithm to cluster similar patterns in the\nsensor data and predict failures\nYour answer is incorrect\nDeepAR Algorithm to forecast future equipment failures based on\nhistorical data\nCorrect answer\nRandom Cut Forest (RCF) Algorithm to detect anomalies in\nsensor data that may indicate impending failures\nLinear Learner Algorithm to classify equipment status as 'healthy'\nor 'at risk' based on sensor readings\nOverall explanation\nCorrect option:\nRandom Cut Forest (RCF) Algorithm to detect anomalies in\nsensor data that may indicate impending failures\nAmazon SageMaker Random Cut Forest (RCF) is an\nunsupervised algorithm for detecting anomalous data points\nwithin a data set. These are observations which diverge from\notherwise well-structured or patterned data. Anomalies can\nmanifest as unexpected spikes in time series data, breaks in\nperiodicity, or unclassifiable data points. They are easy to\ndescribe in that, when viewed in a plot, they are often easily",
    "chunk_id": 222,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "manifest as unexpected spikes in time series data, breaks in\nperiodicity, or unclassifiable data points. They are easy to\ndescribe in that, when viewed in a plot, they are often easily\ndistinguishable from the \"regular\" data. Including these\nanomalies in a data set can drastically increase the complexity of\na machine learning task since the \"regular\" data can often be\ndescribed with a simple model.\nRandom Cut Forest (RCF) is specifically designed for detecting\nanomalies in data. This algorithm excels at identifying\nunexpected patterns in sensor data that could indicate the early\nstages of equipment failure. It’s particularly well-suited for\nscenarios where you need to react to unusual behaviors in near-\nreal-time.\nMapping use cases to built-in algorithms:",
    "chunk_id": 223,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://docs.aws.amazon.com/sagemaker/latest/dg/\nalgos.html\nIncorrect options:\nDeepAR Algorithm to forecast future equipment failures based on\nhistorical data - DeepAR is designed for forecasting future time\nseries data, which could be useful for predicting future\nequipment behavior. However, it is not primarily used for anomaly\ndetection, which is critical for identifying unusual patterns that\nprecede failures.\nLinear Learner Algorithm to classify equipment status as 'healthy'\nor 'at risk' based on sensor readings - Linear Learner could be\nused for classification tasks, but predicting maintenance needs\noften involves detecting subtle anomalies rather than simple\nclassification. Additionally, a binary classification model might not\ncapture the complex patterns associated with potential failures.\nTime Series K-Means Algorithm to cluster similar patterns in the\nsensor data and predict failures - Time Series K-Means can",
    "chunk_id": 224,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "capture the complex patterns associated with potential failures.\nTime Series K-Means Algorithm to cluster similar patterns in the\nsensor data and predict failures - Time Series K-Means can\ncluster similar time series patterns, but clustering alone does not\nprovide the precision needed for real-time anomaly detection,\nwhich is crucial for predictive maintenance.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/algos.html",
    "chunk_id": 225,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "https://docs.aws.amazon.com/sagemaker/latest/dg/\nrandomcutforest.html\nDomain\nML Model Development\nQuestion 42\nIncorrect\nYou are a data scientist at a credit risk management company\nbuilding a machine learning model to predict loan defaults. To\nensure transparency and regulatory compliance, you need to\nexplain how the model makes its predictions, particularly for\nhigh-stakes decisions such as loan approvals or rejections. The\ncompany wants a detailed understanding of the influence of\nindividual features on the model’s predictions for specific\ncustomers, as well as an overall view of how features impact the\nmodel's predictions across the entire dataset.\nWhich of the following explanations BEST describes the\ndifferences between Shapley values and Partial Dependence\nPlots (PDP) in the context of model explainability, and how you\nmight use them for this purpose?\nYour answer is incorrect\nShapley values provide a global view of the model’s behavior by",
    "chunk_id": 226,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Plots (PDP) in the context of model explainability, and how you\nmight use them for this purpose?\nYour answer is incorrect\nShapley values provide a global view of the model’s behavior by\nmeasuring the average effect of each feature across all instances,\nwhile PDP offers a local view by showing the effect of a single\nfeature on the model’s prediction for a specific instance. Use\nShapley values to understand overall feature importance and\nPDP to interpret individual predictions\nCorrect answer\nShapley values provide a local explanation by quantifying the\ncontribution of each feature to the prediction for a specific\ninstance, while PDP provides a global explanation by showing\nthe marginal effect of a feature on the model’s predictions across\nthe dataset. Use Shapley values to explain individual predictions\nand PDP to understand the model's behavior at a dataset level",
    "chunk_id": 227,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Shapley values and PDP are both global explainability methods\nthat show the average effect of features on model predictions.\nUse either method to understand overall feature importance, but\nShapley values are computationally less expensive than PDP\nShapley values provide a visual interpretation of feature\nimportance using plots, while PDP provides numeric values\nindicating the marginal contribution of features to the model's\npredictions. Use Shapley values for visual analysis and PDP for\nquantitative analysis\nOverall explanation\nCorrect option:\nShapley values provide a local explanation by quantifying the\ncontribution of each feature to the prediction for a specific\ninstance, while PDP provides a global explanation by showing\nthe marginal effect of a feature on the model’s predictions across\nthe dataset. Use Shapley values to explain individual predictions\nand PDP to understand the model's behavior at a dataset level\nThis option correctly captures the differences between Shapley",
    "chunk_id": 228,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "the dataset. Use Shapley values to explain individual predictions\nand PDP to understand the model's behavior at a dataset level\nThis option correctly captures the differences between Shapley\nvalues and PDP in the context of model explainability:\nShapley values are a local interpretability method that explains\nindividual predictions by assigning each feature a contribution\nscore based on its marginal effect on the prediction. This method\nis useful for understanding the impact of each feature on a\nspecific instance's prediction.\nPartial Dependence Plots (PDP), on the other hand, provide a\nglobal view of the model’s behavior by illustrating how the\npredicted outcome changes as a single feature is varied across\nits range, holding all other features constant. PDPs help\nunderstand the overall relationship between a feature and the\nmodel output across the entire dataset.\nThus, Shapley values are suited for explaining individual\ndecisions, while PDP is used to understand broader trends in",
    "chunk_id": 229,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "model output across the entire dataset.\nThus, Shapley values are suited for explaining individual\ndecisions, while PDP is used to understand broader trends in\nmodel behavior.",
    "chunk_id": 230,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-\nmodel-explainability.html\nIncorrect options:\nShapley values provide a global view of the model’s behavior by\nmeasuring the average effect of each feature across all instances,\nwhile PDP offers a local view by showing the effect of a single\nfeature on the model’s prediction for a specific instance. Use\nShapley values to understand overall feature importance and\nPDP to interpret individual predictions - This option is incorrect\nbecause it reverses the concepts of local and global\nexplainability methods. PDP is a global method, while Shapley\nvalues provide local explanations.\nShapley values and PDP are both global explainability methods\nthat show the average effect of features on model predictions.\nUse either method to understand overall feature importance, but\nShapley values are computationally less expensive than PDP -\nBoth methods are not purely global explainability tools. Shapley",
    "chunk_id": 231,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "values are computationally more intensive than PDP due to the\nrequirement to compute contributions for each feature across\nmultiple permutations\nShapley values provide a visual interpretation of feature\nimportance using plots, while PDP provides numeric values\nindicating the marginal contribution of features to the model's\npredictions. Use Shapley values for visual analysis and PDP for\nquantitative analysis - This option is incorrect because Shapley\nvalues are not just visual; they provide quantitative contributions\nof features to predictions, while PDP provides visual insight into\nthe marginal effects, not numeric values alone.\nReference:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/clarify-\nmodel-explainability.html\nDomain\nML Model Development\nQuestion 43\nIncorrect\nA data scientist is working for a real estate analytics company to\nbuild an ML model that predicts the rental prices of commercial\noffice spaces. The dataset has several features but the data",
    "chunk_id": 232,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "A data scientist is working for a real estate analytics company to\nbuild an ML model that predicts the rental prices of commercial\noffice spaces. The dataset has several features but the data\nscientist is particularly interested in the following features -\nBuilding Type and Year Constructed (building_type_year), City\nName (city) and Building Size in square meters (building_size).\nThe data has inconsistencies and requires preprocessing before\nmodel training. The data scientist plans to use the following\nfeature engineering techniques to transform the data:\nFeature splitting\nBinning\nOne-hot encoding\nStandardization\nHow would you match the given features with the most relevant\nfeature engineering technique?\nYour answer is incorrect",
    "chunk_id": 233,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "building_type_year - feature splitting, city - One-hot encoding,\nbuilding_size - binning\nCorrect answer\nbuilding_type_year - feature splitting, city - One-hot encoding,\nbuilding_size - standardization\nbuilding_type_year - feature splitting, city - binning, building_size\n- standardization\nbuilding_type_year - standardization, city - binning, building_size\n- One-hot encoding\nOverall explanation\nCorrect option:\nbuilding_type_year - feature splitting, city - One-hot encoding,\nbuilding_size - standardization\nFeature splitting breaks a compound feature (e.g., \"Office_2010\")\ninto separate features such as \"Type\" and \"Year.\" This technique\nimproves model interpretability and performance by treating each\nsub-feature independently. So, this is the correct technique for\nthe feature building_type_year.\nOne-hot encoding converts categorical data (e.g., 'city') into\nbinary numerical columns. It allows ML models to process\ncategorical variables effectively. So, this is the correct technique",
    "chunk_id": 234,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "One-hot encoding converts categorical data (e.g., 'city') into\nbinary numerical columns. It allows ML models to process\ncategorical variables effectively. So, this is the correct technique\nfor the feature city.",
    "chunk_id": 235,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://docs.aws.amazon.com/databrew/latest/dg/recipe-\nactions.ONE_HOT_ENCODING.html\nStandardizing numerical features scales them to have a mean of\n0 and a standard deviation of 1. This technique ensures that\nnumerical features with different units or scales contribute\nequally to the model. So, this is the correct technique for the\nfeature building_size.\nIncorrect options:\nbuilding_type_year - feature splitting, city - One-hot encoding,\nbuilding_size - binning - Binning involves grouping continuous\nnumerical values into bins or ranges. While it is useful for\nsegmenting data, it does not apply to the building_size feature\nfor the given use case.\nbuilding_type_year - feature splitting, city - binning, building_size\n- standardization - Binning involves grouping continuous\nnumerical values into bins or ranges. While it is useful for\nsegmenting data, it does not apply to the city feature for the\ngiven use case.\nbuilding_type_year - standardization, city - binning, building_size",
    "chunk_id": 236,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "segmenting data, it does not apply to the city feature for the\ngiven use case.\nbuilding_type_year - standardization, city - binning, building_size\n- One-hot encoding - Binning involves grouping continuous",
    "chunk_id": 237,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "numerical values into bins or ranges. While it is useful for\nsegmenting data, it does not apply to the city feature for the\ngiven use case.\nReferences:\nhttps://docs.aws.amazon.com/wellarchitected/latest/machine-\nlearning-lens/data-preprocessing.html\nhttps://docs.aws.amazon.com/databrew/latest/dg/recipe-\nactions.ONE_HOT_ENCODING.html\nDomain\nData Preparation for Machine Learning (ML)\nQuestion 44\nCorrect\nA retail company trained an ML model on Amazon SageMaker to\ndetect damaged products from warehouse surveillance images.\nThe training dataset was created using SageMaker Data\nWrangler, which included images of damaged and undamaged\nproducts. During training and validation, the model achieved high\naccuracy; however, in production, the model’s performance is\ndegraded due to differences in lighting conditions and image\nquality across various warehouse locations. The ML engineer\nneeds to improve the model's accuracy to handle variations in",
    "chunk_id": 238,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "degraded due to differences in lighting conditions and image\nquality across various warehouse locations. The ML engineer\nneeds to improve the model's accuracy to handle variations in\nimage quality with the LEAST amount of time and effort.\nWhat do you recommend?\nUse SageMaker Data Wrangler’s outlier detection transform to\nremove outlier images from the dataset and improve the model’s\nperformance\nYour answer is correct\nUse SageMaker Data Wrangler’s corrupt image transform to\npreprocess the training data by simulating variations in image\nquality for robust model training\nFine-tune the model by manually collecting additional data from\nvarious cameras and retrain it with the new dataset",
    "chunk_id": 239,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Deploy a preprocessing pipeline in Amazon SageMaker to filter\nout low-quality images before sending them to the model for\ninference\nOverall explanation\nCorrect option:\nUse SageMaker Data Wrangler’s corrupt image transform to\npreprocess the training data by simulating variations in image\nquality for robust model training\nSageMaker Data Wrangler’s corrupt image transform is\nspecifically designed to simulate real-world image imperfections,\nsuch as noise, blurriness, or resolution changes, during the\npreprocessing stage. By applying this transformation to the\ntraining dataset:\nThe model learns to handle variations in image quality, making it\nmore robust in production.\nThe solution requires minimal time because it builds on the\nexisting training pipeline and does not require extensive data\ncollection or custom scripts.\nThe approach enhances model generalization without the need\nfor a complete retraining process from scratch.\nKey Benefits of Corrupt Image Transform:",
    "chunk_id": 240,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "collection or custom scripts.\nThe approach enhances model generalization without the need\nfor a complete retraining process from scratch.\nKey Benefits of Corrupt Image Transform:\nSimulates real-world image imperfections, improving robustness.\nRequires no manual data collection or custom preprocessing\nscripts.\nIntegrates seamlessly with the existing SageMaker Data Wrangler\nworkflow.\nPrepare image data with Amazon SageMaker Data Wrangler:",
    "chunk_id": 241,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://aws.amazon.com/blogs/machine-learning/prepare-\nimage-data-with-amazon-sagemaker-data-wrangler/\nIncorrect options:\nDeploy a preprocessing pipeline in Amazon SageMaker to filter\nout low-quality images before sending them to the model for\ninference - Filtering low-quality images could result in the loss of\nimportant information, which may decrease model accuracy\nrather than improve it.\nFine-tune the model by manually collecting additional data from\nvarious cameras and retrain it with the new dataset - Collecting\nadditional data and retraining is time-consuming and requires\nsignificant manual effort. It is not the fastest solution for\nimproving the model.\nUse SageMaker Data Wrangler’s outlier detection transform to\nremove outlier images from the dataset and improve the model’s\nperformance - While the outlier detection transform in\nSageMaker Data Wrangler is useful for identifying and removing\nanomalous data points in numerical datasets, it is not designed",
    "chunk_id": 242,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "performance - While the outlier detection transform in\nSageMaker Data Wrangler is useful for identifying and removing\nanomalous data points in numerical datasets, it is not designed\nfor handling image quality variations. Removing outliers based on\npredefined metrics would not address the issue of poor model\nperformance due to lighting and quality inconsistencies in\nproduction images. This approach could even lead to a loss of\nvaluable data.\nReference:\nhttps://aws.amazon.com/blogs/machine-learning/prepare-image-\ndata-with-amazon-sagemaker-data-wrangler/\nDomain\nData Preparation for Machine Learning (ML)\nQuestion 45\nCorrect\nA healthcare organization manages a large dataset containing\npatient records stored in Amazon S3. The dataset includes\ninformation such as patient names, addresses, and phone\nnumbers, but it contains duplicate records due to inconsistencies\nin data entry. Some duplicates are exact matches, while others",
    "chunk_id": 243,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "have slight variations (e.g., spelling errors in names or incomplete\naddresses). The organization needs to identify and group\nduplicate records efficiently while ensuring minimal code\ndevelopment to streamline the deduplication process.\nWhich solution on AWS will detect duplicates in the dataset with\nthe LEAST operational overhead?\nYour answer is correct\nUse AWS Glue FindMatches to automatically detect and group\nduplicate records in the dataset\nUse SageMaker Data Wrangler to detect and group duplicate\nrecords in the dataset by leveraging its data preparation and\ntransformation features\nUse Amazon EMR with Apache Spark to write a custom\ndeduplication script using SparkSQL\nUse AWS Glue ETL to create a custom job for processing and\nfiltering duplicate records.\nOverall explanation\nCorrect option:\nUse AWS Glue FindMatches to automatically detect and group\nduplicate records in the dataset\nAWS Glue FindMatches is a built-in feature designed to detect",
    "chunk_id": 244,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Overall explanation\nCorrect option:\nUse AWS Glue FindMatches to automatically detect and group\nduplicate records in the dataset\nAWS Glue FindMatches is a built-in feature designed to detect\nduplicate records in datasets, even when the records are not\nexact matches. It uses machine learning to find similarities\nacross key attributes, such as customer names, addresses, and\nemails.\nKey Benefits of AWS Glue FindMatches:\nMinimal coding required - The ML-based approach simplifies the\ndeduplication process.\nFlexible matching logic - Automatically identifies fuzzy matches\nand near-duplicates.\nScalable and serverless - Works seamlessly with large datasets in\nAmazon S3.",
    "chunk_id": 245,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Incorrect options:\nUse Amazon EMR with Apache Spark to write a custom\ndeduplication script using SparkSQL - While SparkSQL is\npowerful, writing custom scripts requires significant coding and\noperational overhead compared to the serverless and no-code\nFindMatches feature.\nUse AWS Glue ETL to create a custom job for processing and\nfiltering duplicate records. - AWS Glue ETL jobs require manual\nconfiguration and coding to deduplicate data, which adds\ncomplexity compared to FindMatches’ built-in ML-based\napproach.\nUse SageMaker Data Wrangler to detect and group duplicate\nrecords in the dataset by leveraging its data preparation and\ntransformation features - While Amazon SageMaker Data\nWrangler is a powerful tool for data preparation and\ntransformation, it does not have built-in capabilities for detecting\nor grouping duplicate records, especially those requiring fuzzy\nmatching. Data Wrangler is better suited for cleaning, visualizing,",
    "chunk_id": 246,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "or grouping duplicate records, especially those requiring fuzzy\nmatching. Data Wrangler is better suited for cleaning, visualizing,\nand transforming datasets before training machine learning\nmodels, but it lacks the machine learning-powered deduplication\nfeatures provided by AWS Glue FindMatches.\nReferences:\nhttps://community.aws/content/\n2c0dOcTyJPnK1NR7H05Oii4HblJ/aws-glue\nDomain\nData Preparation for Machine Learning (ML)\nQuestion 46\nCorrect\nYou are a machine learning engineer at an e-commerce company\nthat uses a recommendation model to suggest products to\ncustomers. The model was trained on data from the past year,\nbut after being in production for several months, you notice that\nthe model's recommendations are becoming less relevant. You\nsuspect that either data drift or model drift could be causing the\ndecline in performance. To investigate and resolve the issue, you",
    "chunk_id": 247,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "need to understand the difference between these two types of\ndrift and how to monitor them using Amazon SageMaker.\nWhich of the following statements BEST describes the difference\nbetween data drift and model drift, and how you would address\nthem using Amazon SageMaker?\nData drift is a sudden change in the model’s accuracy, while\nmodel drift is a gradual degradation in model performance. You\nshould use SageMaker Feature Store to manage both types of\ndrift by standardizing input data\nYour answer is correct\nData drift occurs when the distribution of the input data changes\nover time, while model drift happens when the model’s\nunderlying assumptions or parameters become outdated. To\naddress data drift, you should use SageMaker Model Monitor to\ntrack changes in input data distribution. For model drift, you\nshould periodically retrain the model using the latest data\nData drift occurs when the model’s predictions start to deviate\nfrom the expected outcomes, while model drift occurs when the",
    "chunk_id": 248,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "should periodically retrain the model using the latest data\nData drift occurs when the model’s predictions start to deviate\nfrom the expected outcomes, while model drift occurs when the\nmodel's accuracy declines due to changes in the input data.\nSageMaker Pipelines should be used to automate retraining for\nboth types of drift\nData drift refers to changes in the model’s accuracy due to shifts\nin the data, while model drift refers to changes in the underlying\ndata features over time. To address both, you should use\nSageMaker Clarify to detect bias and retrain the model monthly\nOverall explanation\nCorrect option:\nData drift occurs when the distribution of the input data changes\nover time, while model drift happens when the model’s\nunderlying assumptions or parameters become outdated. To\naddress data drift, you should use SageMaker Model Monitor to",
    "chunk_id": 249,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "track changes in input data distribution. For model drift, you\nshould periodically retrain the model using the latest data\nThis option correctly defines data drift as changes in the\ndistribution of the input data over time, which can lead to the\nmodel receiving data that is different from what it was trained on.\nModel drift, on the other hand, occurs when the model’s\nperformance degrades because its assumptions or parameters\nno longer align with the real-world data. SageMaker Model\nMonitor can be used to detect data drift by tracking changes in\ndata distribution, while model drift is addressed by retraining the\nmodel with updated data.\nvia - https://docs.aws.amazon.com/sagemaker/latest/dg/model-\nmonitor.html\nIncorrect options:\nData drift refers to changes in the model’s accuracy due to shifts\nin the data, while model drift refers to changes in the underlying\ndata features over time. To address both, you should use\nSageMaker Clarify to detect bias and retrain the model monthly -",
    "chunk_id": 250,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "in the data, while model drift refers to changes in the underlying\ndata features over time. To address both, you should use\nSageMaker Clarify to detect bias and retrain the model monthly -\nThis option incorrectly describes data drift as related to model\naccuracy and model drift as related to changes in data features.",
    "chunk_id": 251,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "SageMaker Clarify is used for bias detection, not specifically for\ndrift detection.\nData drift occurs when the model’s predictions start to deviate\nfrom the expected outcomes, while model drift occurs when the\nmodel's accuracy declines due to changes in the input data.\nSageMaker Pipelines should be used to automate retraining for\nboth types of drift - This option incorrectly defines data drift and\nmodel drift. Data drift is about changes in data distribution, not\nprediction deviations. SageMaker Pipelines is for automating ML\nworkflows, not directly for drift detection.\nData drift is a sudden change in the model’s accuracy, while\nmodel drift is a gradual degradation in model performance. You\nshould use SageMaker Feature Store to manage both types of\ndrift by standardizing input data - This option misinterprets the\nnature of data and model drift. SageMaker Feature Store is used\nfor managing and serving features, not for directly addressing\ndrift.\nReference:",
    "chunk_id": 252,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "nature of data and model drift. SageMaker Feature Store is used\nfor managing and serving features, not for directly addressing\ndrift.\nReference:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/model-\nmonitor.html\nDomain\nML Solution Monitoring, Maintenance, and Security\nQuestion 47\nCorrect\nYou are a machine learning engineer at a healthcare company\nresponsible for developing and deploying an end-to-end ML\nworkflow for predicting patient readmission rates. The workflow\ninvolves data preprocessing, model training, hyperparameter\ntuning, and deployment. Additionally, the solution must support\nregular retraining of the model as new data becomes available,\nwith minimal manual intervention. You need to select the right\nsolution to orchestrate this workflow efficiently while ensuring\nscalability, reliability, and ease of management.\nGiven these requirements, which of the following options is the\nMOST SUITABLE for orchestrating your ML workflow?",
    "chunk_id": 253,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Use AWS Glue for data preprocessing, Amazon SageMaker for\nmodel training and tuning, and manually deploy the model to an\nAmazon EC2 instance for inference\nYour answer is correct\nImplement the entire ML workflow using Amazon SageMaker\nPipelines, which provides integrated orchestration for data\nprocessing, model training, tuning, and deployment\nLeverage Amazon EC2 instances to manually execute each step\nof the ML workflow, use Amazon RDS for storing intermediate\nresults, and deploy the model using Amazon SageMaker\nendpoints\nUse AWS Step Functions to define and orchestrate each step of\nthe ML workflow, integrate with SageMaker for model training\nand deployment, and leverage AWS Lambda for data\npreprocessing tasks\nOverall explanation\nCorrect option:\nImplement the entire ML workflow using Amazon SageMaker\nPipelines, which provides integrated orchestration for data\nprocessing, model training, tuning, and deployment\nAmazon SageMaker Pipelines is a purpose-built workflow",
    "chunk_id": 254,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Pipelines, which provides integrated orchestration for data\nprocessing, model training, tuning, and deployment\nAmazon SageMaker Pipelines is a purpose-built workflow\norchestration service to automate machine learning (ML)\ndevelopment. SageMaker Pipelines is specifically designed to\norchestrate end-to-end ML workflows, integrating data\nprocessing, model training, hyperparameter tuning, and\ndeployment in a seamless manner. It provides built-in versioning,\nlineage tracking, and support for continuous integration and\ndelivery (CI/CD), making it the best choice for this use case.",
    "chunk_id": 255,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://docs.aws.amazon.com/sagemaker/latest/dg/\npipelines.html\nIncorrect options:\nUse AWS Step Functions to define and orchestrate each step of\nthe ML workflow, integrate with SageMaker for model training\nand deployment, and leverage AWS Lambda for data\npreprocessing tasks - AWS Step Functions is a powerful service\nfor orchestrating workflows, and it can integrate with SageMaker\nand Lambda. However, using Step Functions for the entire ML\nworkflow adds complexity since it requires coordinating multiple\nservices, whereas SageMaker Pipelines provides a more\nseamless, integrated solution for ML-specific workflows.\nLeverage Amazon EC2 instances to manually execute each step\nof the ML workflow, use Amazon RDS for storing intermediate\nresults, and deploy the model using Amazon SageMaker\nendpoints - Manually managing each step of the ML workflow\nusing EC2 instances and RDS is labor-intensive, prone to errors,",
    "chunk_id": 256,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "and not scalable. It also lacks the automation and orchestration\ncapabilities needed for a robust ML workflow.\nUse AWS Glue for data preprocessing, Amazon SageMaker for\nmodel training and tuning, and manually deploy the model to an\nAmazon EC2 instance for inference - While using AWS Glue for\ndata preprocessing and SageMaker for training is possible,\nmanually deploying the model on EC2 lacks the orchestration\nand management features provided by SageMaker Pipelines.\nThis approach also misses out on the integrated tracking,\nautomation, and scalability features offered by SageMaker\nPipelines.\nReference:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/\npipelines.html\nDomain\nDeployment and Orchestration of ML Workflows\nQuestion 48\nCorrect\nA financial institution has deployed a machine learning model\nusing Amazon SageMaker to predict whether credit card\ntransactions are fraudulent. To ensure model performance\nremains consistent, the company configured Amazon SageMaker",
    "chunk_id": 257,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "using Amazon SageMaker to predict whether credit card\ntransactions are fraudulent. To ensure model performance\nremains consistent, the company configured Amazon SageMaker\nModel Monitor to track deviations in the model accuracy over\ntime. The model's baseline accuracy was recorded during its\ninitial deployment. However, after several months of operation,\nthe model’s accuracy drops significantly despite no changes\nbeing made to the model.\nWhat could be the reason for the reduced model accuracy?\nThe model’s hyperparameters have automatically adjusted during\ninference, causing a decline in predictive accuracy\nYour answer is correct",
    "chunk_id": 258,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Concept drift occurred in the underlying customer data that was\nused for predictions, changing the relationship between input\nfeatures and the target variable over time\nThe deployed model’s architecture degraded over time, reducing\nits predictive performance\nAmazon SageMaker Model Monitor miscalculated the accuracy\nmetric due to a configuration error\nOverall explanation\nCorrect option:\nConcept drift occurred in the underlying customer data that was\nused for predictions, changing the relationship between input\nfeatures and the target variable over time\nConcept drift happens when the relationship between input\nfeatures (e.g., customer transaction patterns, demographics, or\nusage behaviors) and the target variable (e.g., fraud detection or\nchurn likelihood) changes over time. For instance:\nCustomer behavior might evolve, resulting in new patterns that\nwere not present in the original training dataset.\nFraud tactics may have changed, altering the model’s",
    "chunk_id": 259,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Customer behavior might evolve, resulting in new patterns that\nwere not present in the original training dataset.\nFraud tactics may have changed, altering the model’s\nassumptions and causing it to make inaccurate predictions.\nWhile the model may still receive input data that appears valid,\nthe patterns it learned during training no longer match real-world\nbehaviors. This leads to a significant drop in predictive\nperformance, such as accuracy or F1 score.\nAutomate model retraining with Amazon SageMaker Pipelines\nwhen drift is detected:",
    "chunk_id": 260,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://aws.amazon.com/blogs/machine-learning/automate-\nmodel-retraining-with-amazon-sagemaker-pipelines-when-drift-\nis-detected/\nIncorrect options:\nThe model’s hyperparameters have automatically adjusted during\ninference, causing a decline in predictive accuracy -\nHyperparameters do not automatically adjust in production.\nSageMaker deployments use a static model until explicitly\nretrained or updated.\nAmazon SageMaker Model Monitor miscalculated the accuracy\nmetric due to a configuration error - SageMaker Model Monitor\naccurately captures metrics unless the input configuration is\nincorrect, which is unlikely if it previously worked as expected.\nThe deployed model’s architecture degraded over time, reducing\nits predictive performance - Model architectures do not degrade\nover time. Reduced accuracy typically results from data drift or\nconcept drift, not the model itself.\nReference:\nhttps://aws.amazon.com/blogs/machine-learning/automate-",
    "chunk_id": 261,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "over time. Reduced accuracy typically results from data drift or\nconcept drift, not the model itself.\nReference:\nhttps://aws.amazon.com/blogs/machine-learning/automate-\nmodel-retraining-with-amazon-sagemaker-pipelines-when-drift-\nis-detected/\nDomain\nML Solution Monitoring, Maintenance, and Security\nQuestion 49\nIncorrect\nA media streaming company processes video metadata using\nAWS Glue jobs orchestrated by an AWS Glue workflow. These\njobs can run on a schedule or be triggered manually. The\ncompany is building Amazon SageMaker Pipelines to develop\nML models for predicting user engagement with video content.\nThe pipelines require the processed metadata from the AWS\nGlue jobs during the feature engineering phase of the ML\nworkflow. The company needs a solution to integrate the AWS\nGlue jobs with SageMaker Pipelines to ensure seamless data\nflow.",
    "chunk_id": 262,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Which solution will meet these requirements with the LEAST\noperational overhead?\nYour answer is incorrect\nConfigure an Amazon EventBridge rule to trigger the SageMaker\npipeline after the completion of AWS Glue jobs and pass job\nmetadata via Lambda\nIntegrate the AWS Glue jobs as processing steps in the\nSageMaker Pipelines workflow to handle data preprocessing\nSet up a custom Python script to orchestrate both the AWS Glue\nworkflow and SageMaker Pipelines, ensuring data dependencies\nare met\nCorrect answer\nUse SageMaker Pipelines callback steps to wait for the AWS\nGlue jobs to complete and retrieve the outputs directly from\nAmazon S3\nOverall explanation\nCorrect option:\nUse SageMaker Pipelines callback steps to wait for the AWS\nGlue jobs to complete and retrieve the outputs directly from\nAmazon S3\nSageMaker Pipelines callback steps are specifically designed to\nintegrate external processes into the SageMaker pipeline\nworkflow. By using a callback step, the SageMaker pipeline waits",
    "chunk_id": 263,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Amazon S3\nSageMaker Pipelines callback steps are specifically designed to\nintegrate external processes into the SageMaker pipeline\nworkflow. By using a callback step, the SageMaker pipeline waits\nuntil the AWS Glue jobs complete. The output of the AWS Glue\njobs, stored in Amazon S3, is then passed to subsequent steps\nin the pipeline. This approach eliminates the need for custom\norchestration scripts, manual intervention, or redundant\nscheduling, ensuring minimal operational overhead. Callback\nsteps are an efficient way to synchronize external workflows, like\nAWS Glue, with SageMaker Pipelines.\nIncorrect options:",
    "chunk_id": 264,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Integrate the AWS Glue jobs as processing steps in the\nSageMaker Pipelines workflow to handle data preprocessing -\nAWS Glue jobs cannot be directly integrated as processing steps\nin SageMaker Pipelines because Glue is an independent service,\nnot a SageMaker-native processing task.\nConfigure an Amazon EventBridge rule to trigger the SageMaker\npipeline after the completion of AWS Glue jobs and pass job\nmetadata via Lambda - While this approach is technically\npossible, it introduces additional operational complexity by\nrequiring EventBridge rules and Lambda for orchestration.\nCallback steps in SageMaker Pipelines are a more direct\nsolution.\nSet up a custom Python script to orchestrate both the AWS Glue\nworkflow and SageMaker Pipelines, ensuring data dependencies\nare met - Custom orchestration scripts require additional\ndevelopment and maintenance, increasing operational overhead\ncompared to the built-in callback steps feature in SageMaker\nPipelines.\nReference:",
    "chunk_id": 265,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "development and maintenance, increasing operational overhead\ncompared to the built-in callback steps feature in SageMaker\nPipelines.\nReference:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/build-and-\nmanage-steps-types.html\nDomain\nDeployment and Orchestration of ML Workflows\nQuestion 50\nCorrect\nA company specializes in providing personalized product\nrecommendations for e-commerce platforms. You’ve been\ntasked with developing a solution that can quickly generate high-\nquality product descriptions, tailor marketing copy based on\ncustomer preferences, and analyze customer reviews to identify\ntrends in sentiment. Given the scale of data and the need for\nflexibility in choosing foundational models, you decide to use an\nAI service that can integrate seamlessly with your existing AWS\ninfrastructure while also offering managed foundational models\nfrom third-party providers.",
    "chunk_id": 266,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Which AWS service would best meet your requirements?\nYour answer is correct\nAmazon Bedrock\nAmazon Rekognition\nAmazon Personalize\nAmazon SageMaker\nOverall explanation\nCorrect option:\nAmazon Bedrock\nAmazon Bedrock is the correct choice for the given use case. It\nis designed to help businesses build and scale generative AI\napplications quickly and efficiently. Bedrock offers access to a\nrange of pre-trained foundational models from Amazon and third-\nparty providers like AI21 Labs, Anthropic, and Stability AI. This\nmakes it ideal for tasks such as generating product descriptions,\ncreating marketing copy, and performing sentiment analysis on\ncustomer reviews. Bedrock allows users to easily integrate these\nAI capabilities into their applications without managing the\nunderlying infrastructure, making it a perfect fit for your business\nneeds.",
    "chunk_id": 267,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://aws.amazon.com/bedrock/faqs/\nIncorrect options:\nAmazon Rekognition - Amazon Rekognition is primarily used for\nimage and video analysis, such as detecting objects, text, and\nactivities. It is not designed for generating text or analyzing\nsentiment based on large datasets, so it would not meet the\nrequirements in this scenario.\nAmazon SageMaker - While Amazon SageMaker is a powerful\nservice for building, training, and deploying machine learning\nmodels, it requires more manual setup and expertise compared\nto Amazon Bedrock. SageMaker would be a more appropriate",
    "chunk_id": 268,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "choice if you needed custom models rather than leveraging pre-\ntrained foundational models with generative AI capabilities.\nAmazon Personalize - Amazon Personalize is a fully managed\nmachine learning service that uses your data to generate item\nrecommendations for your users. It can also generate user\nsegments based on the users' affinity for certain items or item\nmetadata. It also lacks the flexibility provided by Bedrock in\nchoosing from various foundational models.\nvia - https://docs.aws.amazon.com/personalize/latest/dg/what-\nis-personalize.html\nReferences:\nhttps://aws.amazon.com/bedrock/\nhttps://aws.amazon.com/bedrock/faqs/\nhttps://docs.aws.amazon.com/personalize/latest/dg/what-is-\npersonalize.html\nDomain\nML Model Development\nQuestion 51\nCorrect\nAn ML engineer is building a generative AI application on\nAmazon Bedrock using large language models (LLMs). The\nengineer needs to understand key generative AI concepts to\noptimize and customize the model's performance for the",
    "chunk_id": 269,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Amazon Bedrock using large language models (LLMs). The\nengineer needs to understand key generative AI concepts to\noptimize and customize the model's performance for the\napplication. Consider these generative AI terms followed by the\ndescriptions:\nGenerative AI terms:\n1. Embedding\n2. Token",
    "chunk_id": 270,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "3. Retrieval Augmented Generation (RAG)\n4. Temperature\n5. Prompt\nDescriptions: A. A method to combine external knowledge\nretrieval with generative AI to produce more accurate and\nrelevant responses. B. Numerical representation of text (or data)\nin a high-dimensional vector space. C. An input provided to a\nmodel to guide it to generate an appropriate response or output\nfor the input. D. Units of text (like words, subwords, or\ncharacters) processed by an LLM. E. A parameter used to control\nthe randomness of the model's output, influencing whether\nresponses are more creative or deterministic.\nCan you match the generative AI term to its correct description?\n1-B, 2-D, 3-C\nYour answer is correct\n1-B, 2-D, 3-A\n4-C, 5-E, 3-A\n1-D, 2-B, 3-A\nOverall explanation\nCorrect option:\n1-B, 2-D, 3-A\nRAG is a technique that integrates external data retrieval (e.g.,\nfrom a knowledge base) with generative AI to provide more\ncontextually accurate and relevant outputs. It is particularly",
    "chunk_id": 271,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "RAG is a technique that integrates external data retrieval (e.g.,\nfrom a knowledge base) with generative AI to provide more\ncontextually accurate and relevant outputs. It is particularly\nuseful when the model lacks specific information or domain\nknowledge.\nAn embedding is a numerical representation of text (or data) in a\nhigh-dimensional vector space. Embeddings allow models to\nmeasure the similarity between words, phrases, or sentences,\nwhich is essential for tasks like semantic search or clustering.\nTokens are units of text (like words, subwords, or characters)\nprocessed by an LLM.",
    "chunk_id": 272,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Incorrect options:\n4-C, 5-E, 3-A - Prompt is an input provided to a model to guide it\nto generate an appropriate response or output for the input.\nTemperature adjusts the randomness of LLM outputs. A lower\ntemperature makes outputs more deterministic, while a higher\ntemperature increases variability and creativity. So, this option is\nincorrect.\n1-B, 2-D, 3-C - Prompt is an input provided to a model to guide it\nto generate an appropriate response or output for the input. So,\nthis option is incorrect.\n1-D, 2-B, 3-A - This option swaps the definition of token with that\nof embedding. So, this option is incorrect.\nReferences:\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/key-\ndefinitions.html\nhttps://aws.amazon.com/what-is/generative-ai/\nDomain\nML Model Development\nQuestion 52\nIncorrect\nA global e-commerce company has developed a custom\nsentiment analysis model using Amazon Comprehend in Account\nA within the us-west-2 Region. The company now needs to copy",
    "chunk_id": 273,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Question 52\nIncorrect\nA global e-commerce company has developed a custom\nsentiment analysis model using Amazon Comprehend in Account\nA within the us-west-2 Region. The company now needs to copy\nthe model to Account B, which handles customer service\noperations, so that the model can be used to analyze customer\nfeedback. The solution must ensure secure and efficient transfer\nof the model with minimal development effort.\nWhich solution will meet these requirements?\nCorrect answer\nUse the Amazon Comprehend ImportModel API operation in\nAccount B to securely import the sentiment analysis model from\nAccount A by configuring a resource based IAM policy in account\nA",
    "chunk_id": 274,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Leverage AWS Glue Data Catalog to register the model metadata\nin Account A and enable cross-account sharing with Account B.\nUse the Comprehend console in Account B to access the model\nYour answer is incorrect\nUse the Amazon Comprehend ImportModel API operation in\nAccount B to securely import the sentiment analysis model from\nAccount A by configuring a resource based IAM policy in account\nB\nManually download the model artifacts from Amazon S3 in\nAccount A and upload them to an S3 bucket in Account B. Use a\ncustom script to recreate the model in Account B\nOverall explanation\nCorrect option:\nUse the Amazon Comprehend ImportModel API operation in\nAccount B to securely import the sentiment analysis model from\nAccount A by configuring a resource based IAM policy in account\nA\nThe Amazon Comprehend ImportModel API operation is\ndesigned for transferring custom models between accounts. The\nsource model must be in the same AWS Region that you're using",
    "chunk_id": 275,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "A\nThe Amazon Comprehend ImportModel API operation is\ndesigned for transferring custom models between accounts. The\nsource model must be in the same AWS Region that you're using\nwhen you import. You can't import a model that's in a different\nRegion.\nAmazon Comprehend users can copy trained custom models\nbetween AWS accounts in a two-step process. First, a user in\none AWS account (account A), shares a custom model that's in\ntheir account. Then, a user in another AWS account (account B)\nimports the model into their account. The account B user does\nnot need to train the model, and does not need to copy (or\naccess) the original training data or test data.\nTo share a custom model in account A, you need to attach a\nresource based IAM policy in account A. This policy authorizes\nan entity in account B, such as a user or role, to import the model\nversion into Amazon Comprehend in their AWS account. The",
    "chunk_id": 276,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "account B user must import the model into the same AWS\nRegion as the original model.\nvia - https://docs.aws.amazon.com/comprehend/latest/dg/\ncustom-copy-sharing.html#custom-copy-sharing-example-policy\nIncorrect options:\nManually download the model artifacts from Amazon S3 in\nAccount A and upload them to an S3 bucket in Account B. Use a\ncustom script to recreate the model in Account B - Amazon\nComprehend custom models are managed services, and their\nartifacts are not directly accessible as files. You cannot export\nthem to S3 for manual transfer. Instead, you must use the\nAmazon Comprehend APIs for sharing or copying models\nbetween accounts.",
    "chunk_id": 277,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Leverage AWS Glue Data Catalog to register the model metadata\nin Account A and enable cross-account sharing with Account B.\nUse the Comprehend console in Account B to access the model\n- AWS Glue Data Catalog is used for metadata management in\ndata lakes and ETL workflows, not for transferring Comprehend\ncustom models.\nUse the Amazon Comprehend ImportModel API operation in\nAccount B to securely import the sentiment analysis model from\nAccount A by configuring a resource based IAM policy in account\nB - You need to attach a resource based IAM policy in account A\nand NOT in account B.\nReferences:\nhttps://docs.aws.amazon.com/comprehend/latest/APIReference/\nAPI_ImportModel.html\nhttps://docs.aws.amazon.com/comprehend/latest/dg/custom-\ncopy-sharing.html#custom-copy-sharing-example-policy\nhttps://docs.aws.amazon.com/comprehend/latest/dg/custom-\ncopy.html\nDomain\nML Model Development\nQuestion 53\nIncorrect\nYou are a data scientist at a pharmaceutical company that builds",
    "chunk_id": 278,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "https://docs.aws.amazon.com/comprehend/latest/dg/custom-\ncopy.html\nDomain\nML Model Development\nQuestion 53\nIncorrect\nYou are a data scientist at a pharmaceutical company that builds\npredictive models to analyze clinical trial data. Due to regulatory\nrequirements, the company must maintain strict version control\nof all models used in decision-making processes. This includes\ntracking which data, hyperparameters, and code were used to\ntrain each model, as well as ensuring that models can be easily\nreproduced and audited in the future. You decide to implement a\nsystem to manage model versions and track their lifecycle\neffectively.\nWhich of the following strategies is the MOST LIKELY to ensure\nmodel versioning, repeatability, and auditability?",
    "chunk_id": 279,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Use Amazon S3 to store each version of the model manually,\ntagging the stored files with metadata about the training data,\nhyperparameters, and code used for training\nYour answer is incorrect\nUse SageMaker Model Monitor to track the performance of\nmodels in production, ensuring that any changes in model\nbehavior are documented for future audits\nCorrect answer\nLeverage the SageMaker Model Registry to register, track, and\nmanage different versions of models, capturing all relevant\nmetadata, including data sources, hyperparameters, and training\ncode\nCreate a version control system in Git for the model’s training\ncode and configuration files, while storing the trained models in a\nseparate S3 bucket for easy retrieval\nOverall explanation\nCorrect option:\nLeverage the SageMaker Model Registry to register, track, and\nmanage different versions of models, capturing all relevant\nmetadata, including data sources, hyperparameters, and training\ncode",
    "chunk_id": 280,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Leverage the SageMaker Model Registry to register, track, and\nmanage different versions of models, capturing all relevant\nmetadata, including data sources, hyperparameters, and training\ncode\nThe SageMaker Model Registry is specifically designed for\nmanaging model versions in a systematic and organized manner.\nIt allows you to register different versions of a model, track\nmetadata such as data sources, hyperparameters, and training\ncode, and ensure that each version is easily reproducible. This\napproach is ideal for regulatory environments where audit trails\nand model governance are critical.\nWith the Amazon SageMaker Model Registry you can do the\nfollowing:\nCatalog models for production.\nManage model versions.\nAssociate metadata, such as training metrics, with a model.",
    "chunk_id": 281,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "View information from Amazon SageMaker Model Cards in your\nregistered models.\nManage the approval status of a model.\nDeploy models to production.\nAutomate model deployment with CI/CD.\nShare models with other users.\nIncorrect options:\nUse Amazon S3 to store each version of the model manually,\ntagging the stored files with metadata about the training data,\nhyperparameters, and code used for training - While using\nAmazon S3 to store model versions with metadata is possible, it\nrequires a lot of manual effort and lacks the automated tracking\nand management capabilities needed for comprehensive version\ncontrol, repeatability, and auditability.\nCreate a version control system in Git for the model’s training\ncode and configuration files, while storing the trained models in a\nseparate S3 bucket for easy retrieval - Using Git for version\ncontrol of the training code and configurations is a good practice,\nbut it does not address the need to manage the actual trained",
    "chunk_id": 282,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "separate S3 bucket for easy retrieval - Using Git for version\ncontrol of the training code and configurations is a good practice,\nbut it does not address the need to manage the actual trained\nmodels and their associated metadata systematically. The\nSageMaker Model Registry offers a more comprehensive solution\nthat integrates both code and model versioning.\nUse SageMaker Model Monitor to track the performance of\nmodels in production, ensuring that any changes in model\nbehavior are documented for future audits - SageMaker Model\nMonitor is useful for monitoring model performance in\nproduction, but it does not handle version control or track the\nmetadata necessary for repeatability and audits. It is\ncomplementary to, but not a substitute for, the SageMaker Model\nRegistry.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/model-\nregistry.html\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/model-\nmonitor.html\nDomain",
    "chunk_id": 283,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "ML Model Development\nQuestion 54\nIncorrect\nYou are an ML Engineer working for a logistics company that\nuses multiple machine learning models to optimize delivery\nroutes in real-time. Each model needs to process data quickly to\nprovide up-to-the-minute route adjustments, but the company\nalso has strict cost constraints. You need to deploy the models in\nan environment where performance, cost, and latency are\ncarefully balanced. There may be slight variations in the access\nfrequency of the models. Any excessive costs could impact the\nproject’s profitability.\nWhich of the following strategies should you consider to balance\nthe tradeoffs between performance, cost, and latency when\ndeploying your model in Amazon SageMaker? (Select two)\nChoose a lower-cost CPU instance, accepting longer inference\ntimes, as the savings on compute costs are more important than\nminimizing latency\nYour selection is correct\nUse Amazon SageMaker’s multi-model endpoint to deploy",
    "chunk_id": 284,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "times, as the savings on compute costs are more important than\nminimizing latency\nYour selection is correct\nUse Amazon SageMaker’s multi-model endpoint to deploy\nmultiple models on a single instance, reducing costs by sharing\nresources\nDeploy the model on a high-performance GPU instance to\nminimize latency, regardless of the higher cost, ensuring real-time\nroute adjustments\nYour selection is incorrect\nLeverage Amazon SageMaker Neo to compile the model for\noptimized deployment on edge devices, reducing latency and\ncost but with limited scalability for large datasets\nCorrect selection",
    "chunk_id": 285,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Implement auto-scaling on a fleet of medium-sized instances,\nallowing the system to adjust resources based on real-time\ndemand, balancing cost and performance dynamically\nOverall explanation\nCorrect options:\nUse Amazon SageMaker’s multi-model endpoint to deploy\nmultiple models on a single instance, reducing costs by sharing\nresources\nAmazon SageMaker’s multi-model endpoint allows you to deploy\nmultiple models on a single instance. This can significantly\nreduce costs by sharing resources among models, but it may\nintroduce slight increases in latency due to the need to load the\ncorrect model into memory. This tradeoff can be acceptable if\ncost savings are a priority and latency requirements are not ultra-\nstrict.\nvia - https://docs.aws.amazon.com/sagemaker/latest/dg/multi-\nmodel-endpoints.html",
    "chunk_id": 286,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Implement auto-scaling on a fleet of medium-sized instances,\nallowing the system to adjust resources based on real-time\ndemand, balancing cost and performance dynamically\nAuto-scaling allows you to dynamically adjust the number of\ninstances based on demand, which helps balance performance\nand cost. During peak times, more instances can be provisioned\nto maintain low latency, while during off-peak times, fewer\ninstances are used, reducing costs. This strategy offers a flexible\nway to manage the tradeoffs between performance, cost, and\nlatency.\nIncorrect options:\nDeploy the model on a high-performance GPU instance to\nminimize latency, regardless of the higher cost, ensuring real-time\nroute adjustments - While deploying on a high-performance GPU\ninstance would minimize latency, it may not be cost-effective,\nespecially if the model does not require the full computational\npower of a GPU. The high cost might outweigh the benefits of\nlower latency.",
    "chunk_id": 287,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "especially if the model does not require the full computational\npower of a GPU. The high cost might outweigh the benefits of\nlower latency.\nChoose a lower-cost CPU instance, accepting longer inference\ntimes, as the savings on compute costs are more important than\nminimizing latency - Choosing a lower-cost CPU instance could\nlead to unacceptable delays in route adjustments, which could\nimpact delivery times. In this scenario, optimizing latency is\ncritical, and sacrificing performance for cost could be detrimental\nto the business.\nLeverage Amazon SageMaker Neo to compile the model for\noptimized deployment on edge devices, reducing latency and\ncost but with limited scalability for large datasets - While Amazon\nSageMaker Neo can optimize models for deployment on edge\ndevices, it is not the best fit for this scenario. Neo is more\nsuitable for low-latency, cost-effective deployments on devices\nwith limited resources. In this scenario, the need for scalable,",
    "chunk_id": 288,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "devices, it is not the best fit for this scenario. Neo is more\nsuitable for low-latency, cost-effective deployments on devices\nwith limited resources. In this scenario, the need for scalable,\ncloud-based infrastructure is more important.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-\nendpoints.html",
    "chunk_id": 289,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-\nauto-scaling.html\nDomain\nDeployment and Orchestration of ML Workflows\nQuestion 55\nCorrect\nA company stores its training datasets on Amazon S3 in the form\nof tabular data running into millions of rows. The company needs\nto prepare this data for Machine Learning jobs. The data\npreparation involves data selection, cleansing, exploration, and\nvisualization using a single visual interface.\nWhich Amazon SageMaker service is the best fit for this\nrequirement?\nSageMaker Model Dashboard\nAmazon SageMaker Clarify\nYour answer is correct\nAmazon SageMaker Data Wrangler\nAmazon SageMaker Feature Store\nOverall explanation\nCorrect option:\nAmazon SageMaker Data Wrangler\nAmazon SageMaker Data Wrangler reduces the time it takes to\naggregate and prepare tabular and image data for ML from\nweeks to minutes. With SageMaker Data Wrangler, you can\nsimplify the process of data preparation and feature engineering,",
    "chunk_id": 290,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "aggregate and prepare tabular and image data for ML from\nweeks to minutes. With SageMaker Data Wrangler, you can\nsimplify the process of data preparation and feature engineering,\nand complete each step of the data preparation workflow\n(including data selection, cleansing, exploration, visualization,\nand processing at scale) from a single visual interface. You can\nuse SQL to select the data that you want from various data\nsources and import it quickly. Next, you can use the data quality\nand insights report to automatically verify data quality and detect",
    "chunk_id": 291,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "anomalies, such as duplicate rows and target leakage.\nSageMaker Data Wrangler contains over 300 built-in data\ntransformations, so you can quickly transform data without\nwriting code.\nWith the SageMaker Data Wrangler data selection tool, you can\nquickly access and select your tabular and image data from\nvarious popular sources - such as Amazon Simple Storage\nService (Amazon S3), Amazon Athena, Amazon Redshift, AWS\nLake Formation, Snowflake, and Databricks - and over 50 other\nthird-party sources - such as Salesforce, SAP, Facebook Ads,\nand Google Analytics. You can also write queries for data\nsources using SQL and import data directly into SageMaker from\nvarious file formats, such as CSV, Parquet, JSON, and database\ntables.\nHow Data Wrangler works:\nvia - https://aws.amazon.com/sagemaker/data-wrangler/\nIncorrect options:\nSageMaker Model Dashboard - Amazon SageMaker Model\nDashboard is a centralized portal, accessible from the\nSageMaker console, where you can view, search, and explore all",
    "chunk_id": 292,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Incorrect options:\nSageMaker Model Dashboard - Amazon SageMaker Model\nDashboard is a centralized portal, accessible from the\nSageMaker console, where you can view, search, and explore all\nof the models in your account. You can track which models are\ndeployed for inference and if they are used in batch transform\njobs or hosted on endpoints.\nAmazon SageMaker Clarify - SageMaker Clarify helps identify\npotential bias during data preparation without writing code. You",
    "chunk_id": 293,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "specify input features, such as gender or age, and SageMaker\nClarify runs an analysis job to detect potential bias in those\nfeatures.\nAmazon SageMaker Feature Store - Amazon SageMaker Feature\nStore is a fully managed, purpose-built repository to store, share,\nand manage features for machine learning (ML) models. Features\nare inputs to ML models used during training and inference.\nReference:\nhttps://aws.amazon.com/sagemaker/data-wrangler/\nDomain\nData Preparation for Machine Learning (ML)\nQuestion 56\nCorrect\nA retail analytics company stores historical data in .csv files in\nAmazon S3. The data is partially populated, lacks column labels,\nand contains missing values. The company needs to prepare and\nstructure this data so it can be used effectively for training ML\nmodels. Given this context, consider the following five steps:\n1. Use AWS Glue crawlers to infer the schemas and available\ncolumns.\n2. Use Amazon EMR with Apache Spark for data cleaning and\nfeature engineering.",
    "chunk_id": 294,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "1. Use AWS Glue crawlers to infer the schemas and available\ncolumns.\n2. Use Amazon EMR with Apache Spark for data cleaning and\nfeature engineering.\n3. Store the resulting data back in Amazon S3.\n4. Use Amazon Redshift Spectrum to infer the schemas and\navailable columns.\n5. Use AWS Glue DataBrew for data cleaning and feature\nengineering.\nWhat is the correct order in which three of these steps should be\nselected to achieve this task efficiently?\n4,5,3\n1,2,3\n5,1,3",
    "chunk_id": 295,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Your answer is correct\n1,5,3\nOverall explanation\nCorrect option:\n1,5,3\n1. Use AWS Glue crawlers to infer the schemas and available\ncolumns.\nAWS Glue crawlers can analyze the .csv files in Amazon S3 and\nautomatically infer the schema and structure of the data. This\ncreates a table definition in the AWS Glue Data Catalog, enabling\nthe data to be organized and understood.\nUsing crawlers to populate the Data Catalog:",
    "chunk_id": 296,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://docs.aws.amazon.com/glue/latest/dg/add-\ncrawler.html\n1. Use AWS Glue DataBrew for data cleaning and feature\nengineering.\nAWS Glue DataBrew is a visual data preparation tool that allows\nthe ML engineer to clean and preprocess the .csv data. Tasks\nsuch as filling in missing fields, transforming formats, and\nnormalizing data can be performed easily with Glue DataBrew.\nvia - https://docs.aws.amazon.com/databrew/latest/dg/what-\nis.html\n1. Store the resulting data back in Amazon S3.\nAfter the data is cleaned and processed, it is stored back into\nAmazon S3, where it can be accessed for ML model training or\nfurther analysis.\nIncorrect options:\n5,1,3 - You cannot proceed with data cleaning and feature\nengineering before inferring the schema of the underlying data,\ntherefore this option is incorrect.\n1,2,3 - While EMR with Apache Spark is capable of data cleaning\nand feature engineering, it is a more complex and costly solution",
    "chunk_id": 297,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "therefore this option is incorrect.\n1,2,3 - While EMR with Apache Spark is capable of data cleaning\nand feature engineering, it is a more complex and costly solution\ncompared to AWS Glue DataBrew, which is a managed service",
    "chunk_id": 298,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "specifically designed for such tasks with lower operational\noverhead.\n4,5,3 - Amazon Redshift Spectrum allows querying data in\nAmazon S3 using Redshift’s SQL engine. However, it does not\ninfer schemas automatically; this is a capability of AWS Glue\nCrawlers. Spectrum is typically used for ad hoc querying rather\nthan schema inference.\nReferences:\nhttps://docs.aws.amazon.com/glue/latest/dg/add-crawler.html\nhttps://docs.aws.amazon.com/databrew/latest/dg/what-is.html\nDomain\nData Preparation for Machine Learning (ML)\nQuestion 57\nIncorrect\nA research organization collects weather data from multiple\nsensor devices across various locations. The data is stored as\nCSV files in a central Amazon S3 bucket. The CSV files contain\nthe same schema, with an observation date column to record\nwhen each reading was taken. The organization needs to perform\nad-hoc queries on the data to filter and analyze based on the\nobservation date.\nThe solution must enable efficient querying. Which solution will",
    "chunk_id": 299,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "ad-hoc queries on the data to filter and analyze based on the\nobservation date.\nThe solution must enable efficient querying. Which solution will\nmeet this requirement with the LEAST operational overhead?\nUse Amazon EMR with Apache Hive to preprocess the CSV files\nand filter the data using HiveQL\nYour answer is incorrect\nStream the new CSV files into Amazon Redshift and query the\ndata using SQL filters on the observation date column\nUse AWS Glue to create an ETL job that partitions and\ntransforms the CSV files for querying by the observation date and\nprocesses the output into a new S3 bucket\nCorrect answer",
    "chunk_id": 300,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Use Amazon Athena and run a CTAS (CREATE TABLE AS\nSELECT) query with partitioning enabled on the observation date\ncolumn to query and optimize the CSV files based on the\nobservation date column\nOverall explanation\nCorrect option:\nUse Amazon Athena and run a CTAS (CREATE TABLE AS\nSELECT) query with partitioning enabled on the observation date\ncolumn to query and optimize the CSV files based on the\nobservation date column\nAmazon Athena is a serverless query service that allows you to\nrun SQL queries directly on data stored in Amazon S3. Using the\nCTAS (CREATE TABLE AS SELECT) query:\nYou can filter the data efficiently based on the observation date\ncolumn since the data is partitioned.\nAthena can write the filtered results back to S3 in optimized\nformats like Parquet or ORC, which significantly improves query\nperformance and reduces costs.\nAthena requires no infrastructure management, making it the\nmost efficient and low-operational solution for querying S3-\nbased CSV data.",
    "chunk_id": 301,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "performance and reduces costs.\nAthena requires no infrastructure management, making it the\nmost efficient and low-operational solution for querying S3-\nbased CSV data.\nIncorrect options:\nUse AWS Glue to create an ETL job that partitions and\ntransforms the CSV files for querying by the observation date and\nprocesses the output into a new S3 bucket - AWS Glue is\nsuitable for ETL tasks but requires more configuration and\noperational effort compared to Athena for simple ad-hoc\nquerying. In addition, using a new S3 bucket for writing the\noutput is inefficient.\nUse Amazon EMR with Apache Hive to preprocess the CSV files\nand filter the data using HiveQL - EMR with Hive is capable of\nhandling this task, but it introduces cluster management\noverhead and is not as cost-effective as Athena for serverless\nqueries.",
    "chunk_id": 302,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Stream the new CSV files into Amazon Redshift and query the\ndata using SQL filters on the observation date column - Amazon\nRedshift is a managed data warehouse that requires data\ningestion, schema design, and ongoing management, adding\ncomplexity for simple querying tasks.\nReferences:\nhttps://docs.aws.amazon.com/athena/latest/ug/ctas.html\nhttps://docs.aws.amazon.com/athena/latest/ug/ctas-\nexamples.html\nDomain\nData Preparation for Machine Learning (ML)\nQuestion 58\nIncorrect\nYou are an ML engineer at an e-commerce company tasked with\nbuilding an automated recommendation system that scales\nduring peak shopping seasons. The solution requires\nprovisioning multiple compute resources, including SageMaker\nfor model training, EC2 instances for data preprocessing, and an\nRDS database for storing user interaction data. You need to\nautomate the deployment and management of these resources,\nensuring that the stacks can communicate effectively. The",
    "chunk_id": 303,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "RDS database for storing user interaction data. You need to\nautomate the deployment and management of these resources,\nensuring that the stacks can communicate effectively. The\ncompany prioritizes infrastructure as code (IaC) to maintain\nconsistency and scalability across environments.\nWhich approach is the MOST SUITABLE for automating the\nprovisioning of compute resources and ensuring seamless\ncommunication between stacks?\nYour answer is incorrect\nUse AWS Elastic Beanstalk to deploy the entire ML solution,\nrelying on its built-in environment management to handle the\nprovisioning and communication between resources\nautomatically\nManually provision the SageMaker, EC2, and RDS resources\nusing the AWS Management Console, ensuring that",
    "chunk_id": 304,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "communication is established by manually updating security\ngroups and networking configurations\nUse AWS CDK (Cloud Development Kit) to define the\ninfrastructure in a high-level programming language, deploying\neach service as an independent stack without configuring inter-\nstack communication\nCorrect answer\nUse AWS CloudFormation with nested stacks to automate the\nprovisioning of SageMaker, EC2, and RDS resources, and\nconfigure outputs from one stack as inputs to another to enable\ncommunication between them\nOverall explanation\nCorrect option:\nUse AWS CloudFormation with nested stacks to automate the\nprovisioning of SageMaker, EC2, and RDS resources, and\nconfigure outputs from one stack as inputs to another to enable\ncommunication between them\nAWS CloudFormation with nested stacks allows you to\nmodularize your infrastructure, making it easier to manage and\nreuse components. By passing outputs from one stack as inputs\nto another, you can automate the provisioning of resources while",
    "chunk_id": 305,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "modularize your infrastructure, making it easier to manage and\nreuse components. By passing outputs from one stack as inputs\nto another, you can automate the provisioning of resources while\nensuring that all stacks can communicate effectively. This\napproach also enables consistent and scalable deployments\nacross environments.",
    "chunk_id": 306,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://docs.aws.amazon.com/AWSCloudFormation/latest/\nUserGuide/using-cfn-nested-stacks.html\nIncorrect options:\nUse AWS CDK (Cloud Development Kit) to define the\ninfrastructure in a high-level programming language, deploying\neach service as an independent stack without configuring inter-\nstack communication - AWS CDK allows you to define\ninfrastructure using high-level programming languages, which is\nflexible and powerful. However, failing to configure inter-stack\ncommunication would lead to a disjointed deployment, where\nservices may not function together as required.\nManually provision the SageMaker, EC2, and RDS resources\nusing the AWS Management Console, ensuring that\ncommunication is established by manually updating security\ngroups and networking configurations - Manually provisioning\nresources through the AWS Management Console is error-prone\nand not scalable. It lacks the automation and repeatability that\ninfrastructure as code provides, making it unsuitable for",
    "chunk_id": 307,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "resources through the AWS Management Console is error-prone\nand not scalable. It lacks the automation and repeatability that\ninfrastructure as code provides, making it unsuitable for\nmanaging complex ML solutions that require seamless\ncommunication between multiple resources.",
    "chunk_id": 308,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Use AWS Elastic Beanstalk to deploy the entire ML solution,\nrelying on its built-in environment management to handle the\nprovisioning and communication between resources\nautomatically - AWS Elastic Beanstalk is a managed service for\ndeploying applications, but it is not designed for orchestrating\ncomplex ML workflows with multiple resource types like\nSageMaker, EC2, and RDS. It also lacks fine-grained control over\nresource provisioning and inter-stack communication.\nReference:\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/\nUserGuide/using-cfn-nested-stacks.html\nDomain\nDeployment and Orchestration of ML Workflows\nQuestion 59\nIncorrect\nYou are a Machine Learning Engineer working for a large retail\ncompany that has developed multiple machine learning models\nto improve various aspects of their business, including\npersonalized recommendations, generative AI, and fraud\ndetection. The models have different deployment requirements:",
    "chunk_id": 309,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "to improve various aspects of their business, including\npersonalized recommendations, generative AI, and fraud\ndetection. The models have different deployment requirements:\n1. The recommendations models need to handle real-time\ninference with low latency.\n2. The generative AI model requires high scalability to manage\nfluctuating loads.\n3. The fraud detection model is a large model and needs to be\nintegrated into serverless applications to minimize\ninfrastructure management.\nWhich of the following deployment targets should you choose for\nthe different machine learning models, given their specific\nrequirements? (Select two)\nCorrect selection\nDeploy the real-time recommendation model using Amazon\nSageMaker endpoints to ensure low-latency, high-availability,\nand managed infrastructure for real-time inference",
    "chunk_id": 310,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Deploy all models using Amazon SageMaker endpoints for\nconsistency and ease of management, regardless of their\nindividual requirements for scalability, latency, or integration\nYour selection is incorrect\nChoose Amazon Elastic Container Service (Amazon ECS) for the\nrecommendation model, as it provides container orchestration for\nlarge-scale, batch processing workloads with tight integration\ninto other AWS services\nYour selection is incorrect\nUse AWS Lambda to deploy the fraud detection model, which\nrequires rapid scaling and integration into an existing serverless\narchitecture, minimizing infrastructure management\nCorrect selection\nDeploy the generative AI model using Amazon Elastic Kubernetes\nService (Amazon EKS) to leverage containerized microservices\nfor high scalability and control over the deployment environment\nOverall explanation\nCorrect options:\nDeploy the real-time recommendation model using Amazon\nSageMaker endpoints to ensure low-latency, high-availability,",
    "chunk_id": 311,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Overall explanation\nCorrect options:\nDeploy the real-time recommendation model using Amazon\nSageMaker endpoints to ensure low-latency, high-availability,\nand managed infrastructure for real-time inference\nReal-time inference is ideal for inference workloads where you\nhave real-time, interactive, low latency requirements. You can\ndeploy your model to SageMaker hosting services and get an\nendpoint that can be used for inference. These endpoints are\nfully managed and support autoscaling.\nThis makes it an ideal choice for the recommendation model,\nwhich must provide fast responses to user interactions with\nminimal downtime.\nDeploy the generative AI model using Amazon Elastic Kubernetes\nService (Amazon EKS) to leverage containerized microservices\nfor high scalability and control over the deployment environment",
    "chunk_id": 312,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Amazon EKS is designed for containerized applications that need\nhigh scalability and flexibility. It is suitable for the generative AI\nmodel, which may require complex orchestration and scaling in\nresponse to varying demand, while giving you full control over\nthe deployment environment.\nvia - https://aws.amazon.com/blogs/containers/deploy-\ngenerative-ai-models-on-amazon-eks/\nIncorrect options:\nUse AWS Lambda to deploy the fraud detection model, which\nrequires rapid scaling and integration into an existing serverless\narchitecture, minimizing infrastructure management - While AWS",
    "chunk_id": 313,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Lambda is excellent for serverless applications, it may not be the\nbest choice for a fraud detection model if it requires continuous,\nlow-latency processing or needs to handle very high throughput.\nLambda is better suited for lightweight, event-driven tasks rather\nthan long-running, complex inference jobs.\nChoose Amazon Elastic Container Service (Amazon ECS) for the\nrecommendation model, as it provides container orchestration for\nlarge-scale, batch processing workloads with tight integration\ninto other AWS services - Amazon ECS is a good choice for\ncontainerized workloads but is generally more appropriate for\nbatch processing or large-scale, stateless applications. It might\nnot provide the low-latency and real-time capabilities needed for\nthe recommendation model.\nDeploy all models using Amazon SageMaker endpoints for\nconsistency and ease of management, regardless of their\nindividual requirements for scalability, latency, or integration -",
    "chunk_id": 314,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Deploy all models using Amazon SageMaker endpoints for\nconsistency and ease of management, regardless of their\nindividual requirements for scalability, latency, or integration -\nDeploying all models using Amazon SageMaker endpoints\nwithout considering their specific requirements for latency,\nscalability, and integration would be suboptimal. While\nSageMaker endpoints are highly versatile, they may not be the\nbest fit for every use case, especially for models requiring\nserverless architecture or advanced container orchestration.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/realtime-\nendpoints.html\nhttps://aws.amazon.com/blogs/containers/deploy-generative-ai-\nmodels-on-amazon-eks/\nDomain\nDeployment and Orchestration of ML Workflows\nQuestion 60\nIncorrect\nA financial services company is building a customer churn\nprediction model on AWS. The dataset includes call logs,\ncustomer interaction history, and transactional data from an on-",
    "chunk_id": 315,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Incorrect\nA financial services company is building a customer churn\nprediction model on AWS. The dataset includes call logs,\ncustomer interaction history, and transactional data from an on-\npremises PostgreSQL database. The call logs and interaction",
    "chunk_id": 316,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "history are stored in Amazon S3, while the PostgreSQL tables\nremain on-premises. The data science team needs to aggregate\nand preprocess data from these various sources to ensure it is\nready for machine learning model training. They must also\nresolve challenges such as feature inconsistencies and ensure\nschema alignment across the data sources.\nWhich AWS service or feature can efficiently connect and\naggregate the data from these sources?\nCorrect answer\nUse AWS Lake Formation to aggregate and centrally manage the\ndata from S3 and on-premises PostgreSQL for seamless access\nand integration\nYour answer is incorrect\nUse Amazon SageMaker Data Wrangler to connect, aggregate,\nand preprocess the data\nUse AWS Database Migration Service (DMS) to transfer and\npreprocess data for ML training\nUse Amazon EMR Spark jobs to preprocess and aggregate the\ndata directly from S3 and on-premises sources for ML model\ntraining\nOverall explanation\nCorrect option:",
    "chunk_id": 317,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "preprocess data for ML training\nUse Amazon EMR Spark jobs to preprocess and aggregate the\ndata directly from S3 and on-premises sources for ML model\ntraining\nOverall explanation\nCorrect option:\nUse AWS Lake Formation to aggregate and centrally manage the\ndata from S3 and on-premises PostgreSQL for seamless access\nand integration\nAWS Lake Formation is specifically designed for aggregating and\nmanaging large datasets from various sources, including Amazon\nS3, databases, and other on-premises or cloud-based sources. It\nsimplifies the process of:\nIntegrating data from various locations.\nCataloging the data in a centralized data lake.\nManaging permissions and security for the aggregated dataset.",
    "chunk_id": 318,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "AWS Lake Formation supports connecting to on-premises\nPostgreSQL databases and Amazon S3, making it the best\nchoice for aggregating transaction logs, customer profiles, and\ndatabase tables. Additionally, the centralized data lake can be\nused for further analysis and ML training.\nvia - https://docs.aws.amazon.com/lake-formation/latest/dg/\nwhat-is-lake-formation.html\nIncorrect options:\nUse Amazon SageMaker Data Wrangler to connect, aggregate,\nand preprocess the data - While SageMaker Data Wrangler is\ngreat for data preparation, it does not perform large-scale ETL\noperations or automate schema inference from on-premises\ndatabases. AWS Lake Formation is better suited for aggregating\nraw data from multiple sources.\nUse AWS Database Migration Service (DMS) to transfer and\npreprocess data for ML training - AWS DMS is used to migrate\ndata from on-premises databases to AWS services, but it does",
    "chunk_id": 319,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "not include preprocessing or schema alignment functionalities\nneeded for ML model training.\nUse Amazon EMR Spark jobs to preprocess and aggregate the\ndata directly from S3 and on-premises sources for ML model\ntraining - While Amazon EMR with Spark can preprocess and\naggregate data, it requires significant manual configuration and\nmanagement. Compared to AWS Lake Formation, it introduces\noperational overhead and complexity, which is not ideal for\nsimpler workflows requiring schema inference and automated\nintegration.\nReferences:\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/what-is-\nlake-formation.html\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/\nemr-what-is-emr.html\nDomain\nData Preparation for Machine Learning (ML)\nQuestion 61\nIncorrect\nYou are working on a machine learning project for a financial\nservices company, developing a model to predict credit risk. After\ndeploying the initial version of the model using Amazon",
    "chunk_id": 320,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Incorrect\nYou are working on a machine learning project for a financial\nservices company, developing a model to predict credit risk. After\ndeploying the initial version of the model using Amazon\nSageMaker, you find that its performance, measured by the AUC\n(Area Under the Curve), is not meeting the company’s accuracy\nrequirements. Your team has gathered more data and believes\nthat the model can be further optimized.\nYou are considering various methods to improve the model’s\nperformance, including feature engineering, hyperparameter\ntuning, and trying different algorithms. However, given the limited\ntime and computational resources, you need to prioritize the\nmost impactful strategies.\nWhich of the following approaches are the MOST LIKELY to lead\nto a significant improvement in model performance? (Select two)",
    "chunk_id": 321,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Switch to a more complex algorithm, such as deep learning, and\nuse transfer learning to leverage pre-trained models\nCorrect selection\nUse Amazon SageMaker Debugger to debug and improve model\nperformance by addressing underlying problems such as\noverfitting, saturated activation functions, and vanishing\ngradients\nYour selection is incorrect\nPerform hyperparameter tuning using Bayesian optimization and\nincrease the number of trials to explore a broader search space\nIncrease the size of the training dataset by incorporating\nsynthetic data and then retrain the existing model\nYour selection is correct\nFocus on feature engineering by creating domain-specific\nfeatures and use SageMaker Clarify to evaluate feature\nimportance\nOverall explanation\nCorrect options:\nFocus on feature engineering by creating domain-specific\nfeatures and use SageMaker Clarify to evaluate feature\nimportance\nFeature engineering is one of the most effective ways to boost",
    "chunk_id": 322,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Focus on feature engineering by creating domain-specific\nfeatures and use SageMaker Clarify to evaluate feature\nimportance\nFeature engineering is one of the most effective ways to boost\nmodel performance, particularly in domain-specific applications\nlike credit risk modeling. By creating more informative features,\nyou can provide the model with better signals for prediction.\nSageMaker Clarify can be used to evaluate feature importance,\nhelping you identify the most impactful features and further refine\nthe model.",
    "chunk_id": 323,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://aws.amazon.com/sagemaker/clarify/\nUse Amazon SageMaker Debugger to debug and improve model\nperformance by addressing underlying problems such as\noverfitting, saturated activation functions, and vanishing\ngradients\nA machine learning (ML) training job can have problems such as\noverfitting, saturated activation functions, and vanishing\ngradients, which can compromise model performance.\nSageMaker Debugger provides tools to debug training jobs and\nresolve such problems to improve the performance of your\nmodel. Debugger also offers tools to send alerts when training\nanomalies are found, take actions against the problems, and\nidentify the root cause of them by visualizing collected metrics\nand tensors.\nSageMaker Debugger:",
    "chunk_id": 324,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://docs.aws.amazon.com/sagemaker/latest/dg/train-\ndebugger.html\nIncorrect options:\nIncrease the size of the training dataset by incorporating\nsynthetic data and then retrain the existing model - Increasing\nthe size of the dataset with synthetic data can improve model\nperformance, but it also introduces the risk of adding noise or\nbias if the synthetic data is not carefully generated. This\napproach may not guarantee a significant performance boost\nunless the original dataset was severely lacking in size.\nSwitch to a more complex algorithm, such as deep learning, and\nuse transfer learning to leverage pre-trained models - Switching\nto a more complex algorithm or using transfer learning could\nimprove performance, but it also increases the risk of overfitting,\nespecially if the new algorithm is not well suited to the data.\nAdditionally, deep learning models require more data and tuning,\nwhich may not be feasible given the time and resource\nconstraints.",
    "chunk_id": 325,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "especially if the new algorithm is not well suited to the data.\nAdditionally, deep learning models require more data and tuning,\nwhich may not be feasible given the time and resource\nconstraints.\nPerform hyperparameter tuning using Bayesian optimization and\nincrease the number of trials to explore a broader search space -\nHyperparameter tuning, especially using Bayesian optimization,\ncan help optimize the model’s performance, but the gains might\nbe marginal if the underlying features are not informative. It’s a\nvaluable approach, but may not be the most impactful first step.\nReferences:",
    "chunk_id": 326,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "https://docs.aws.amazon.com/sagemaker/latest/dg/train-\ndebugger.html\nhttps://aws.amazon.com/sagemaker/clarify/\nDomain\nML Model Development\nQuestion 62\nCorrect\nA pharmaceutical company is using Amazon SageMaker to\ndevelop machine learning models for drug discovery. The data\nscientists need a solution that provides granular control over their\nML pipelines to manage the steps involved in preclinical testing\nsimulations. They also require the ability to visualize workflows\nfor experiments as a directed acyclic graph (DAG) to better\nunderstand dependencies. In addition, the solution must allow\nthem to maintain a history of experiment trials for reproducibility\nand optimization, as well as tools to implement model\ngovernance for regulatory audits and compliance verifications.\nWhich solution will meet these requirements?\nUse SageMaker Experiments for tracking and visualizing all\nworkflows and rely on AWS CodePipeline for orchestration and\ngovernance",
    "chunk_id": 327,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Which solution will meet these requirements?\nUse SageMaker Experiments for tracking and visualizing all\nworkflows and rely on AWS CodePipeline for orchestration and\ngovernance\nUse SageMaker Pipelines for orchestration, visualize workflows\nas a DAG, and leverage SageMaker Experiments for model\ngovernance and compliance\nUse AWS CodePipeline for orchestration, visualize workflows as\na series of stages, and implement SageMaker Experiments to\nmaintain a running history of model discovery experiments\nYour answer is correct\nUse SageMaker Pipelines for orchestrating ML workflows,\nvisualize them as a DAG, and leverage SageMaker ML Lineage\nTracking for auditing and compliance",
    "chunk_id": 328,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Overall explanation\nCorrect option:\nUse SageMaker Pipelines for orchestrating ML workflows,\nvisualize them as a DAG, and leverage SageMaker ML Lineage\nTracking for auditing and compliance\nSageMaker Pipelines is specifically designed for ML workflow\norchestration. It provides:\nFine-grained control - Data scientists can define step-by-step ML\nworkflows, including data preparation, model training, and\ndeployment.\nVisualization as a DAG - Workflows are represented as a directed\nacyclic graph (DAG), making it easy to visualize dependencies.\nSeamless integration with SageMaker ML Lineage Tracking -\nAutomatically tracks lineage information, including input\ndatasets, model artifacts, and inference endpoints, ensuring\ncompliance and auditability.\nSageMaker ML Lineage Tracking provides tools to establish\nmodel governance by maintaining a detailed record of all\ncomponents involved in the ML lifecycle.\nAmazon SageMaker ML Lineage Tracking:",
    "chunk_id": 329,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://docs.aws.amazon.com/sagemaker/latest/dg/lineage-\ntracking.html\nIncorrect options:\nUse AWS CodePipeline for orchestration, visualize workflows as\na series of stages, and implement SageMaker Experiments to\nmaintain a running history of model discovery experiments - AWS\nCodePipeline is a general-purpose CI/CD tool for automating\nworkflows, but it does not provide DAG visualization or fine-\ngrained control for ML-specific workflows like SageMaker\nPipelines. Additionally, SageMaker Experiments does not track\nlineage for auditing and compliance.\nUse SageMaker Experiments for tracking and visualizing all\nworkflows and rely on AWS CodePipeline for orchestration and",
    "chunk_id": 330,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "governance - SageMaker Experiments tracks and organizes\nexperiment trials but does not provide workflow orchestration or\nDAG visualization. It also lacks tools for comprehensive\ncompliance and auditing.\nUse SageMaker Pipelines for orchestration, visualize workflows\nas a DAG, and leverage SageMaker Experiments for model\ngovernance and compliance - SageMaker Experiments is useful\nfor managing experiments but does not provide lineage tracking,\nwhich is critical for ensuring governance for regulatory audits and\ncompliance verifications.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/lineage-\ntracking.html\nhttps://aws.amazon.com/sagemaker-ai/experiments/\nDomain\nML Solution Monitoring, Maintenance, and Security\nQuestion 63\nIncorrect\nA retail company has deployed a machine learning (ML) model\nusing Amazon SageMaker to forecast product demand. The\nmodel is exposed via a SageMaker endpoint that processes\nrequests from multiple applications. The company needs to",
    "chunk_id": 331,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "using Amazon SageMaker to forecast product demand. The\nmodel is exposed via a SageMaker endpoint that processes\nrequests from multiple applications. The company needs to\nrecord and monitor all API call events made to the endpoint and\nreceive a notification whenever the number of requests exceeds\na specific threshold during peak traffic hours.\nWhich solution will meet these requirements?\nYour answer is incorrect\nEnable AWS CloudTrail to log all SageMaker API call events and\nuse CloudTrail Insights to send notifications when the API call\nvolume exceeds a threshold\nUse SageMaker Model Monitor to capture and analyze endpoint\ntraffic and configure a rule to notify when API calls exceed the\nspecified threshold",
    "chunk_id": 332,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Use Amazon EventBridge to capture SageMaker API call events\nand configure a rule to send a notification when the event count\nbreaches the threshold\nCorrect answer\nUse Amazon CloudWatch to monitor the API call metrics for the\nSageMaker endpoint and create an alarm to send notifications\nthrough Amazon SNS when the call count breaches the threshold\nOverall explanation\nCorrect option:\nUse Amazon CloudWatch to monitor the API call metrics for the\nSageMaker endpoint and create an alarm to send notifications\nthrough Amazon SNS when the call count breaches the threshold\nAmazon CloudWatch is the most suitable solution for this\nscenario because it provides:\nMetrics for API call counts - CloudWatch automatically collects\ninvocation metrics for SageMaker endpoints, including\nInvocations, InvocationErrors, and Latency.\nAlarms - Alarms can be created to monitor thresholds for metrics,\nsuch as the number of API calls.\nNotifications - When a threshold is breached, the alarm can send",
    "chunk_id": 333,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Alarms - Alarms can be created to monitor thresholds for metrics,\nsuch as the number of API calls.\nNotifications - When a threshold is breached, the alarm can send\nnotifications through Amazon Simple Notification Service (SNS).",
    "chunk_id": 334,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://docs.aws.amazon.com/sagemaker/latest/dg/\nmonitoring-cloudwatch.html\nIncorrect options:\nEnable AWS CloudTrail to log all SageMaker API call events and\nuse CloudTrail Insights to send notifications when the API call\nvolume exceeds a threshold - AWS CloudTrail logs API calls for\nauditing purposes and can detect unusual activity using\nCloudTrail Insights. However, it is not designed for threshold-\nbased alarms.",
    "chunk_id": 335,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Use SageMaker Model Monitor to capture and analyze endpoint\ntraffic and configure a rule to notify when API calls exceed the\nspecified threshold - SageMaker Model Monitor is used to track\ndata quality, bias, and drift for endpoint traffic. It does not\nnatively monitor API call metrics or provide mechanisms for\ntriggering notifications based on call counts.\nUse Amazon EventBridge to capture SageMaker API call events\nand configure a rule to send a notification when the event count\nbreaches the threshold - EventBridge is suitable for capturing\nand routing specific event patterns, but it does not aggregate or\nmonitor API metrics over time. It lacks the capability to set\nthresholds or alarms for continuous monitoring.\nReference:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-\ncloudwatch.html\nDomain\nML Solution Monitoring, Maintenance, and Security\nQuestion 64\nIncorrect\nAn ML engineer is training a time series forecasting model using",
    "chunk_id": 336,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "cloudwatch.html\nDomain\nML Solution Monitoring, Maintenance, and Security\nQuestion 64\nIncorrect\nAn ML engineer is training a time series forecasting model using\na recurrent neural network (RNN) to predict electricity demand for\na utility company. The model is trained using stochastic gradient\ndescent (SGD) as the optimizer. During training, the engineer\nnotices the following:\nThe training loss and validation loss remain high.\nThe loss values oscillate, decreasing for a few epochs and then\nincreasing again before repeating the cycle.\nThe ML engineer needs to resolve this issue to stabilize the\ntraining process and improve model performance. What should\nthe ML engineer do to improve the training process?\nYour answer is incorrect\nApply dropout regularization to the RNN layers to improve\ngeneralization and reduce oscillations in the loss",
    "chunk_id": 337,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Increase the number of training epochs to give the model more\ntime to learn the patterns in the data\nIncrease the learning rate to allow the gradient updates to\nconverge more smoothly and prevent oscillations in the loss\nvalues\nCorrect answer\nReduce the learning rate to allow the gradient updates to\nconverge more smoothly and prevent oscillations in the loss\nvalues\nOverall explanation\nCorrect option:\nReduce the learning rate to allow the gradient updates to\nconverge more smoothly and prevent oscillations in the loss\nvalues\nThe oscillating pattern of the loss values during training and\nvalidation suggests that the learning rate is too high. When the\nlearning rate is large:\nThe gradient updates overshoot the optimal solution, causing\nloss values to oscillate instead of converging.\nTraining cannot settle into a local minimum, resulting in poor\nperformance on the test set.\nBy reducing the learning rate, the gradient updates become\nsmaller, allowing the model to converge more smoothly and",
    "chunk_id": 338,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "performance on the test set.\nBy reducing the learning rate, the gradient updates become\nsmaller, allowing the model to converge more smoothly and\nstabilize the training process. This will help the loss values\ndecrease steadily over time.\nIncorrect options:\nApply dropout regularization to the RNN layers to improve\ngeneralization and reduce oscillations in the loss - Dropout\nregularization helps reduce overfitting, but the described issue is\na training instability problem caused by a high learning rate.\nIncrease the learning rate to allow the gradient updates to\nconverge more smoothly and prevent oscillations in the loss\nvalues - Increasing the learning rate would further exacerbate the",
    "chunk_id": 339,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "problem. Too large a learning rate might prevent the weights from\napproaching the optimal solution.\nIncrease the number of training epochs to give the model more\ntime to learn the patterns in the data - Increasing epochs won’t\nresolve oscillations caused by a high learning rate. The model will\ncontinue to oscillate without convergence.\nReference:\nhttps://docs.aws.amazon.com/machine-learning/latest/dg/\ntraining-parameters1.html\nDomain\nML Model Development\nQuestion 65\nIncorrect\nYou are a data scientist at a healthcare company developing a\nmachine learning model to analyze medical imaging data, such\nas X-rays and MRIs, for disease detection. The dataset consists\nof 10 million high-resolution images stored in Amazon S3,\namounting to several terabytes of data. The training process\nrequires processing these images efficiently to avoid delays due\nto I/O bottlenecks, and you must ensure that the chosen data\naccess method aligns with the large dataset size and the high",
    "chunk_id": 340,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "requires processing these images efficiently to avoid delays due\nto I/O bottlenecks, and you must ensure that the chosen data\naccess method aligns with the large dataset size and the high\nthroughput requirements of the model.\nGiven the size and nature of the dataset, which SageMaker input\nmode and AWS Cloud Storage configuration is the MOST\nSUITABLE for this use case?\nCorrect answer\nSelect the Pipe input mode to stream the data directly from\nAmazon S3 to the training instances, allowing the model to start\nprocessing data immediately without requiring local storage for\nthe entire dataset\nUse the File input mode to download the entire dataset from\nAmazon S3 to the training instances' local storage before starting",
    "chunk_id": 341,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "the training process, ensuring that all data is available locally\nduring training\nUse the File input mode with EFS (Amazon Elastic File System) to\nmount the dataset across multiple instances, ensuring data is\nshared and accessible during distributed training\nYour answer is incorrect\nImplement the FastFile input mode with FSx for Lustre, to enable\non-demand streaming of data chunks from Amazon S3 with low\nlatency and high throughput\nOverall explanation\nCorrect option:\nSelect the Pipe input mode to stream the data directly from\nAmazon S3 to the training instances, allowing the model to start\nprocessing data immediately without requiring local storage for\nthe entire dataset\nIn pipe mode, data is pre-fetched from Amazon S3 at high\nconcurrency and throughput, and streamed into a named pipe,\nwhich also known as a First-In-First-Out (FIFO) pipe for its\nbehavior. Each pipe may only be read by a single process.\nPipe input mode is designed for large datasets, allowing data to",
    "chunk_id": 342,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "which also known as a First-In-First-Out (FIFO) pipe for its\nbehavior. Each pipe may only be read by a single process.\nPipe input mode is designed for large datasets, allowing data to\nbe streamed directly from Amazon S3 into the training instances.\nThis minimizes disk usage and allows training to begin\nimmediately as the data streams in, making it ideal for your\nscenario where high throughput and efficiency are critical.",
    "chunk_id": 343,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "via - https://docs.aws.amazon.com/sagemaker/latest/dg/model-\naccess-training-data.html",
    "chunk_id": 344,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "Incorrect options:\nUse the File input mode to download the entire dataset from\nAmazon S3 to the training instances' local storage before starting\nthe training process, ensuring that all data is available locally\nduring training - The File input mode downloads the entire\ndataset to the training instance before starting the training job.\nFor a dataset as large as yours, this would lead to significant\ndelays and require large amounts of local storage, which is not\noptimal for efficiency or cost.\nImplement the FastFile input mode with FSx for Lustre, to enable\non-demand streaming of data chunks from Amazon S3 with low\nlatency and high throughput - FastFile mode is useful for\nscenarios where you need rapid access to data with low latency,\nbut it is best suited for workloads with many small files. You\nshould note that FastFile mode can be used only while accessing\ndata from Amazon S3 and not with Amazon FSx for Lustre. So,\nthis option acts as a distractor.",
    "chunk_id": 345,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  },
  {
    "text": "should note that FastFile mode can be used only while accessing\ndata from Amazon S3 and not with Amazon FSx for Lustre. So,\nthis option acts as a distractor.\nUse the File input mode with EFS (Amazon Elastic File System) to\nmount the dataset across multiple instances, ensuring data is\nshared and accessible during distributed training - Using Amazon\nEFS for the given use case requires transferring the medical\nimaging data from Amazon S3 into Amazon EFS, which leads to\nunnecessary data transfer as well as data storage costs. So, this\noption is ruled out.\nReferences:\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/model-\naccess-training-data.html\nhttps://aws.amazon.com/about-aws/whats-new/2021/10/\namazon-sagemaker-fast-file-mode/\nDomain\nData Preparation for Machine Learning (ML)",
    "chunk_id": 346,
    "metadata": {
      "source": "practice_test",
      "doc_type": "questions",
      "filename": "exam practice machine learning review 1.pdf"
    }
  }
]